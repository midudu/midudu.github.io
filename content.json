{"meta":{"title":"Midudu's Home","subtitle":"","description":"Tech Blog","author":"Midudu","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"Redis 源码阅读11_客户端和服务器_redisServer","slug":"Redis源码阅读11_客户端和服务器_redisServer","date":"2020-03-11T12:25:46.351Z","updated":"2020-03-11T12:28:36.000Z","comments":true,"path":"开源组件/Redis/源码阅读/Redis源码阅读11_客户端和服务器_redisServer/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/Redis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB11_%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%99%A8_redisServer/","excerpt":"","text":"redis 单机服务器 涉及文件 redis server 1. 设置各种东西 2. 初始化服务器 3. 如果服务器以 Sentinel 模式启动，那么进行 Sentinel 功能相关的初始化 4. 读命令行用户输入，进行配置 5. 将服务器设置为守护进程 6. 创建并初始化服务器数据结构 7. 如果服务器是守护进程，那么创建 PID 文件 8. 为服务器进程设置名字 9. 打印 ASCII LOGO 10. 载入 rdb 和 aof 文件 11. 设置 before sleep 方法 12. 开始主 loop 13. 结束，清理 loop 关于几个文件事件处理器的逻辑关系 redis 不能简单说是一个单进程单线程 reactor 涉及文件redis.c redis servermain 函数在 redis.c 中 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159int main(int argc, char **argv) &#123; struct timeval tv; /* We need to initialize our libraries, and the server configuration. */ // 初始化库#ifdef INIT_SETPROCTITLE_REPLACEMENT spt_init(argc, argv);#endif setlocale(LC_COLLATE, \"\"); zmalloc_enable_thread_safeness(); zmalloc_set_oom_handler(redisOutOfMemoryHandler); srand(time(NULL) ^ getpid()); gettimeofday(&amp;tv, NULL); dictSetHashFunctionSeed(tv.tv_sec ^ tv.tv_usec ^ getpid()); // 检查服务器是否以 Sentinel 模式启动 server.sentinel_mode = checkForSentinelMode(argc, argv); // 初始化服务器 initServerConfig(); /* We need to init sentinel right now as parsing the configuration file * in sentinel mode will have the effect of populating the sentinel * data structures with master nodes to monitor. */ // 如果服务器以 Sentinel 模式启动，那么进行 Sentinel 功能相关的初始化 // 并为要监视的主服务器创建一些相应的数据结构 if (server.sentinel_mode) &#123; initSentinelConfig(); initSentinel(); &#125; // 检查用户是否指定了配置文件，或者配置选项 if (argc &gt;= 2) &#123; int j = 1; /* First option to parse in argv[] */ sds options = sdsempty(); char *configfile = NULL; /* Handle special options --help and --version */ // 处理特殊选项 -h 、-v 和 --test-memory if (strcmp(argv[1], \"-v\") == 0 || strcmp(argv[1], \"--version\") == 0) version(); if (strcmp(argv[1], \"--help\") == 0 || strcmp(argv[1], \"-h\") == 0) usage(); if (strcmp(argv[1], \"--test-memory\") == 0) &#123; if (argc == 3) &#123; memtest(atoi(argv[2]), 50); exit(0); &#125; else &#123; fprintf(stderr, \"Please specify the amount of memory to test in megabytes.\\n\"); fprintf(stderr, \"Example: ./redis-server --test-memory 4096\\n\\n\"); exit(1); &#125; &#125; /* First argument is the config file name? */ // 如果第一个参数（argv[1]）不是以 \"--\" 开头 // 那么它应该是一个配置文件 if (argv[j][0] != '-' || argv[j][1] != '-') configfile = argv[j++]; /* All the other options are parsed and conceptually appended to the * configuration file. For instance --port 6380 will generate the * string \"port 6380\\n\" to be parsed after the actual file name * is parsed, if any. */ // 对用户给定的其余选项进行分析，并将分析所得的字符串追加稍后载入的配置文件的内容之后 // 比如 --port 6380 会被分析为 \"port 6380\\n\" while (j != argc) &#123; if (argv[j][0] == '-' &amp;&amp; argv[j][1] == '-') &#123; /* Option name */ if (sdslen(options)) options = sdscat(options, \"\\n\"); options = sdscat(options, argv[j] + 2); options = sdscat(options, \" \"); &#125; else &#123; /* Option argument */ options = sdscatrepr(options, argv[j], strlen(argv[j])); options = sdscat(options, \" \"); &#125; j++; &#125; if (configfile) server.configfile = getAbsolutePath(configfile); // 重置保存条件 resetServerSaveParams(); // 载入配置文件， options 是前面分析出的给定选项 loadServerConfig(configfile, options); sdsfree(options); // 获取配置文件的绝对路径 if (configfile) server.configfile = getAbsolutePath(configfile); &#125; else &#123; redisLog(REDIS_WARNING, \"Warning: no config file specified, using the default config. In order to specify a config file use %s /path/to/%s.conf\", argv[0], server.sentinel_mode ? \"sentinel\" : \"redis\"); &#125; // 将服务器设置为守护进程 if (server.daemonize) daemonize(); // 创建并初始化服务器数据结构 initServer(); // 如果服务器是守护进程，那么创建 PID 文件 if (server.daemonize) createPidFile(); // 为服务器进程设置名字 redisSetProcTitle(argv[0]); // 打印 ASCII LOGO redisAsciiArt(); // 如果服务器不是运行在 SENTINEL 模式，那么执行以下代码 if (!server.sentinel_mode) &#123; /* Things not needed when running in Sentinel mode. */ // 打印问候语 redisLog(REDIS_WARNING, \"Server started, Redis version \" REDIS_VERSION);#ifdef __linux__ // 打印内存警告 linuxOvercommitMemoryWarning();#endif // 从 AOF 文件或者 RDB 文件中载入数据 loadDataFromDisk(); // 启动集群？ if (server.cluster_enabled) &#123; if (verifyClusterConfigWithData() == REDIS_ERR) &#123; redisLog(REDIS_WARNING, \"You can't have keys in a DB different than DB 0 when in \" \"Cluster mode. Exiting.\"); exit(1); &#125; &#125; // 打印 TCP 端口 if (server.ipfd_count &gt; 0) redisLog(REDIS_NOTICE, \"The server is now ready to accept connections on port %d\", server.port); // 打印本地套接字端口 if (server.sofd &gt; 0) redisLog(REDIS_NOTICE, \"The server is now ready to accept connections at %s\", server.unixsocket); &#125; else &#123; sentinelIsRunning(); &#125; /* Warning the user about suspicious maxmemory setting. */ // 检查不正常的 maxmemory 配置 if (server.maxmemory &gt; 0 &amp;&amp; server.maxmemory &lt; 1024 * 1024) &#123; redisLog(REDIS_WARNING, \"WARNING: You specified a maxmemory value that is less than 1MB (current value is %llu bytes). Are you sure this is what you really want?\", server.maxmemory); &#125; // 运行事件处理器，一直到服务器关闭为止 aeSetBeforeSleepProc(server.el, beforeSleep); aeMain(server.el); // 服务器关闭，停止事件循环 aeDeleteEventLoop(server.el); return 0;&#125; 1. 设置各种东西123456setlocale(LC_COLLATE, \"\"); // localezmalloc_enable_thread_safeness(); // zmalloc thread safezmalloc_set_oom_handler(redisOutOfMemoryHandler); // 这个 handler 只是 log，基本啥也不干srand(time(NULL) ^ getpid()); // 生成时间随机数种子，用于 hash 的种子gettimeofday(&amp;tv, NULL);dictSetHashFunctionSeed(tv.tv_sec ^ tv.tv_usec ^ getpid());2. 初始化服务器123initServerConfig();... initServerConfig 实在太长，还是直接看源码吧。 但是初始化 command 的部分有点意思，看一下： 12345678910server.commands = dictCreate(&amp;commandTableDictType, NULL);server.orig_commands = dictCreate(&amp;commandTableDictType, NULL);populateCommandTable();server.delCommand = lookupCommandByCString(\"del\");server.multiCommand = lookupCommandByCString(\"multi\");server.lpushCommand = lookupCommandByCString(\"lpush\");server.lpopCommand = lookupCommandByCString(\"lpop\");server.rpopCommand = lookupCommandByCString(\"rpop\"); 这里又分为 3 个部分： - 首先创建两个字典 server.commands 和 server.orig_commands ，之所以要有两个的原因是，客户端可以通过 rename 操作把一些命令改个名字，所以要用 orig_commands 备份一份 - 然后初始化命令列表 populateCommandTable() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237struct redisCommand redisCommandTable[] = &#123;&#123;\"get\", getCommand, 2, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"set\", setCommand, -3, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"setnx\", setnxCommand, 3, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"setex\", setexCommand, 4, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"psetex\", psetexCommand, 4, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"append\", appendCommand, 3, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"strlen\", strlenCommand, 2, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"del\", delCommand, -2, \"w\", 0, NULL, 1, -1, 1, 0, 0&#125;,&#123;\"exists\", existsCommand, 2, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"setbit\", setbitCommand, 4, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"getbit\", getbitCommand, 3, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"setrange\", setrangeCommand, 4, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"getrange\", getrangeCommand, 4, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"substr\", getrangeCommand, 4, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"incr\", incrCommand, 2, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"decr\", decrCommand, 2, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"mget\", mgetCommand, -2, \"r\", 0, NULL, 1, -1, 1, 0, 0&#125;,&#123;\"rpush\", rpushCommand, -3, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"lpush\", lpushCommand, -3, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"rpushx\", rpushxCommand, 3, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"lpushx\", lpushxCommand, 3, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"linsert\", linsertCommand, 5, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"rpop\", rpopCommand, 2, \"w\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"lpop\", lpopCommand, 2, \"w\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"brpop\", brpopCommand, -3, \"ws\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"brpoplpush\", brpoplpushCommand, 4, \"wms\", 0, NULL, 1, 2, 1, 0, 0&#125;,&#123;\"blpop\", blpopCommand, -3, \"ws\", 0, NULL, 1, -2, 1, 0, 0&#125;,&#123;\"llen\", llenCommand, 2, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"lindex\", lindexCommand, 3, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"lset\", lsetCommand, 4, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"lrange\", lrangeCommand, 4, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"ltrim\", ltrimCommand, 4, \"w\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"lrem\", lremCommand, 4, \"w\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"rpoplpush\", rpoplpushCommand, 3, \"wm\", 0, NULL, 1, 2, 1, 0, 0&#125;,&#123;\"sadd\", saddCommand, -3, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"srem\", sremCommand, -3, \"w\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"smove\", smoveCommand, 4, \"w\", 0, NULL, 1, 2, 1, 0, 0&#125;,&#123;\"sismember\", sismemberCommand, 3, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"scard\", scardCommand, 2, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"spop\", spopCommand, 2, \"wRs\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"srandmember\", srandmemberCommand, -2, \"rR\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"sinter\", sinterCommand, -2, \"rS\", 0, NULL, 1, -1, 1, 0, 0&#125;,&#123;\"sinterstore\", sinterstoreCommand, -3, \"wm\", 0, NULL, 1, -1, 1, 0, 0&#125;,&#123;\"sunion\", sunionCommand, -2, \"rS\", 0, NULL, 1, -1, 1, 0, 0&#125;,&#123;\"sunionstore\", sunionstoreCommand, -3, \"wm\", 0, NULL, 1, -1, 1, 0, 0&#125;,&#123;\"sdiff\", sdiffCommand, -2, \"rS\", 0, NULL, 1, -1, 1, 0, 0&#125;,&#123;\"sdiffstore\", sdiffstoreCommand, -3, \"wm\", 0, NULL, 1, -1, 1, 0, 0&#125;,&#123;\"smembers\", sinterCommand, 2, \"rS\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"sscan\", sscanCommand, -3, \"rR\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zadd\", zaddCommand, -4, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zincrby\", zincrbyCommand, 4, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zrem\", zremCommand, -3, \"w\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zremrangebyscore\", zremrangebyscoreCommand, 4, \"w\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zremrangebyrank\", zremrangebyrankCommand, 4, \"w\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zremrangebylex\", zremrangebylexCommand, 4, \"w\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zunionstore\", zunionstoreCommand, -4, \"wm\", 0, zunionInterGetKeys, 0, 0, 0, 0, 0&#125;,&#123;\"zinterstore\", zinterstoreCommand, -4, \"wm\", 0, zunionInterGetKeys, 0, 0, 0, 0, 0&#125;,&#123;\"zrange\", zrangeCommand, -4, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zrangebyscore\", zrangebyscoreCommand, -4, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zrevrangebyscore\", zrevrangebyscoreCommand, -4, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zrangebylex\", zrangebylexCommand, -4, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zrevrangebylex\", zrevrangebylexCommand, -4, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zcount\", zcountCommand, 4, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zlexcount\", zlexcountCommand, 4, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zrevrange\", zrevrangeCommand, -4, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zcard\", zcardCommand, 2, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zscore\", zscoreCommand, 3, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zrank\", zrankCommand, 3, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zrevrank\", zrevrankCommand, 3, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"zscan\", zscanCommand, -3, \"rR\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"hset\", hsetCommand, 4, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"hsetnx\", hsetnxCommand, 4, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"hget\", hgetCommand, 3, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"hmset\", hmsetCommand, -4, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"hmget\", hmgetCommand, -3, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"hincrby\", hincrbyCommand, 4, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"hincrbyfloat\", hincrbyfloatCommand, 4, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"hdel\", hdelCommand, -3, \"w\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"hlen\", hlenCommand, 2, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"hkeys\", hkeysCommand, 2, \"rS\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"hvals\", hvalsCommand, 2, \"rS\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"hgetall\", hgetallCommand, 2, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"hexists\", hexistsCommand, 3, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"hscan\", hscanCommand, -3, \"rR\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"incrby\", incrbyCommand, 3, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"decrby\", decrbyCommand, 3, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"incrbyfloat\", incrbyfloatCommand, 3, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"getset\", getsetCommand, 3, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"mset\", msetCommand, -3, \"wm\", 0, NULL, 1, -1, 2, 0, 0&#125;,&#123;\"msetnx\", msetnxCommand, -3, \"wm\", 0, NULL, 1, -1, 2, 0, 0&#125;,&#123;\"randomkey\", randomkeyCommand, 1, \"rR\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"select\", selectCommand, 2, \"rl\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"move\", moveCommand, 3, \"w\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"rename\", renameCommand, 3, \"w\", 0, NULL, 1, 2, 1, 0, 0&#125;,&#123;\"renamenx\", renamenxCommand, 3, \"w\", 0, NULL, 1, 2, 1, 0, 0&#125;,&#123;\"expire\", expireCommand, 3, \"w\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"expireat\", expireatCommand, 3, \"w\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"pexpire\", pexpireCommand, 3, \"w\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"pexpireat\", pexpireatCommand, 3, \"w\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"keys\", keysCommand, 2, \"rS\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"scan\", scanCommand, -2, \"rR\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"dbsize\", dbsizeCommand, 1, \"r\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"auth\", authCommand, 2, \"rslt\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"ping\", pingCommand, 1, \"rt\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"echo\", echoCommand, 2, \"r\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"save\", saveCommand, 1, \"ars\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"bgsave\", bgsaveCommand, 1, \"ar\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"bgrewriteaof\", bgrewriteaofCommand, 1, \"ar\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"shutdown\", shutdownCommand, -1, \"arlt\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"lastsave\", lastsaveCommand, 1, \"rR\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"type\", typeCommand, 2, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"multi\", multiCommand, 1, \"rs\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"exec\", execCommand, 1, \"sM\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"discard\", discardCommand, 1, \"rs\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"sync\", syncCommand, 1, \"ars\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"psync\", syncCommand, 3, \"ars\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"replconf\", replconfCommand, -1, \"arslt\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"flushdb\", flushdbCommand, 1, \"w\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"flushall\", flushallCommand, 1, \"w\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"sort\", sortCommand, -2, \"wm\", 0, sortGetKeys, 1, 1, 1, 0, 0&#125;,&#123;\"info\", infoCommand, -1, \"rlt\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"monitor\", monitorCommand, 1, \"ars\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"ttl\", ttlCommand, 2, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"pttl\", pttlCommand, 2, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"persist\", persistCommand, 2, \"w\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"slaveof\", slaveofCommand, 3, \"ast\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"debug\", debugCommand, -2, \"as\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"config\", configCommand, -2, \"art\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"subscribe\", subscribeCommand, -2, \"rpslt\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"unsubscribe\", unsubscribeCommand, -1, \"rpslt\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"psubscribe\", psubscribeCommand, -2, \"rpslt\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"punsubscribe\", punsubscribeCommand, -1, \"rpslt\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"publish\", publishCommand, 3, \"pltr\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"pubsub\", pubsubCommand, -2, \"pltrR\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"watch\", watchCommand, -2, \"rs\", 0, NULL, 1, -1, 1, 0, 0&#125;,&#123;\"unwatch\", unwatchCommand, 1, \"rs\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"cluster\", clusterCommand, -2, \"ar\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"restore\", restoreCommand, -4, \"awm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"restore-asking\", restoreCommand, -4, \"awmk\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"migrate\", migrateCommand, -6, \"aw\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"asking\", askingCommand, 1, \"r\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"readonly\", readonlyCommand, 1, \"r\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"readwrite\", readwriteCommand, 1, \"r\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"dump\", dumpCommand, 2, \"ar\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"object\", objectCommand, -2, \"r\", 0, NULL, 2, 2, 2, 0, 0&#125;,&#123;\"client\", clientCommand, -2, \"ar\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"eval\", evalCommand, -3, \"s\", 0, evalGetKeys, 0, 0, 0, 0, 0&#125;,&#123;\"evalsha\", evalShaCommand, -3, \"s\", 0, evalGetKeys, 0, 0, 0, 0, 0&#125;,&#123;\"slowlog\", slowlogCommand, -2, \"r\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"script\", scriptCommand, -2, \"ras\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"time\", timeCommand, 1, \"rR\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"bitop\", bitopCommand, -4, \"wm\", 0, NULL, 2, -1, 1, 0, 0&#125;,&#123;\"bitcount\", bitcountCommand, -2, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"bitpos\", bitposCommand, -3, \"r\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"wait\", waitCommand, 3, \"rs\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"pfselftest\", pfselftestCommand, 1, \"r\", 0, NULL, 0, 0, 0, 0, 0&#125;,&#123;\"pfadd\", pfaddCommand, -2, \"wm\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"pfcount\", pfcountCommand, -2, \"w\", 0, NULL, 1, 1, 1, 0, 0&#125;,&#123;\"pfmerge\", pfmergeCommand, -2, \"wm\", 0, NULL, 1, -1, 1, 0, 0&#125;,&#123;\"pfdebug\", pfdebugCommand, -3, \"w\", 0, NULL, 0, 0, 0, 0, 0&#125;&#125;;void populateCommandTable(void) &#123; int j; // 命令的数量 int numcommands = sizeof(redisCommandTable) / sizeof(struct redisCommand); for (j = 0; j &lt; numcommands; j++) &#123; // 指定命令 struct redisCommand *c = redisCommandTable + j; // 取出字符串 FLAG char *f = c-&gt;sflags; int retval1, retval2; // 根据字符串 FLAG 生成实际 FLAG while (*f != '\\0') &#123; switch (*f) &#123; case 'w': c-&gt;flags |= REDIS_CMD_WRITE; break; case 'r': c-&gt;flags |= REDIS_CMD_READONLY; break; case 'm': c-&gt;flags |= REDIS_CMD_DENYOOM; break; case 'a': c-&gt;flags |= REDIS_CMD_ADMIN; break; case 'p': c-&gt;flags |= REDIS_CMD_PUBSUB; break; case 's': c-&gt;flags |= REDIS_CMD_NOSCRIPT; break; case 'R': c-&gt;flags |= REDIS_CMD_RANDOM; break; case 'S': c-&gt;flags |= REDIS_CMD_SORT_FOR_SCRIPT; break; case 'l': c-&gt;flags |= REDIS_CMD_LOADING; break; case 't': c-&gt;flags |= REDIS_CMD_STALE; break; case 'M': c-&gt;flags |= REDIS_CMD_SKIP_MONITOR; break; case 'k': c-&gt;flags |= REDIS_CMD_ASKING; break; default: redisPanic(\"Unsupported command flag\"); break; &#125; f++; &#125; // 将命令关联到命令表 retval1 = dictAdd(server.commands, sdsnew(c-&gt;name), c); // 将命令也关联到原始命令表, 原始命令表不会受 redis.conf 中命令改名的影响 retval2 = dictAdd(server.orig_commands, sdsnew(c-&gt;name), c); redisAssert(retval1 == DICT_OK &amp;&amp; retval2 == DICT_OK); &#125;&#125; 前面那个常常的数组就是 redis 的所有命令，populateCommandTable 函数的作用是把这些命令放到 table 里面 - 最后把几个命令从 table 里查好记录一下，因为接下来 load config 的时候可能会用到 12345server.delCommand = lookupCommandByCString(\"del\");server.multiCommand = lookupCommandByCString(\"multi\");server.lpushCommand = lookupCommandByCString(\"lpush\");server.lpopCommand = lookupCommandByCString(\"lpop\");server.rpopCommand = lookupCommandByCString(\"rpop\");3. 如果服务器以 Sentinel 模式启动，那么进行 Sentinel 功能相关的初始化1234if (server.sentinel_mode) &#123; initSentinelConfig(); initSentinel();&#125; 4. 读命令行用户输入，进行配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364if (argc &gt;= 2) &#123; int j = 1; /* First option to parse in argv[] */ sds options = sdsempty(); char *configfile = NULL; /* Handle special options --help and --version */ // 处理特殊选项 -h 、-v 和 --test-memory if (strcmp(argv[1], \"-v\") == 0 || strcmp(argv[1], \"--version\") == 0) version(); if (strcmp(argv[1], \"--help\") == 0 || strcmp(argv[1], \"-h\") == 0) usage(); if (strcmp(argv[1], \"--test-memory\") == 0) &#123; if (argc == 3) &#123; memtest(atoi(argv[2]), 50); exit(0); &#125; else &#123; fprintf(stderr, \"Please specify the amount of memory to test in megabytes.\\n\"); fprintf(stderr, \"Example: ./redis-server --test-memory 4096\\n\\n\"); exit(1); &#125; &#125; /* First argument is the config file name? */ // 如果第一个参数（argv[1]）不是以 \"--\" 开头 // 那么它应该是一个配置文件 if (argv[j][0] != '-' || argv[j][1] != '-') configfile = argv[j++]; /* All the other options are parsed and conceptually appended to the * configuration file. For instance --port 6380 will generate the * string \"port 6380\\n\" to be parsed after the actual file name * is parsed, if any. */ // 对用户给定的其余选项进行分析，并将分析所得的字符串追加稍后载入的配置文件的内容之后 // 比如 --port 6380 会被分析为 \"port 6380\\n\" while (j != argc) &#123; if (argv[j][0] == '-' &amp;&amp; argv[j][1] == '-') &#123; /* Option name */ if (sdslen(options)) options = sdscat(options, \"\\n\"); options = sdscat(options, argv[j] + 2); options = sdscat(options, \" \"); &#125; else &#123; /* Option argument */ options = sdscatrepr(options, argv[j], strlen(argv[j])); options = sdscat(options, \" \"); &#125; j++; &#125; if (configfile) server.configfile = getAbsolutePath(configfile); // 重置保存条件 resetServerSaveParams(); // 载入配置文件， options 是前面分析出的给定选项 loadServerConfig(configfile, options); sdsfree(options); // 获取配置文件的绝对路径 if (configfile) server.configfile = getAbsolutePath(configfile);&#125; else &#123; redisLog(REDIS_WARNING, \"Warning: no config file specified, using the default config. In order to specify a config file use %s /path/to/%s.conf\", argv[0], server.sentinel_mode ? \"sentinel\" : \"redis\");&#125; 如果没指定 argv 参数，那么就 redisLog 一下就完了，重点看有参数的情况处理 几种特殊情况，直接干完以后 exit 1234567891011121314151617// 处理特殊选项 -h 、-v 和 --test-memoryif (strcmp(argv[1], \"-v\") == 0 || strcmp(argv[1], \"--version\") == 0) &#123; version();&#125;if (strcmp(argv[1], \"--help\") == 0 || strcmp(argv[1], \"-h\") == 0) &#123; usage();&#125;if (strcmp(argv[1], \"--test-memory\") == 0) &#123; if (argc == 3) &#123; memtest(atoi(argv[2]), 50); exit(0); &#125; else &#123; fprintf(stderr, \"Please specify the amount of memory to test in megabytes.\\n\"); fprintf(stderr, \"Example: ./redis-server --test-memory 4096\\n\\n\"); exit(1); &#125;&#125; 接下来看看怎么解析的 12345int j = 1; /* First option to parse in argv[] */sds options = sdsempty();char *configfile = NULL;... 如果第一个参数（argv[1]）不是以 “–” 开头, 那么它应该是一个配置文件 123if (argv[j][0] != '-' || argv[j][1] != '-') &#123; configfile = argv[j++];&#125; 接下来命令中的参数解析一下，拼到 options 后面 1234567891011121314151617// 对用户给定的其余选项进行分析，并将分析所得的字符串追加稍后载入的配置文件的内容之后// 比如 --port 6380 会被分析为 \"port 6380\\n\"while (j != argc) &#123; if (argv[j][0] == '-' &amp;&amp; argv[j][1] == '-') &#123; /* Option name */ if (sdslen(options)) &#123; options = sdscat(options, \"\\n\"); &#125; options = sdscat(options, argv[j] + 2); options = sdscat(options, \" \"); &#125; else &#123; /* Option argument */ options = sdscatrepr(options, argv[j], strlen(argv[j])); options = sdscat(options, \" \"); &#125; j++;&#125; 然后如果有 config file 的话，把它打开，然后再把内容拼到字符串里面，再逐行拿出来配置 redis server 的参数 12&#x2F;&#x2F; 载入配置文件， options 是前面分析出的给定选项loadServerConfig(configfile, options); 5. 将服务器设置为守护进程fork 出一个子进程，然后把所有标准 io 重定向到 /dev/null，接下来的所有操作都在子进程里面了 123456789101112131415161718192021void daemonize(void) &#123; int fd; if (fork() != 0) &#123; exit(0); /* parent exits */ &#125; setsid(); /* create a new session */ /* Every output goes to /dev/null. If Redis is daemonized but * the 'logfile' is set to 'stdout' in the configuration file * it will not log at all. */ if ((fd = open(\"/dev/null\", O_RDWR, 0)) != -1) &#123; dup2(fd, STDIN_FILENO); dup2(fd, STDOUT_FILENO); dup2(fd, STDERR_FILENO); if (fd &gt; STDERR_FILENO) &#123; close(fd); &#125; &#125;&#125; 6. 创建并初始化服务器数据结构123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163void initServer() &#123; int j; // 设置信号处理函数 signal(SIGHUP, SIG_IGN); signal(SIGPIPE, SIG_IGN); setupSignalHandlers(); // 设置 syslog if (server.syslog_enabled) &#123; openlog(server.syslog_ident, LOG_PID | LOG_NDELAY | LOG_NOWAIT, server.syslog_facility); &#125; // 初始化并创建数据结构 server.current_client = NULL; server.clients = listCreate(); server.clients_to_close = listCreate(); server.slaves = listCreate(); server.monitors = listCreate(); server.slaveseldb = -1; /* Force to emit the first SELECT command. */ server.unblocked_clients = listCreate(); server.ready_keys = listCreate(); server.clients_waiting_acks = listCreate(); server.get_ack_from_slaves = 0; server.clients_paused = 0; // 创建共享对象 createSharedObjects(); adjustOpenFilesLimit(); server.el = aeCreateEventLoop(server.maxclients + REDIS_EVENTLOOP_FDSET_INCR); server.db = zmalloc(sizeof(redisDb) * server.dbnum); /* Open the TCP listening socket for the user commands. */ // 打开 TCP 监听端口，用于等待客户端的命令请求 if (server.port != 0 &amp;&amp; listenToPort(server.port, server.ipfd, &amp;server.ipfd_count) == REDIS_ERR) exit(1); /* Open the listening Unix domain socket. */ // 打开 UNIX 本地端口 if (server.unixsocket != NULL) &#123; unlink(server.unixsocket); /* don't care if this fails */ server.sofd = anetUnixServer(server.neterr, server.unixsocket, server.unixsocketperm, server.tcp_backlog); if (server.sofd == ANET_ERR) &#123; redisLog(REDIS_WARNING, \"Opening socket: %s\", server.neterr); exit(1); &#125; anetNonBlock(NULL, server.sofd); &#125; /* Abort if there are no listening sockets at all. */ if (server.ipfd_count == 0 &amp;&amp; server.sofd &lt; 0) &#123; redisLog(REDIS_WARNING, \"Configured to not listen anywhere, exiting.\"); exit(1); &#125; /* Create the Redis databases, and initialize other internal state. */ // 创建并初始化数据库结构 for (j = 0; j &lt; server.dbnum; j++) &#123; server.db[j].dict = dictCreate(&amp;dbDictType, NULL); server.db[j].expires = dictCreate(&amp;keyptrDictType, NULL); server.db[j].blocking_keys = dictCreate(&amp;keylistDictType, NULL); server.db[j].ready_keys = dictCreate(&amp;setDictType, NULL); server.db[j].watched_keys = dictCreate(&amp;keylistDictType, NULL); server.db[j].eviction_pool = evictionPoolAlloc(); server.db[j].id = j; server.db[j].avg_ttl = 0; &#125; // 创建 PUBSUB 相关结构 server.pubsub_channels = dictCreate(&amp;keylistDictType, NULL); server.pubsub_patterns = listCreate(); listSetFreeMethod(server.pubsub_patterns, freePubsubPattern); listSetMatchMethod(server.pubsub_patterns, listMatchPubsubPattern); server.cronloops = 0; server.rdb_child_pid = -1; server.aof_child_pid = -1; aofRewriteBufferReset(); server.aof_buf = sdsempty(); server.lastsave = time(NULL); /* At startup we consider the DB saved. */ server.lastbgsave_try = 0; /* At startup we never tried to BGSAVE. */ server.rdb_save_time_last = -1; server.rdb_save_time_start = -1; server.dirty = 0; resetServerStats(); /* A few stats we don't want to reset: server startup time, and peak mem. */ server.stat_starttime = time(NULL); server.stat_peak_memory = 0; server.resident_set_size = 0; server.lastbgsave_status = REDIS_OK; server.aof_last_write_status = REDIS_OK; server.aof_last_write_errno = 0; server.repl_good_slaves_count = 0; updateCachedTime(); /* Create the serverCron() time event, that's our main way to process * background operations. */ // 为 serverCron() 创建时间事件 if (aeCreateTimeEvent(server.el, 1, serverCron, NULL, NULL) == AE_ERR) &#123; redisPanic(\"Can't create the serverCron time event.\"); exit(1); &#125; /* Create an event handler for accepting new connections in TCP and Unix * domain sockets. */ // 为 TCP 连接关联连接应答（accept）处理器 // 用于接受并应答客户端的 connect() 调用 for (j = 0; j &lt; server.ipfd_count; j++) &#123; if (aeCreateFileEvent(server.el, server.ipfd[j], AE_READABLE, acceptTcpHandler, NULL) == AE_ERR) &#123; redisPanic( \"Unrecoverable error creating server.ipfd file event.\"); &#125; &#125; // 为本地套接字关联应答处理器 if (server.sofd &gt; 0 &amp;&amp; aeCreateFileEvent(server.el, server.sofd, AE_READABLE, acceptUnixHandler, NULL) == AE_ERR) redisPanic(\"Unrecoverable error creating server.sofd file event.\"); /* Open the AOF file if needed. */ // 如果 AOF 持久化功能已经打开，那么打开或创建一个 AOF 文件 if (server.aof_state == REDIS_AOF_ON) &#123; server.aof_fd = open(server.aof_filename, O_WRONLY | O_APPEND | O_CREAT, 0644); if (server.aof_fd == -1) &#123; redisLog(REDIS_WARNING, \"Can't open the append-only file: %s\", strerror(errno)); exit(1); &#125; &#125; /* 32 bit instances are limited to 4GB of address space, so if there is * no explicit limit in the user provided configuration we set a limit * at 3 GB using maxmemory with 'noeviction' policy'. This avoids * useless crashes of the Redis instance for out of memory. */ // 对于 32 位实例来说，默认将最大可用内存限制在 3 GB if (server.arch_bits == 32 &amp;&amp; server.maxmemory == 0) &#123; redisLog(REDIS_WARNING, \"Warning: 32 bit instance detected but no memory limit set. Setting 3 GB maxmemory limit with 'noeviction' policy now.\"); server.maxmemory = 3072LL * (1024 * 1024); /* 3 GB */ server.maxmemory_policy = REDIS_MAXMEMORY_NO_EVICTION; &#125; // 如果服务器以 cluster 模式打开，那么初始化 cluster if (server.cluster_enabled) clusterInit(); // 初始化复制功能有关的脚本缓存 replicationScriptCacheInit(); // 初始化脚本系统 scriptingInit(); // 初始化慢查询功能 slowlogInit(); // 初始化 BIO 系统 bioInit();&#125; 这个函数很长，分段看一下。 设置各种信号处理函数 12345678910111213141516171819202122232425262728293031323334353637signal(SIGHUP, SIG_IGN);signal(SIGPIPE, SIG_IGN);setupSignalHandlers();void setupSignalHandlers(void) &#123; struct sigaction act; sigemptyset(&amp;act.sa_mask); act.sa_flags = 0; act.sa_handler = sigtermHandler; sigaction(SIGTERM, &amp;act, NULL);#ifdef HAVE_BACKTRACE sigemptyset(&amp;act.sa_mask); act.sa_flags = SA_NODEFER | SA_RESETHAND | SA_SIGINFO; act.sa_sigaction = sigsegvHandler; sigaction(SIGSEGV, &amp;act, NULL); sigaction(SIGBUS, &amp;act, NULL); sigaction(SIGFPE, &amp;act, NULL); sigaction(SIGILL, &amp;act, NULL);#endif return;&#125;// SIGTERM 信号的处理器static void sigtermHandler(int sig) &#123; REDIS_NOTUSED(sig); redisLogFromHandler(REDIS_WARNING, \"Received SIGTERM, scheduling shutdown...\"); // 打开关闭标识 server.shutdown_asap = 1;&#125; 设置 syslog 1234if (server.syslog_enabled) &#123; openlog(server.syslog_ident, LOG_PID | LOG_NDELAY | LOG_NOWAIT, server.syslog_facility);&#125; 初始化并创建数据结构 1234567891011server.current_client = NULL;server.clients = listCreate();server.clients_to_close = listCreate();server.slaves = listCreate();server.monitors = listCreate();server.slaveseldb = -1; /* Force to emit the first SELECT command. */server.unblocked_clients = listCreate();server.ready_keys = listCreate();server.clients_waiting_acks = listCreate();server.get_ack_from_slaves = 0;server.clients_paused = 0; 创建共享对象 1createSharedObjects(); 这个函数里面把 struct sharedObjectsStruct shared 构造好，这个结构体包含了常见的命令回复 123456789101112131415// 通过复用来减少内存碎片，以及减少操作耗时的共享对象struct sharedObjectsStruct &#123; robj *crlf, *ok, *err, *emptybulk, *czero, *cone, *cnegone, *pong, *space, *colon, *nullbulk, *nullmultibulk, *queued, *emptymultibulk, *wrongtypeerr, *nokeyerr, *syntaxerr, *sameobjecterr, *outofrangeerr, *noscripterr, *loadingerr, *slowscripterr, *bgsaveerr, *masterdownerr, *roslaveerr, *execaborterr, *noautherr, *noreplicaserr, *busykeyerr, *oomerr, *plus, *messagebulk, *pmessagebulk, *subscribebulk, *unsubscribebulk, *psubscribebulk, *punsubscribebulk, *del, *rpop, *lpop, *lpush, *emptyscan, *minstring, *maxstring, *select[REDIS_SHARED_SELECT_CMDS], *integers[REDIS_SHARED_INTEGERS], *mbulkhdr[REDIS_SHARED_BULKHDR_LEN], /* \"*&lt;value&gt;\\r\\n\" */ *bulkhdr[REDIS_SHARED_BULKHDR_LEN]; /* \"$&lt;value&gt;\\r\\n\" */&#125;; 初始化事件处理器状态 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253server.el = aeCreateEventLoop(server.maxclients + REDIS_EVENTLOOP_FDSET_INCR);aeEventLoop *aeCreateEventLoop(int setsize) &#123; aeEventLoop *eventLoop; int i; // 创建事件状态结构 if ((eventLoop = zmalloc(sizeof(*eventLoop))) == NULL) &#123; goto err; &#125; // 初始化文件事件结构和已就绪文件事件结构数组 eventLoop-&gt;events = zmalloc(sizeof(aeFileEvent) * setsize); eventLoop-&gt;fired = zmalloc(sizeof(aeFiredEvent) * setsize); if (eventLoop-&gt;events == NULL || eventLoop-&gt;fired == NULL) &#123; goto err; &#125; // 设置数组大小 eventLoop-&gt;setsize = setsize; // 初始化执行最近一次执行时间 eventLoop-&gt;lastTime = time(NULL); // 初始化时间事件结构 eventLoop-&gt;timeEventHead = NULL; eventLoop-&gt;timeEventNextId = 0; eventLoop-&gt;stop = 0; eventLoop-&gt;maxfd = -1; eventLoop-&gt;beforesleep = NULL; if (aeApiCreate(eventLoop) == -1) &#123; goto err; &#125; // 初始化监听事件 for (i = 0; i &lt; setsize; i++) &#123; eventLoop-&gt;events[i].mask = AE_NONE; &#125; // 返回事件循环 return eventLoop; err: if (eventLoop) &#123; zfree(eventLoop-&gt;events); zfree(eventLoop-&gt;fired); zfree(eventLoop); &#125; return NULL;&#125; 这个方法之前看过，底层是 IO 多路复用库，创建一个 eventloop 来监听端口 打开 TCP 监听端口，用于等待客户端的命令请求 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849listenToPort(server.port, server.ipfd, &amp;server.ipfd_count);int listenToPort(int port, int *fds, int *count) &#123; int j; if (server.bindaddr_count == 0) server.bindaddr[0] = NULL; for (j = 0; j &lt; server.bindaddr_count || j == 0; j++) &#123; if (server.bindaddr[j] == NULL) &#123; fds[*count] = anetTcp6Server(server.neterr, port, NULL, server.tcp_backlog); if (fds[*count] != ANET_ERR) &#123; anetNonBlock(NULL, fds[*count]); (*count)++; &#125; fds[*count] = anetTcpServer(server.neterr, port, NULL, server.tcp_backlog); if (fds[*count] != ANET_ERR) &#123; anetNonBlock(NULL, fds[*count]); (*count)++; &#125; if (*count) break; &#125; else if (strchr(server.bindaddr[j], ':')) &#123; fds[*count] = anetTcp6Server(server.neterr, port, server.bindaddr[j], server.tcp_backlog); &#125; else &#123; fds[*count] = anetTcpServer(server.neterr, port, server.bindaddr[j], server.tcp_backlog); &#125; if (fds[*count] == ANET_ERR) &#123; redisLog(REDIS_WARNING, \"Creating Server TCP listening socket %s:%d: %s\", server.bindaddr[j] ? server.bindaddr[j] : \"*\", port, server.neterr); return REDIS_ERR; &#125; anetNonBlock(NULL, fds[*count]); (*count)++; &#125; return REDIS_OK;&#125; 这个方法初始化一组文件描述符以侦听指定的“端口”，绑定Redis服务器配置中指定的地址，并设置为非阻塞。 打开 UNIX 本地端口 12345678910if (server.unixsocket != NULL) &#123; unlink(server.unixsocket); /* don't care if this fails */ server.sofd = anetUnixServer(server.neterr, server.unixsocket, server.unixsocketperm, server.tcp_backlog); if (server.sofd == ANET_ERR) &#123; redisLog(REDIS_WARNING, \"Opening socket: %s\", server.neterr); exit(1); &#125; anetNonBlock(NULL, server.sofd);&#125; 创建并初始化数据库结构 12345678910for (j = 0; j &lt; server.dbnum; j++) &#123; server.db[j].dict = dictCreate(&amp;dbDictType, NULL); server.db[j].expires = dictCreate(&amp;keyptrDictType, NULL); server.db[j].blocking_keys = dictCreate(&amp;keylistDictType, NULL); server.db[j].ready_keys = dictCreate(&amp;setDictType, NULL); server.db[j].watched_keys = dictCreate(&amp;keylistDictType, NULL); server.db[j].eviction_pool = evictionPoolAlloc(); server.db[j].id = j; server.db[j].avg_ttl = 0;&#125; 创建时间事件 1aeCreateTimeEvent(server.el, 1, serverCron, NULL, NULL); 注意到这里周期是 1ms，靠的是后面的 run_with_period 来控制具体周期 这里面的重点是 serverCron 函数，这个函数相当长，看看 run_with_period 挺好的： 12345run_with_period(100) &#123; trackOperationsPerSecond();&#125;#define run_with_period(_ms_) if ((_ms_ &lt;= 1000/server.hz) || !(server.cronloops%((_ms_)/(1000/server.hz)))) run_with_period 本身是一个宏，server.cronloops 每转一圈会自增。 为 TCP 连接关联连接应答（accept）处理器，用于接受并应答客户端的 connect() 调用 1234567for (j = 0; j &lt; server.ipfd_count; j++) &#123; if (aeCreateFileEvent(server.el, server.ipfd[j], AE_READABLE, acceptTcpHandler, NULL) == AE_ERR) &#123; redisPanic( \"Unrecoverable error creating server.ipfd file event.\"); &#125;&#125; 为本地套接字关联应答处理器 注：例如 lua 脚本需要伪客户端 12345if (server.sofd &gt; 0 &amp;&amp; aeCreateFileEvent( server.el, server.sofd, AE_READABLE, acceptUnixHandler, NULL) == AE_ERR) &#123; redisPanic(\"Unrecoverable error creating server.sofd file event.\");&#125; 如果 AOF 持久化功能已经打开，那么打开或创建一个 AOF 文件 123456789if (server.aof_state == REDIS_AOF_ON) &#123; server.aof_fd = open(server.aof_filename, O_WRONLY | O_APPEND | O_CREAT, 0644); if (server.aof_fd == -1) &#123; redisLog(REDIS_WARNING, \"Can't open the append-only file: %s\", strerror(errno)); exit(1); &#125;&#125; 集群、复制功能相关 1234567891011121314151617181920if (server.arch_bits == 32 &amp;&amp; server.maxmemory == 0) &#123; redisLog(REDIS_WARNING, \"Warning: 32 bit instance detected but no memory limit set. Setting 3 GB maxmemory limit with 'noeviction' policy now.\"); server.maxmemory = 3072LL * (1024 * 1024); /* 3 GB */ server.maxmemory_policy = REDIS_MAXMEMORY_NO_EVICTION;&#125;// 如果服务器以 cluster 模式打开，那么初始化 clusterif (server.cluster_enabled) &#123; clusterInit();&#125;// 初始化复制功能有关的脚本缓存replicationScriptCacheInit();// 初始化脚本系统scriptingInit();// 初始化慢查询功能slowlogInit(); 初始化 BIO 系统 1bioInit(); 这个函数是用于初始化线程和临界区 12345678910111213141516171819202122232425262728293031323334353637383940pthread_attr_t attr;pthread_t thread;size_t stacksize;int j;/* Initialization of state vars and objects * * 初始化 job 队列，以及线程状态 */for (j = 0; j &lt; REDIS_BIO_NUM_OPS; j++) &#123; pthread_mutex_init(&amp;bio_mutex[j],NULL); pthread_cond_init(&amp;bio_condvar[j],NULL); bio_jobs[j] = listCreate(); bio_pending[j] = 0;&#125;/* Set the stack size as by default it may be small in some system * * 设置栈大小 */pthread_attr_init(&amp;attr);pthread_attr_getstacksize(&amp;attr,&amp;stacksize);if (!stacksize) stacksize = 1; /* The world is full of Solaris Fixes */while (stacksize &lt; REDIS_THREAD_STACK_SIZE) stacksize *= 2;pthread_attr_setstacksize(&amp;attr, stacksize);/* Ready to spawn our threads. We use the single argument the thread * function accepts in order to pass the job ID the thread is * responsible of. * * 创建线程 */for (j = 0; j &lt; REDIS_BIO_NUM_OPS; j++) &#123; void *arg = (void*)(unsigned long) j; if (pthread_create(&amp;thread,&amp;attr,bioProcessBackgroundJobs,arg) != 0) &#123; redisLog(REDIS_WARNING,\"Fatal: Can't initialize Background Jobs.\"); exit(1); &#125; bio_threads[j] = thread;&#125; 所以，Redis 不是完全的单线程，会开两个后台线程： 后台线程的作用是 aof 的 fsync 系统调用 文件的最后一个 owner 对文件进行 close 实现方式是临界区放一个 list，生产者-消费者模式 7. 如果服务器是守护进程，那么创建 PID 文件123456789101112if (server.daemonize) &#123; createPidFile();&#125;void createPidFile(void) &#123; /* Try to write the pid file in a best-effort way. */ FILE *fp = fopen(server.pidfile, \"w\"); if (fp) &#123; fprintf(fp, \"%d\\n\", (int) getpid()); fclose(fp); &#125;&#125; 8. 为服务器进程设置名字1234567891011121314151617redisSetProcTitle(argv[0]);void redisSetProcTitle(char *title) &#123;#ifdef USE_SETPROCTITLE char *server_mode = \"\"; if (server.cluster_enabled) server_mode = \" [cluster]\"; else if (server.sentinel_mode) server_mode = \" [sentinel]\"; setproctitle(\"%s %s:%d%s\", title, server.bindaddr_count ? server.bindaddr[0] : \"*\", server.port, server_mode);#else REDIS_NOTUSED(title);#endif&#125; 9. 打印 ASCII LOGO1234567891011121314151617181920212223242526void redisAsciiArt(void) &#123;#include \"asciilogo.h\" char *buf = zmalloc(1024 * 16); char *mode = \"stand alone\"; if (server.cluster_enabled) &#123; mode = \"cluster\"; &#125; else if (server.sentinel_mode) &#123; mode = \"sentinel\"; &#125; snprintf(buf, 1024 * 16, ascii_logo, REDIS_VERSION, redisGitSHA1(), strtol(redisGitDirty(), NULL, 10) &gt; 0, (sizeof(long) == 8) ? \"64\" : \"32\", mode, server.port, (long) getpid() ); redisLogRaw(REDIS_NOTICE | REDIS_LOG_RAW, buf); zfree(buf);&#125; 10. 载入 rdb 和 aof 文件1loadDataFromDisk(); 这里面分为 2 种， aof 和 rdb 123456789101112131415161718192021222324void loadDataFromDisk(void) &#123; // 记录开始时间 long long start = ustime(); // AOF 持久化已打开？ if (server.aof_state == REDIS_AOF_ON) &#123; // 尝试载入 AOF 文件 if (loadAppendOnlyFile(server.aof_filename) == REDIS_OK) // 打印载入信息，并计算载入耗时长度 redisLog(REDIS_NOTICE, \"DB loaded from append only file: %.3f seconds\", (float) (ustime() - start) / 1000000); // AOF 持久化未打开 &#125; else &#123; // 尝试载入 RDB 文件 if (rdbLoad(server.rdb_filename) == REDIS_OK) &#123; // 打印载入信息，并计算载入耗时长度 redisLog(REDIS_NOTICE, \"DB loaded from disk: %.3f seconds\", (float) (ustime() - start) / 1000000); &#125; else if (errno != ENOENT) &#123; redisLog(REDIS_WARNING, \"Fatal error loading the DB: %s. Exiting.\", strerror(errno)); exit(1); &#125; &#125;&#125; 11. 设置 before sleep 方法1aeSetBeforeSleepProc(server.el, beforeSleep); 这个方法是每一圈 loop 之前执行的方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546void beforeSleep(struct aeEventLoop *eventLoop) &#123; REDIS_NOTUSED(eventLoop); /* Run a fast expire cycle (the called function will return * ASAP if a fast cycle is not needed). */ // 执行一次快速的主动过期检查 if (server.active_expire_enabled &amp;&amp; server.masterhost == NULL) activeExpireCycle(ACTIVE_EXPIRE_CYCLE_FAST); /* Send all the slaves an ACK request if at least one client blocked * during the previous event loop iteration. */ if (server.get_ack_from_slaves) &#123; robj *argv[3]; argv[0] = createStringObject(\"REPLCONF\", 8); argv[1] = createStringObject(\"GETACK\", 6); argv[2] = createStringObject(\"*\", 1); /* Not used argument. */ replicationFeedSlaves(server.slaves, server.slaveseldb, argv, 3); decrRefCount(argv[0]); decrRefCount(argv[1]); decrRefCount(argv[2]); server.get_ack_from_slaves = 0; &#125; /* Unblock all the clients blocked for synchronous replication * in WAIT. */ if (listLength(server.clients_waiting_acks)) processClientsWaitingReplicas(); /* Try to process pending commands for clients that were just unblocked. */ if (listLength(server.unblocked_clients)) processUnblockedClients(); /* Write the AOF buffer on disk */ // 将 AOF 缓冲区的内容写入到 AOF 文件 flushAppendOnlyFile(0); /* Call the Redis Cluster before sleep function. */ // 在进入下个事件循环前，执行一些集群收尾工作 if (server.cluster_enabled) &#123; clusterBeforeSleep(); &#125;&#125; 其中一个重要的函数是 flushAppendOnlyFile ，用于将 AOF 缓冲区的内容写入到 AOF 文件。 12. 开始主 loop12345678910111213141516171819aeMain(server.el);/* * 事件处理器的主循环 */void aeMain(aeEventLoop *eventLoop) &#123; eventLoop-&gt;stop = 0; while (!eventLoop-&gt;stop) &#123; // 如果有需要在事件处理前执行的函数，那么运行它 if (eventLoop-&gt;beforesleep != NULL) eventLoop-&gt;beforesleep(eventLoop); // 开始处理事件 aeProcessEvents(eventLoop, AE_ALL_EVENTS); &#125;&#125; aeProcessEvents 方法之前看过，典型的 reactor 模式 13. 结束，清理 loopaeMain 是一个 while 循环，如果能执行到这里说明结束了，回收一下空间就行了 123456void aeDeleteEventLoop(aeEventLoop *eventLoop) &#123; aeApiFree(eventLoop); zfree(eventLoop-&gt;events); zfree(eventLoop-&gt;fired); zfree(eventLoop);&#125; 关于几个文件事件处理器的逻辑关系一共有 3 个： networking.c/acceptTcpHandler 函数是Redis的连接应答处理器，这个处理器用于对连接服务器监听套接字的客户端进行应答 networking.c/readQueryFromClient 函数是Redis的命令请求处理器，这个处理器负责从套接字中读入客户端发送的命令请求内容，具体实现为unistd.h/read函数的包装 networking.c/sendReplyToClient 函数是Redis的命令回复处理器，这个处理器负责将服务器执行命令后得到的命令回复通过套接字返回给客户端，具体实现为unistd.h/write函数的包装 它们的逻辑关系是这样的： 在 redis.c/initServer 中，redis server 的 main 方法在开始 EventLoop 之前，会进行一系列初始化，其中就包括了 initServer 一步，这个函数中的其中一个环节是开启 TCP 监听端口，注册了 acceptTcpHandler 到文件事件 123456789// 为 TCP 连接关联连接应答（accept）处理器// 用于接受并应答客户端的 connect() 调用for (j = 0; j &lt; server.ipfd_count; j++) &#123; if (aeCreateFileEvent(server.el, server.ipfd[j], AE_READABLE, acceptTcpHandler, NULL) == AE_ERR) &#123; redisPanic( \"Unrecoverable error creating server.ipfd file event.\"); &#125;&#125; 一旦客户端 connect ，就会进入 acceptTcpHandler 方法，这个方法里会 accept 客户端连接，如果成功会进入 acceptCommonHandler 方法 12345678910111213while (max--) &#123; // accept 客户端连接 cfd = anetTcpAccept(server.neterr, fd, cip, sizeof(cip), &amp;cport); if (cfd == ANET_ERR) &#123; if (errno != EWOULDBLOCK) redisLog(REDIS_WARNING, \"Accepting client connection: %s\", server.neterr); return; &#125; redisLog(REDIS_VERBOSE, \"Accepted %s:%d\", cip, cport); // 为客户端创建客户端状态（redisClient） acceptCommonHandler(cfd, 0);&#125; acceptCommonHandler 中会创建 redisClient 1234567if ((c = createClient(fd)) == NULL) &#123; redisLog(REDIS_WARNING, \"Error registering fd event for the new client: %s (fd=%d)\", strerror(errno), fd); close(fd); /* May be already closed, just ignore errors */ return;&#125; 在创建客户端的方法 createClient 中，会绑定读事件到事件 loop ，开始接收命令请求，并注册 readQueryFromClient 这一事件处理器 12345678910111213141516if (fd != -1) &#123; // 非阻塞 anetNonBlock(NULL, fd); // 禁用 Nagle 算法 anetEnableTcpNoDelay(NULL, fd); // 设置 keep alive if (server.tcpkeepalive) anetKeepAlive(NULL, fd, server.tcpkeepalive); // 绑定读事件到事件 loop （开始接收命令请求） if (aeCreateFileEvent(server.el, fd, AE_READABLE, readQueryFromClient, c) == AE_ERR) &#123; close(fd); zfree(c); return NULL; &#125;&#125; 一旦客户端的请求到达，就会进入 readQueryFromClient 这一事件处理器，这个处理器里会读 client 的查询 buffer 12345678910void readQueryFromClient(aeEventLoop *el, int fd, void *privdata, int mask) &#123; ... // 从查询缓存重读取内容，创建参数，并执行命令 // 函数会执行到缓存中的所有内容都被处理完为止 processInputBuffer(c); ...&#125; 这个 processInputBuffer 方法会对请求的信息进行解析，变成 command 形式，然后执行这个 command 12345678void processInputBuffer(redisClient *c) &#123; ... if (processCommand(c) == REDIS_OK) ...&#125; 执行完 command 以后，会有执行结果，这个结果最终会返回给客户端，但是要客户端变得 WRITABLE 才行，所以要先放到 redis client 的 buffer 里面，这个过程要进行一堆函数调用 processCommand -&gt; addReply -&gt; _addReplyToBuffer -&gt; memcpy(c-&gt;buf + c-&gt;bufpos, s, len); 在 addReply 的时候，会绑定写事件到事件 loop ，注册 sendReplyToClient 这一事件处理器，当 fd 变得可写时触发 addReply -&gt; prepareClientToWrite -&gt; aeCreateFileEvent 12345678910int prepareClientToWrite(redisClient *c) &#123; ... // 一般情况，为客户端套接字安装写处理器到事件循环 if (c-&gt;bufpos == 0 &amp;&amp; listLength(c-&gt;reply) == 0 &amp;&amp; (c-&gt;replstate == REDIS_REPL_NONE || c-&gt;replstate == REDIS_REPL_ONLINE) &amp;&amp; aeCreateFileEvent(server.el, c-&gt;fd, AE_WRITABLE, sendReplyToClient, c) == AE_ERR) ...&#125; redis 不能简单说是一个单进程单线程 reactor原因： 执行 BGSAVE 、 BGREWRITEAOF 等命令时，会 fork 子进程 bio 会开两个后台线程，用于 aof 的 fsync 强制刷磁盘 和 文件的 close","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"Redis","slug":"开源组件/Redis","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/"},{"name":"源码阅读","slug":"开源组件/Redis/源码阅读","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"},{"name":"C","slug":"C","permalink":"http://yoursite.com/tags/C/"}]},{"title":"Redis 源码阅读10_客户端和服务器_networking","slug":"Redis源码阅读10_客户端和服务器_networking","date":"2020-03-11T12:25:46.335Z","updated":"2020-03-11T12:27:46.000Z","comments":true,"path":"开源组件/Redis/源码阅读/Redis源码阅读10_客户端和服务器_networking/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/Redis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB10_%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%99%A8_networking/","excerpt":"","text":"Redis 的网络连接库，负责发送命令回复和接受命令请求， 同时也负责创建/销毁客户端， 以及通信协议分析等工作。 涉及文件 API redisClient *createClient(int fd) void acceptTcpHandler(aeEventLoop *el, int fd, void *privdata, int mask) static void acceptCommonHandler(int fd, int flags) 涉及文件networking.c API redisClient *createClient(int fd) int prepareClientToWrite(redisClient *c) void acceptTcpHandler(aeEventLoop *el, int fd, void *privdata, int mask) void acceptUnixHandler(aeEventLoop *el, int fd, void *privdata, int mask) void freeClient(redisClient *c) void sendReplyToClient(aeEventLoop *el, int fd, void *privdata, int mask) redisClient *createClient(int fd)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114/* * 创建一个新客户端 */redisClient *createClient(int fd) &#123; // 分配空间 redisClient *c = zmalloc(sizeof(redisClient)); /* passing -1 as fd it is possible to create a non connected client. * This is useful since all the Redis commands needs to be executed * in the context of a client. When commands are executed in other * contexts (for instance a Lua script) we need a non connected client. */ // 当 fd 不为 -1 时，创建带网络连接的客户端 // 如果 fd 为 -1 ，那么创建无网络连接的伪客户端 // 因为 Redis 的命令必须在客户端的上下文中使用，所以在执行 Lua 环境中的命令时 // 需要用到这种伪终端 if (fd != -1) &#123; // 非阻塞 anetNonBlock(NULL, fd); // 禁用 Nagle 算法 anetEnableTcpNoDelay(NULL, fd); // 设置 keep alive if (server.tcpkeepalive) anetKeepAlive(NULL, fd, server.tcpkeepalive); // 绑定读事件到事件 loop （开始接收命令请求） if (aeCreateFileEvent(server.el, fd, AE_READABLE, readQueryFromClient, c) == AE_ERR) &#123; close(fd); zfree(c); return NULL; &#125; &#125; // 初始化各个属性 // 默认数据库 selectDb(c, 0); // 套接字 c-&gt;fd = fd; // 名字 c-&gt;name = NULL; // 回复缓冲区的偏移量 c-&gt;bufpos = 0; // 查询缓冲区 c-&gt;querybuf = sdsempty(); // 查询缓冲区峰值 c-&gt;querybuf_peak = 0; // 命令请求的类型 c-&gt;reqtype = 0; // 命令参数数量 c-&gt;argc = 0; // 命令参数 c-&gt;argv = NULL; // 当前执行的命令和最近一次执行的命令 c-&gt;cmd = c-&gt;lastcmd = NULL; // 查询缓冲区中未读入的命令内容数量 c-&gt;multibulklen = 0; // 读入的参数的长度 c-&gt;bulklen = -1; // 已发送字节数 c-&gt;sentlen = 0; // 状态 FLAG c-&gt;flags = 0; // 创建时间和最后一次互动时间 c-&gt;ctime = c-&gt;lastinteraction = server.unixtime; // 认证状态 c-&gt;authenticated = 0; // 复制状态 c-&gt;replstate = REDIS_REPL_NONE; // 复制偏移量 c-&gt;reploff = 0; // 通过 ACK 命令接收到的偏移量 c-&gt;repl_ack_off = 0; // 通过 AKC 命令接收到偏移量的时间 c-&gt;repl_ack_time = 0; // 客户端为从服务器时使用，记录了从服务器所使用的端口号 c-&gt;slave_listening_port = 0; // 回复链表 c-&gt;reply = listCreate(); // 回复链表的字节量 c-&gt;reply_bytes = 0; // 回复缓冲区大小达到软限制的时间 c-&gt;obuf_soft_limit_reached_time = 0; // 回复链表的释放和复制函数 listSetFreeMethod(c-&gt;reply, decrRefCountVoid); listSetDupMethod(c-&gt;reply, dupClientReplyValue); // 阻塞类型 c-&gt;btype = REDIS_BLOCKED_NONE; // 阻塞超时 c-&gt;bpop.timeout = 0; // 造成客户端阻塞的列表键 c-&gt;bpop.keys = dictCreate(&amp;setDictType, NULL); // 在解除阻塞时将元素推入到 target 指定的键中 // BRPOPLPUSH 命令时使用 c-&gt;bpop.target = NULL; c-&gt;bpop.numreplicas = 0; c-&gt;bpop.reploffset = 0; c-&gt;woff = 0; // 进行事务时监视的键 c-&gt;watched_keys = listCreate(); // 订阅的频道和模式 c-&gt;pubsub_channels = dictCreate(&amp;setDictType, NULL); c-&gt;pubsub_patterns = listCreate(); c-&gt;peerid = NULL; listSetFreeMethod(c-&gt;pubsub_patterns, decrRefCountVoid); listSetMatchMethod(c-&gt;pubsub_patterns, listMatchObjects); // 如果不是伪客户端，那么添加到服务器的客户端链表中 if (fd != -1) listAddNodeTail(server.clients, c); // 初始化客户端的事务状态 initClientMultiState(c); // 返回客户端 return c;&#125; 分配空间 1redisClient *c = zmalloc(sizeof(redisClient)); 当 fd 非 -1 时，创建带网络连接的客户端 12345678910111213141516if (fd != -1) &#123; // 非阻塞 anetNonBlock(NULL, fd); // 禁用 Nagle 算法 anetEnableTcpNoDelay(NULL, fd); // 设置 keep alive if (server.tcpkeepalive) anetKeepAlive(NULL, fd, server.tcpkeepalive); // 绑定读事件到事件 loop （开始接收命令请求） if (aeCreateFileEvent(server.el, fd, AE_READABLE, readQueryFromClient, c) == AE_ERR) &#123; close(fd); zfree(c); return NULL; &#125;&#125; 首先把 fd 设置为非阻塞 123456789101112131415161718anetNonBlock(NULL, fd);int anetNonBlock(char *err, int fd) &#123; int flags; if ((flags = fcntl(fd, F_GETFL)) == -1) &#123; anetSetError(err, \"fcntl(F_GETFL): %s\", strerror(errno)); return ANET_ERR; &#125; if (fcntl(fd, F_SETFL, flags | O_NONBLOCK) == -1) &#123; anetSetError(err, \"fcntl(F_SETFL,O_NONBLOCK): %s\", strerror(errno)); return ANET_ERR; &#125; return ANET_OK;&#125; 然后禁用 Nagle 算法（TCP?IP 协议中的一种算法） 1anetEnableTcpNoDelay(NULL, fd); 设置 tcp 的 keep-alive 参数 12if (server.tcpkeepalive) anetKeepAlive(NULL, fd, server.tcpkeepalive); 最后绑定 EventLoop 12aeCreateFileEvent(server.el, fd, AE_READABLE, readQueryFromClient, c) 在这里设置了回调函数 readQueryFromClient 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990/* * 读取客户端的查询缓冲区内容 */void readQueryFromClient(aeEventLoop *el, int fd, void *privdata, int mask) &#123; redisClient *c = (redisClient *) privdata; int nread, readlen; size_t qblen; REDIS_NOTUSED(el); REDIS_NOTUSED(mask); // 设置服务器的当前客户端 server.current_client = c; // 读入长度（默认为 16 MB） readlen = REDIS_IOBUF_LEN; if (c-&gt;reqtype == REDIS_REQ_MULTIBULK &amp;&amp; c-&gt;multibulklen &amp;&amp; c-&gt;bulklen != -1 &amp;&amp; c-&gt;bulklen &gt;= REDIS_MBULK_BIG_ARG) &#123; int remaining = (unsigned) (c-&gt;bulklen + 2) - sdslen(c-&gt;querybuf); if (remaining &lt; readlen) readlen = remaining; &#125; // 获取查询缓冲区当前内容的长度 // 如果读取出现 short read ，那么可能会有内容滞留在读取缓冲区里面 // 这些滞留内容也许不能完整构成一个符合协议的命令， qblen = sdslen(c-&gt;querybuf); // 如果有需要，更新缓冲区内容长度的峰值（peak） if (c-&gt;querybuf_peak &lt; qblen) c-&gt;querybuf_peak = qblen; // 为查询缓冲区分配空间 c-&gt;querybuf = sdsMakeRoomFor(c-&gt;querybuf, readlen); // 读入内容到查询缓存 nread = read(fd, c-&gt;querybuf + qblen, readlen); // 读入出错 if (nread == -1) &#123; if (errno == EAGAIN) &#123; nread = 0; &#125; else &#123; redisLog(REDIS_VERBOSE, \"Reading from client: %s\", strerror(errno)); freeClient(c); return; &#125; // 遇到 EOF &#125; else if (nread == 0) &#123; redisLog(REDIS_VERBOSE, \"Client closed connection\"); freeClient(c); return; &#125; if (nread) &#123; // 根据内容，更新查询缓冲区（SDS） free 和 len 属性 // 并将 '\\0' 正确地放到内容的最后 sdsIncrLen(c-&gt;querybuf, nread); // 记录服务器和客户端最后一次互动的时间 c-&gt;lastinteraction = server.unixtime; // 如果客户端是 master 的话，更新它的复制偏移量 if (c-&gt;flags &amp; REDIS_MASTER) c-&gt;reploff += nread; &#125; else &#123; // 在 nread == -1 且 errno == EAGAIN 时运行 server.current_client = NULL; return; &#125; // 查询缓冲区长度超出服务器最大缓冲区长度 // 清空缓冲区并释放客户端 if (sdslen(c-&gt;querybuf) &gt; server.client_max_querybuf_len) &#123; sds ci = catClientInfoString(sdsempty(), c), bytes = sdsempty(); bytes = sdscatrepr(bytes, c-&gt;querybuf, 64); redisLog(REDIS_WARNING, \"Closing client that reached max query buffer length: %s (qbuf initial bytes: %s)\", ci, bytes); sdsfree(ci); sdsfree(bytes); freeClient(c); return; &#125; // 从查询缓存重读取内容，创建参数，并执行命令 // 函数会执行到缓存中的所有内容都被处理完为止 processInputBuffer(c); server.current_client = NULL;&#125; 这个回调函数又包括了几个环节 首先把数据读入 client 的缓存区 1234567891011121314151617181920212223242526272829303132333435363738redisClient *c = (redisClient *) privdata;int nread, readlen;size_t qblen;REDIS_NOTUSED(el);REDIS_NOTUSED(mask);// 设置服务器的当前客户端server.current_client = c;// 读入长度（默认为 16 MB）readlen = REDIS_IOBUF_LEN;if (c-&gt;reqtype == REDIS_REQ_MULTIBULK &amp;&amp; c-&gt;multibulklen &amp;&amp; c-&gt;bulklen != -1 &amp;&amp; c-&gt;bulklen &gt;= REDIS_MBULK_BIG_ARG) &#123; int remaining = (unsigned) (c-&gt;bulklen + 2) - sdslen(c-&gt;querybuf); if (remaining &lt; readlen) &#123; readlen = remaining; &#125;&#125;// 获取查询缓冲区当前内容的长度// 如果读取出现 short read ，那么可能会有内容滞留在读取缓冲区里面// 这些滞留内容也许不能完整构成一个符合协议的命令，qblen = sdslen(c-&gt;querybuf);// 如果有需要，更新缓冲区内容长度的峰值（peak）if (c-&gt;querybuf_peak &lt; qblen) &#123; c-&gt;querybuf_peak = qblen;&#125;// 为查询缓冲区分配空间c-&gt;querybuf = sdsMakeRoomFor(c-&gt;querybuf, readlen);// 读入内容到查询缓存nread = read(fd, c-&gt;querybuf + qblen, readlen); if (nread) { // 根据内容，更新查询缓冲区（SDS） free 和 len 属性 // 并将 &apos;\\0&apos; 正确地放到内容的最后 sdsIncrLen(c-&gt;querybuf, nread); // 记录服务器和客户端最后一次互动的时间 c-&gt;lastinteraction = server.unixtime; // 如果客户端是 master 的话，更新它的复制偏移量 if (c-&gt;flags &amp; REDIS_MASTER) { c-&gt;reploff += nread; } } else { // 在 nread == -1 且 errno == EAGAIN 时运行 server.current_client = NULL; return; } 如果缓冲区内容长度超过限制，则关闭 client 1234567891011121314if (sdslen(c-&gt;querybuf) &gt; server.client_max_querybuf_len) &#123; sds ci = catClientInfoString(sdsempty(), c), bytes = sdsempty(); bytes = sdscatrepr(bytes, c-&gt;querybuf, 64); redisLog(REDIS_WARNING, \"Closing client that reached max query buffer length: %s (qbuf initial bytes: %s)\", ci, bytes); sdsfree(ci); sdsfree(bytes); freeClient(c); return;&#125; 从查询缓存重读取内容，创建参数，并执行命令，函数会执行到缓存中的所有内容都被处理完为止 1processInputBuffer(c); processInputBuffer 函数也很长，里面关键的部分有两个： 读 redisClient.buffer 的内容，解析成命令和命令参数 123processInlineBufferprocessMultibulkBuffer 执行命令并重置客户端 123processCommandresetClient void acceptTcpHandler(aeEventLoop *el, int fd, void *privdata, int mask)12345678910111213141516171819202122232425262728/* * 创建一个 TCP 连接处理器 */void acceptTcpHandler(aeEventLoop *el, int fd, void *privdata, int mask) &#123; int cport, cfd, max = MAX_ACCEPTS_PER_CALL; char cip[REDIS_IP_STR_LEN]; REDIS_NOTUSED(el); REDIS_NOTUSED(mask); REDIS_NOTUSED(privdata); while (max--) &#123; // accept 客户端连接 cfd = anetTcpAccept(server.neterr, fd, cip, sizeof(cip), &amp;cport); if (cfd == ANET_ERR) &#123; if (errno != EWOULDBLOCK) redisLog(REDIS_WARNING, \"Accepting client connection: %s\", server.neterr); return; &#125; redisLog(REDIS_VERBOSE, \"Accepted %s:%d\", cip, cport); // 为客户端创建客户端状态（redisClient） acceptCommonHandler(cfd, 0); &#125;&#125;static void acceptCommonHandler(int fd, int flags)12345678910111213141516171819202122232425262728293031323334353637383940414243/* * TCP 连接 accept 处理器 */#define MAX_ACCEPTS_PER_CALL 1000static void acceptCommonHandler(int fd, int flags) &#123; // 创建客户端 redisClient *c; if ((c = createClient(fd)) == NULL) &#123; redisLog(REDIS_WARNING, \"Error registering fd event for the new client: %s (fd=%d)\", strerror(errno), fd); close(fd); /* May be already closed, just ignore errors */ return; &#125; /* If maxclient directive is set and this is one client more... close the * connection. Note that we create the client instead to check before * for this condition, since now the socket is already set in non-blocking * mode and we can send an error for free using the Kernel I/O */ // 如果新添加的客户端令服务器的最大客户端数量达到了 // 那么向新客户端写入错误信息，并关闭新客户端 // 先创建客户端，再进行数量检查是为了方便地进行错误信息写入 if (listLength(server.clients) &gt; server.maxclients) &#123; char *err = \"-ERR max number of clients reached\\r\\n\"; /* That's a best effort error message, don't check write errors */ if (write(c-&gt;fd, err, strlen(err)) == -1) &#123; /* Nothing to do, Just to avoid the warning... */ &#125; // 更新拒绝连接数 server.stat_rejected_conn++; freeClient(c); return; &#125; // 更新连接次数 server.stat_numconnections++; // 设置 FLAG c-&gt;flags |= flags;&#125; 这里面的重点在于 createClient，现在应该 accept 出了一个 fd，用这个 fd 来创建 redisClient","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"Redis","slug":"开源组件/Redis","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/"},{"name":"源码阅读","slug":"开源组件/Redis/源码阅读","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"},{"name":"C","slug":"C","permalink":"http://yoursite.com/tags/C/"}]},{"title":"Redis 源码阅读9_客户端和服务器_ae","slug":"Redis源码阅读9_客户端和服务器_ae","date":"2020-03-11T12:25:46.319Z","updated":"2020-03-11T12:26:56.000Z","comments":true,"path":"开源组件/Redis/源码阅读/Redis源码阅读9_客户端和服务器_ae/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/Redis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB9_%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%99%A8_ae/","excerpt":"","text":"Redis 的事件处理器实现（基于 Reactor 模式）。 涉及文件 结构体 aeFileEvent 文件事件 aeTimeEvent 时间事件 aeFiredEvent 已就绪事件 aeEventLoop 事件处理器的状态 事件处理器接口 API aeEventLoop *aeCreateEventLoop(int setsize); void aeDeleteEventLoop(aeEventLoop *eventLoop) void aeStop(aeEventLoop *eventLoop) int aeCreateFileEvent(aeEventLoop *eventLoop, int fd, int mask, aeFileProc *proc, void *clientData) long long aeCreateTimeEvent(aeEventLoop *eventLoop, long long milliseconds, aeTimeProc *proc, void *clientData, aeEventFinalizerProc *finalizerProc); void aeMain(aeEventLoop *eventLoop) 涉及文件ae.h ae.c ae_epoll.c ae_evport.c ae_kqueue.c ae_select.c 结构体aeFileEvent 文件事件123456789101112131415161718192021/* File event structure * * 文件事件结构 */typedef struct aeFileEvent &#123; // 监听事件类型掩码， // 值可以是 AE_READABLE 或 AE_WRITABLE ， // 或者 AE_READABLE | AE_WRITABLE int mask; /* one of AE_(READABLE|WRITABLE) */ // 读事件处理器 aeFileProc *rfileProc; // 写事件处理器 aeFileProc *wfileProc; // 多路复用库的私有数据 void *clientData;&#125; aeFileEvent; aeTimeEvent 时间事件1234567891011121314151617181920212223242526/* Time event structure * * 时间事件结构 */typedef struct aeTimeEvent &#123; // 时间事件的唯一标识符 long long id; /* time event identifier. */ // 事件的到达时间 long when_sec; /* seconds */ long when_ms; /* milliseconds */ // 事件处理函数 aeTimeProc *timeProc; // 事件释放函数 aeEventFinalizerProc *finalizerProc; // 多路复用库的私有数据 void *clientData; // 指向下个时间事件结构，形成链表 struct aeTimeEvent *next;&#125; aeTimeEvent; aeFiredEvent 已就绪事件123456789101112131415/* A fired event * * 已就绪事件 */typedef struct aeFiredEvent &#123; // 已就绪文件描述符 int fd; // 事件类型掩码， // 值可以是 AE_READABLE 或 AE_WRITABLE // 或者是两者的或 int mask;&#125; aeFiredEvent; aeEventLoop 事件处理器的状态12345678910111213141516171819202122232425262728293031323334353637/* State of an event based program * * 事件处理器的状态 */typedef struct aeEventLoop &#123; // 目前已注册的最大描述符 int maxfd; /* highest file descriptor currently registered */ // 目前已追踪的最大描述符 int setsize; /* max number of file descriptors tracked */ // 用于生成时间事件 id long long timeEventNextId; // 最后一次执行时间事件的时间 time_t lastTime; /* Used to detect system clock skew */ // 已注册的文件事件 aeFileEvent *events; /* Registered events */ // 已就绪的文件事件 aeFiredEvent *fired; /* Fired events */ // 时间事件 aeTimeEvent *timeEventHead; // 事件处理器的开关 int stop; // 多路复用库的私有数据 void *apidata; /* This is used for polling API specific data */ // 在处理事件前要执行的函数 aeBeforeSleepProc *beforesleep;&#125; aeEventLoop; 事件处理器接口 typedef void aeFileProc(struct aeEventLoop *eventLoop, int fd, void *clientData, int mask); typedef int aeTimeProc(struct aeEventLoop *eventLoop, long long id, void *clientData); typedef void aeEventFinalizerProc(struct aeEventLoop *eventLoop, void *clientData); typedef void aeBeforeSleepProc(struct aeEventLoop *eventLoop); APIaeEventLoop *aeCreateEventLoop(int setsize); void aeDeleteEventLoop(aeEventLoop *eventLoop); void aeStop(aeEventLoop *eventLoop); int aeCreateFileEvent(aeEventLoop *eventLoop, int fd, int mask, aeFileProc *proc, void *clientData); void aeDeleteFileEvent(aeEventLoop *eventLoop, int fd, int mask); int aeGetFileEvents(aeEventLoop *eventLoop, int fd); long long aeCreateTimeEvent(aeEventLoop *eventLoop, long long milliseconds, aeTimeProc *proc, void *clientData, aeEventFinalizerProc *finalizerProc); int aeDeleteTimeEvent(aeEventLoop *eventLoop, long long id); int aeProcessEvents(aeEventLoop *eventLoop, int flags); int aeWait(int fd, int mask, long long milliseconds); void aeMain(aeEventLoop *eventLoop); char *aeGetApiName(void); void aeSetBeforeSleepProc(aeEventLoop *eventLoop, aeBeforeSleepProc *beforesleep); int aeGetSetSize(aeEventLoop *eventLoop); int aeResizeSetSize(aeEventLoop *eventLoop, int setsize); aeEventLoop *aeCreateEventLoop(int setsize);1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/* * 初始化事件处理器状态 */aeEventLoop *aeCreateEventLoop(int setsize) &#123; aeEventLoop *eventLoop; int i; // 创建事件状态结构 if ((eventLoop = zmalloc(sizeof(*eventLoop))) == NULL) &#123; goto err; &#125; // 初始化文件事件结构和已就绪文件事件结构数组 eventLoop-&gt;events = zmalloc(sizeof(aeFileEvent) * setsize); eventLoop-&gt;fired = zmalloc(sizeof(aeFiredEvent) * setsize); if (eventLoop-&gt;events == NULL || eventLoop-&gt;fired == NULL) &#123; goto err; &#125; // 设置数组大小 eventLoop-&gt;setsize = setsize; // 初始化执行最近一次执行时间 eventLoop-&gt;lastTime = time(NULL); // 初始化时间事件结构 eventLoop-&gt;timeEventHead = NULL; eventLoop-&gt;timeEventNextId = 0; eventLoop-&gt;stop = 0; eventLoop-&gt;maxfd = -1; eventLoop-&gt;beforesleep = NULL; if (aeApiCreate(eventLoop) == -1) &#123; goto err; &#125; /* Events with mask == AE_NONE are not set. So let's initialize the * vector with it. */ // 初始化监听事件 for (i = 0; i &lt; setsize; i++) &#123; eventLoop-&gt;events[i].mask = AE_NONE; &#125; // 返回事件循环 return eventLoop; err: if (eventLoop) &#123; zfree(eventLoop-&gt;events); zfree(eventLoop-&gt;fired); zfree(eventLoop); &#125; return NULL;&#125; 首先是创建 aeEventLoop 对象，然后各种设置属性参数 123456789101112131415161718192021222324252627282930313233aeEventLoop *eventLoop;int i;// 创建事件状态结构if ((eventLoop = zmalloc(sizeof(*eventLoop))) == NULL) &#123; goto err;&#125;// 初始化文件事件结构和已就绪文件事件结构数组eventLoop-&gt;events = zmalloc(sizeof(aeFileEvent) * setsize);eventLoop-&gt;fired = zmalloc(sizeof(aeFiredEvent) * setsize);if (eventLoop-&gt;events == NULL || eventLoop-&gt;fired == NULL) &#123; goto err;&#125;// 设置数组大小eventLoop-&gt;setsize = setsize;// 初始化执行最近一次执行时间eventLoop-&gt;lastTime = time(NULL);// 初始化时间事件结构eventLoop-&gt;timeEventHead = NULL;eventLoop-&gt;timeEventNextId = 0;eventLoop-&gt;stop = 0;eventLoop-&gt;maxfd = -1;eventLoop-&gt;beforesleep = NULL;// 初始化监听事件for (i = 0; i &lt; setsize; i++) &#123; eventLoop-&gt;events[i].mask = AE_NONE;&#125; 然后进行各种底层 IO 多路复用库的初始化 123if (aeApiCreate(eventLoop) == -1) &#123; goto err;&#125; 到底使用了哪种 IO 多路复用库，取决于系统有啥和 include 顺序 12345678910111213#ifdef HAVE_EVPORT#include \"ae_evport.c\"#else#ifdef HAVE_EPOLL#include \"ae_epoll.c\"#else#ifdef HAVE_KQUEUE#include \"ae_kqueue.c\"#else#include \"ae_select.c\"#endif#endif#endif 以 epoll 为例， 123456789101112131415161718192021222324252627282930313233343536373839404142/* * 事件状态 */typedef struct aeApiState &#123; // epoll_event 实例描述符 int epfd; // 事件槽 struct epoll_event *events;&#125; aeApiState;/* * 创建一个新的 epoll 实例，并将它赋值给 eventLoop */static int aeApiCreate(aeEventLoop *eventLoop) &#123; aeApiState *state = zmalloc(sizeof(aeApiState)); if (!state) return -1; // 初始化事件槽空间 state-&gt;events = zmalloc(sizeof(struct epoll_event) * eventLoop-&gt;setsize); if (!state-&gt;events) &#123; zfree(state); return -1; &#125; // 创建 epoll 实例 state-&gt;epfd = epoll_create(1024); /* 1024 is just a hint for the kernel */ if (state-&gt;epfd == -1) &#123; zfree(state-&gt;events); zfree(state); return -1; &#125; // 赋值给 eventLoop eventLoop-&gt;apidata = state; return 0;&#125; 初始化以后，把 state 赋值给 eventLoop 的 apidata void aeDeleteEventLoop(aeEventLoop *eventLoop)123456789/* * 删除事件处理器 */void aeDeleteEventLoop(aeEventLoop *eventLoop) &#123; aeApiFree(eventLoop); zfree(eventLoop-&gt;events); zfree(eventLoop-&gt;fired); zfree(eventLoop);&#125; 以 epoll 为例 12345678910/* * 释放 epoll 实例和事件槽 */static void aeApiFree(aeEventLoop *eventLoop) &#123; aeApiState *state = eventLoop-&gt;apidata; close(state-&gt;epfd); zfree(state-&gt;events); zfree(state);&#125; void aeStop(aeEventLoop *eventLoop)123456/* * 停止事件处理器 */void aeStop(aeEventLoop *eventLoop) &#123; eventLoop-&gt;stop = 1;&#125; int aeCreateFileEvent(aeEventLoop *eventLoop, int fd, int mask, aeFileProc *proc, void *clientData)1234567891011121314151617181920212223242526272829303132333435363738/* * 根据 mask 参数的值，监听 fd 文件的状态， * 当 fd 可用时，执行 proc 函数 */int aeCreateFileEvent(aeEventLoop *eventLoop, int fd, int mask, aeFileProc *proc, void *clientData) &#123; if (fd &gt;= eventLoop-&gt;setsize) &#123; errno = ERANGE; return AE_ERR; &#125; // 取出文件事件结构 aeFileEvent *fe = &amp;eventLoop-&gt;events[fd]; // 监听指定 fd 的指定事件 if (aeApiAddEvent(eventLoop, fd, mask) == -1) return AE_ERR; // 设置文件事件类型，以及事件的处理器 fe-&gt;mask |= mask; if (mask &amp; AE_READABLE) &#123; fe-&gt;rfileProc = proc; &#125; if (mask &amp; AE_WRITABLE) &#123; fe-&gt;wfileProc = proc; &#125; // 私有数据 fe-&gt;clientData = clientData; // 如果有需要，更新事件处理器的最大 fd if (fd &gt; eventLoop-&gt;maxfd) &#123; eventLoop-&gt;maxfd = fd; &#125; return AE_OK;&#125; 这个方法的目标是创建文件事件，具体要交给底层的 IO 多路复用组件，还是以 epoll 为例 12345678910111213141516171819202122232425262728293031323334353637/* * 关联给定事件到 fd */static int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask) &#123; aeApiState *state = eventLoop-&gt;apidata; struct epoll_event ee; /* If the fd was already monitored for some event, we need a MOD * operation. Otherwise we need an ADD operation. * * 如果 fd 没有关联任何事件，那么这是一个 ADD 操作。 * * 如果已经关联了某个/某些事件，那么这是一个 MOD 操作。 */ int op = eventLoop-&gt;events[fd].mask == AE_NONE ? EPOLL_CTL_ADD : EPOLL_CTL_MOD; // 注册事件到 epoll ee.events = 0; mask |= eventLoop-&gt;events[fd].mask; /* Merge old events */ if (mask &amp; AE_READABLE) &#123; ee.events |= EPOLLIN; &#125; if (mask &amp; AE_WRITABLE) &#123; ee.events |= EPOLLOUT; &#125; ee.data.u64 = 0; /* avoid valgrind warning */ ee.data.fd = fd; if (epoll_ctl(state-&gt;epfd, op, fd, &amp;ee) == -1) &#123; return -1; &#125; return 0;&#125; 基本思路是设置好各种 epoll 的参数，然后调用 epoll_ctl 向 epoll 添加文件事件 long long aeCreateTimeEvent(aeEventLoop *eventLoop, long long milliseconds, aeTimeProc *proc, void *clientData, aeEventFinalizerProc *finalizerProc);123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/* * 创建时间事件 */long long aeCreateTimeEvent(aeEventLoop *eventLoop, long long milliseconds, aeTimeProc *proc, void *clientData, aeEventFinalizerProc *finalizerProc) &#123; // 更新时间计数器 long long id = eventLoop-&gt;timeEventNextId++; // 创建时间事件结构 aeTimeEvent *te; te = zmalloc(sizeof(*te)); if (te == NULL) &#123; return AE_ERR; &#125; // 设置 ID te-&gt;id = id; // 设定处理事件的时间 aeAddMillisecondsToNow(milliseconds, &amp;te-&gt;when_sec, &amp;te-&gt;when_ms); // 设置事件处理器 te-&gt;timeProc = proc; te-&gt;finalizerProc = finalizerProc; // 设置私有数据 te-&gt;clientData = clientData; // 将新事件放入表头 te-&gt;next = eventLoop-&gt;timeEventHead; eventLoop-&gt;timeEventHead = te; return id;&#125;/* * 在当前时间上加上 milliseconds 毫秒， * 并且将加上之后的秒数和毫秒数分别保存在 sec 和 ms 指针中。 */static void aeAddMillisecondsToNow(long long milliseconds, long *sec, long *ms) &#123; long cur_sec, cur_ms, when_sec, when_ms; // 获取当前时间 aeGetTime(&amp;cur_sec, &amp;cur_ms); // 计算增加 milliseconds 之后的秒数和毫秒数 when_sec = cur_sec + milliseconds / 1000; when_ms = cur_ms + milliseconds % 1000; // 进位： // 如果 when_ms 大于等于 1000 // 那么将 when_sec 增大一秒 if (when_ms &gt;= 1000) &#123; when_sec++; when_ms -= 1000; &#125; // 保存到指针中 *sec = when_sec; *ms = when_ms;&#125; 基本逻辑是设置各种事件处理器、参数等，然后获取时间用于定时。时间事件是一个链表。 void aeMain(aeEventLoop *eventLoop)1234567891011121314151617/* * 事件处理器的主循环 */void aeMain(aeEventLoop *eventLoop) &#123; eventLoop-&gt;stop = 0; while (!eventLoop-&gt;stop) &#123; // 如果有需要在事件处理前执行的函数，那么运行它 if (eventLoop-&gt;beforesleep != NULL) eventLoop-&gt;beforesleep(eventLoop); // 开始处理事件 aeProcessEvents(eventLoop, AE_ALL_EVENTS); &#125;&#125; 这是一个 while 循环，会一直进行下去。每次循环先执行注册好的 beforesleep 方法，然后开始处理事件 aeProcessEvents,参数是全部事件 AE_ALL_EVENTS。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128/* Process every pending time event, then every pending file event * (that may be registered by time event callbacks just processed). * * 处理所有已到达的时间事件，以及所有已就绪的文件事件。 * * Without special flags the function sleeps until some file event * fires, or when the next time event occurs (if any). * * 如果不传入特殊 flags 的话，那么函数睡眠直到文件事件就绪， * 或者下个时间事件到达（如果有的话）。 * * If flags is 0, the function does nothing and returns. * 如果 flags 为 0 ，那么函数不作动作，直接返回。 * * if flags has AE_ALL_EVENTS set, all the kind of events are processed. * 如果 flags 包含 AE_ALL_EVENTS ，所有类型的事件都会被处理。 * * if flags has AE_FILE_EVENTS set, file events are processed. * 如果 flags 包含 AE_FILE_EVENTS ，那么处理文件事件。 * * if flags has AE_TIME_EVENTS set, time events are processed. * 如果 flags 包含 AE_TIME_EVENTS ，那么处理时间事件。 * * if flags has AE_DONT_WAIT set the function returns ASAP until all * the events that's possible to process without to wait are processed. * 如果 flags 包含 AE_DONT_WAIT ， * 那么函数在处理完所有不许阻塞的事件之后，即刻返回。 * * The function returns the number of events processed. * 函数的返回值为已处理事件的数量 */int aeProcessEvents(aeEventLoop *eventLoop, int flags) &#123; int processed = 0, numevents; /* Nothing to do? return ASAP */ if (!(flags &amp; AE_TIME_EVENTS) &amp;&amp; !(flags &amp; AE_FILE_EVENTS)) return 0; /* Note that we want call select() even if there are no * file events to process as long as we want to process time * events, in order to sleep until the next time event is ready * to fire. */ if (eventLoop-&gt;maxfd != -1 || ((flags &amp; AE_TIME_EVENTS) &amp;&amp; !(flags &amp; AE_DONT_WAIT))) &#123; int j; aeTimeEvent *shortest = NULL; struct timeval tv, *tvp; // 获取最近的时间事件 if (flags &amp; AE_TIME_EVENTS &amp;&amp; !(flags &amp; AE_DONT_WAIT)) shortest = aeSearchNearestTimer(eventLoop); if (shortest) &#123; // 如果时间事件存在的话 // 那么根据最近可执行时间事件和现在时间的时间差来决定文件事件的阻塞时间 long now_sec, now_ms; /* Calculate the time missing for the nearest * timer to fire. */ // 计算距今最近的时间事件还要多久才能达到 // 并将该时间距保存在 tv 结构中 aeGetTime(&amp;now_sec, &amp;now_ms); tvp = &amp;tv; tvp-&gt;tv_sec = shortest-&gt;when_sec - now_sec; if (shortest-&gt;when_ms &lt; now_ms) &#123; tvp-&gt;tv_usec = ((shortest-&gt;when_ms + 1000) - now_ms) * 1000; tvp-&gt;tv_sec--; &#125; else &#123; tvp-&gt;tv_usec = (shortest-&gt;when_ms - now_ms) * 1000; &#125; // 时间差小于 0 ，说明事件已经可以执行了，将秒和毫秒设为 0 （不阻塞） if (tvp-&gt;tv_sec &lt; 0) tvp-&gt;tv_sec = 0; if (tvp-&gt;tv_usec &lt; 0) tvp-&gt;tv_usec = 0; &#125; else &#123; // 执行到这一步，说明没有时间事件 // 那么根据 AE_DONT_WAIT 是否设置来决定是否阻塞，以及阻塞的时间长度 /* If we have to check for events but need to return * ASAP because of AE_DONT_WAIT we need to set the timeout * to zero */ if (flags &amp; AE_DONT_WAIT) &#123; // 设置文件事件不阻塞 tv.tv_sec = tv.tv_usec = 0; tvp = &amp;tv; &#125; else &#123; /* Otherwise we can block */ // 文件事件可以阻塞直到有事件到达为止 tvp = NULL; /* wait forever */ &#125; &#125; // 处理文件事件，阻塞时间由 tvp 决定 numevents = aeApiPoll(eventLoop, tvp); for (j = 0; j &lt; numevents; j++) &#123; // 从已就绪数组中获取事件 aeFileEvent *fe = &amp;eventLoop-&gt;events[eventLoop-&gt;fired[j].fd]; int mask = eventLoop-&gt;fired[j].mask; int fd = eventLoop-&gt;fired[j].fd; int rfired = 0; /* note the fe-&gt;mask &amp; mask &amp; ... code: maybe an already processed * event removed an element that fired and we still didn't * processed, so we check if the event is still valid. */ // 读事件 if (fe-&gt;mask &amp; mask &amp; AE_READABLE) &#123; // rfired 确保读/写事件只能执行其中一个 rfired = 1; fe-&gt;rfileProc(eventLoop, fd, fe-&gt;clientData, mask); &#125; // 写事件 if (fe-&gt;mask &amp; mask &amp; AE_WRITABLE) &#123; if (!rfired || fe-&gt;wfileProc != fe-&gt;rfileProc) fe-&gt;wfileProc(eventLoop, fd, fe-&gt;clientData, mask); &#125; processed++; &#125; &#125; /* Check time events */ // 执行时间事件 if (flags &amp; AE_TIME_EVENTS) processed += processTimeEvents(eventLoop); return processed; /* return the number of processed file/time events */&#125; 首先获取最近的时间事件 12345678910111213141516171819202122// 获取最近的时间事件if (flags &amp; AE_TIME_EVENTS &amp;&amp; !(flags &amp; AE_DONT_WAIT)) &#123; shortest = aeSearchNearestTimer(eventLoop);&#125;// 寻找里目前时间最近的时间事件// 因为链表是乱序的，所以查找复杂度为 O（N）static aeTimeEvent *aeSearchNearestTimer(aeEventLoop *eventLoop) &#123; aeTimeEvent *te = eventLoop-&gt;timeEventHead; aeTimeEvent *nearest = NULL; while (te) &#123; if (!nearest || te-&gt;when_sec &lt; nearest-&gt;when_sec || (te-&gt;when_sec == nearest-&gt;when_sec &amp;&amp; te-&gt;when_ms &lt; nearest-&gt;when_ms)) nearest = te; te = te-&gt;next; &#125; return nearest;&#125; 具体做法是从 eventLoop 的 timeEventHead 链表中遍历找最小值 接下来设置文件事件的阻塞时间，一共有 3 种情况 0：时间事件已经到达，不阻塞 n：根据时间事件的到时时间设置一个大于 0 的值 null：无限制阻塞 然后等待文件事件 12// 处理文件事件，阻塞时间由 tvp 决定numevents = aeApiPoll(eventLoop, tvp); 还是以 epoll 为例 1234567891011121314151617181920212223242526272829303132333435363738/* * 获取可执行事件 */static int aeApiPoll(aeEventLoop *eventLoop, struct timeval *tvp) &#123; aeApiState *state = eventLoop-&gt;apidata; int retval, numevents = 0; // 等待时间 retval = epoll_wait(state-&gt;epfd, state-&gt;events, eventLoop-&gt;setsize, tvp ? (tvp-&gt;tv_sec * 1000 + tvp-&gt;tv_usec / 1000) : -1); // 有至少一个事件就绪？ if (retval &gt; 0) &#123; int j; // 为已就绪事件设置相应的模式 // 并加入到 eventLoop 的 fired 数组中 numevents = retval; for (j = 0; j &lt; numevents; j++) &#123; int mask = 0; struct epoll_event *e = state-&gt;events + j; if (e-&gt;events &amp; EPOLLIN) mask |= AE_READABLE; if (e-&gt;events &amp; EPOLLOUT) mask |= AE_WRITABLE; if (e-&gt;events &amp; EPOLLERR) mask |= AE_WRITABLE; if (e-&gt;events &amp; EPOLLHUP) mask |= AE_WRITABLE; eventLoop-&gt;fired[j].fd = e-&gt;data.fd; eventLoop-&gt;fired[j].mask = mask; &#125; &#125; // 返回已就绪事件个数 return numevents;&#125; 底层是 epoll_wait 方法，用于阻塞一段时间，返回事件个数。然后把各种 fd、type 设置好以后返回 接下来处理文件事件 12345678910111213141516171819202122for (j = 0; j &lt; numevents; j++) &#123; // 从已就绪数组中获取事件 aeFileEvent *fe = &amp;eventLoop-&gt;events[eventLoop-&gt;fired[j].fd]; int mask = eventLoop-&gt;fired[j].mask; int fd = eventLoop-&gt;fired[j].fd; int rfired = 0; // 读事件 if (fe-&gt;mask &amp; mask &amp; AE_READABLE) &#123; // rfired 确保读/写事件只能执行其中一个 rfired = 1; fe-&gt;rfileProc(eventLoop, fd, fe-&gt;clientData, mask); &#125; // 写事件 if (fe-&gt;mask &amp; mask &amp; AE_WRITABLE) &#123; if (!rfired || fe-&gt;wfileProc != fe-&gt;rfileProc) fe-&gt;wfileProc(eventLoop, fd, fe-&gt;clientData, mask); &#125; processed++;&#125; 由于到底哪些 fd 被触发已经知道了，所以依次取，事件处理器交给注册好的 CallBack，同一 fd 的读写事件只处理读的 最后处理时间事件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970if (flags &amp; AE_TIME_EVENTS) &#123; processed += processTimeEvents(eventLoop);&#125;// 处理所有已到达的时间事件 static int processTimeEvents(aeEventLoop *eventLoop) &#123; int processed = 0; aeTimeEvent *te; long long maxId; time_t now = time(NULL); // 通过重置事件的运行时间，防止因时间穿插（skew）而造成的事件处理混乱 if (now &lt; eventLoop-&gt;lastTime) &#123; te = eventLoop-&gt;timeEventHead; while (te) &#123; te-&gt;when_sec = 0; te = te-&gt;next; &#125; &#125; // 更新最后一次处理时间事件的时间 eventLoop-&gt;lastTime = now; // 遍历链表, 执行那些已经到达的事件 te = eventLoop-&gt;timeEventHead; maxId = eventLoop-&gt;timeEventNextId - 1; while (te) &#123; long now_sec, now_ms; long long id; // 跳过无效事件 if (te-&gt;id &gt; maxId) &#123; te = te-&gt;next; continue; &#125; // 获取当前时间 aeGetTime(&amp;now_sec, &amp;now_ms); // 如果当前时间等于或等于事件的执行时间，那么说明事件已到达，执行这个事件 if (now_sec &gt; te-&gt;when_sec || (now_sec == te-&gt;when_sec &amp;&amp; now_ms &gt;= te-&gt;when_ms)) &#123; int retval; id = te-&gt;id; // 执行事件处理器，并获取返回值 retval = te-&gt;timeProc(eventLoop, id, te-&gt;clientData); processed++; // 记录是否有需要循环执行这个事件时间 if (retval != AE_NOMORE) &#123; // 是的， retval 毫秒之后继续执行这个时间事件 aeAddMillisecondsToNow(retval, &amp;te-&gt;when_sec, &amp;te-&gt;when_ms); &#125; else &#123; // 不，将这个事件删除 aeDeleteTimeEvent(eventLoop, id); &#125; // 因为执行事件之后，事件列表可能已经被改变了 // 因此需要将 te 放回表头，继续开始执行事件 te = eventLoop-&gt;timeEventHead; &#125; else &#123; te = te-&gt;next; &#125; &#125; return processed;&#125; 逻辑比较简单，基本上就是把链表里的时间事件扫一遍，把到时间的时间事件，执行一下处理器回调函数，然后根据返回值，决定把是否把时间事件重新放回链表，还是删除这个时间事件。","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"Redis","slug":"开源组件/Redis","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/"},{"name":"源码阅读","slug":"开源组件/Redis/源码阅读","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"},{"name":"C","slug":"C","permalink":"http://yoursite.com/tags/C/"}]},{"title":"Redis 源码阅读8_数据库实现相关_rdb","slug":"Redis源码阅读8_数据库实现相关_rdb","date":"2020-03-11T12:21:42.921Z","updated":"2020-03-11T12:24:30.000Z","comments":true,"path":"开源组件/Redis/源码阅读/Redis源码阅读8_数据库实现相关_rdb/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/Redis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB8_%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%9E%E7%8E%B0%E7%9B%B8%E5%85%B3_rdb/","excerpt":"","text":"Redis 的 RDB 持久化实现代码。 涉及的文件 API int rdbSave(char *filename) int rdbSaveBackground(char *filename) 涉及的文件rdb.h rdb.c API12345678910111213141516171819int rdbSaveType(rio *rdb, unsigned char type);int rdbLoadType(rio *rdb);int rdbSaveTime(rio *rdb, time_t t);time_t rdbLoadTime(rio *rdb);int rdbSaveLen(rio *rdb, uint32_t len);uint32_t rdbLoadLen(rio *rdb, int *isencoded);int rdbSaveObjectType(rio *rdb, robj *o);int rdbLoadObjectType(rio *rdb);int rdbLoad(char *filename);int rdbSaveBackground(char *filename);void rdbRemoveTempFile(pid_t childpid);int rdbSave(char *filename);int rdbSaveObject(rio *rdb, robj *o);off_t rdbSavedObjectLen(robj *o);off_t rdbSavedObjectPages(robj *o);robj *rdbLoadObject(int type, rio *rdb);void backgroundSaveDoneHandler(int exitcode, int bysignal);int rdbSaveKeyValuePair(rio *rdb, robj *key, robj *val, long long expiretime, long long now);robj *rdbLoadStringObject(rio *rdb); int rdbSave(char *filename)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146/* Save the DB on disk. Return REDIS_ERR on error, REDIS_OK on success * * 将数据库保存到磁盘上。 * * 保存成功返回 REDIS_OK ，出错/失败返回 REDIS_ERR 。 */int rdbSave(char *filename) &#123; dictIterator *di = NULL; dictEntry *de; char tmpfile[256]; char magic[10]; int j; long long now = mstime(); FILE *fp; rio rdb; uint64_t cksum; // 创建临时文件 snprintf(tmpfile, 256, \"temp-%d.rdb\", (int) getpid()); fp = fopen(tmpfile, \"w\"); if (!fp) &#123; redisLog(REDIS_WARNING, \"Failed opening .rdb for saving: %s\", strerror(errno)); return REDIS_ERR; &#125; // 初始化 I/O rioInitWithFile(&amp;rdb, fp); // 设置校验和函数 if (server.rdb_checksum) rdb.update_cksum = rioGenericUpdateChecksum; // 写入 RDB 版本号 snprintf(magic, sizeof(magic), \"REDIS%04d\", REDIS_RDB_VERSION); if (rdbWriteRaw(&amp;rdb, magic, 9) == -1) goto werr; // 遍历所有数据库 for (j = 0; j &lt; server.dbnum; j++) &#123; // 指向数据库 redisDb *db = server.db + j; // 指向数据库键空间 dict *d = db-&gt;dict; // 跳过空数据库 if (dictSize(d) == 0) continue; // 创建键空间迭代器 di = dictGetSafeIterator(d); if (!di) &#123; fclose(fp); return REDIS_ERR; &#125; /* Write the SELECT DB opcode * * 写入 DB 选择器 */ if (rdbSaveType(&amp;rdb, REDIS_RDB_OPCODE_SELECTDB) == -1) goto werr; if (rdbSaveLen(&amp;rdb, j) == -1) goto werr; /* Iterate this DB writing every entry * * 遍历数据库，并写入每个键值对的数据 */ while ((de = dictNext(di)) != NULL) &#123; sds keystr = dictGetKey(de); robj key, *o = dictGetVal(de); long long expire; // 根据 keystr ，在栈中创建一个 key 对象 initStaticStringObject(key, keystr); // 获取键的过期时间 expire = getExpire(db, &amp;key); // 保存键值对数据 if (rdbSaveKeyValuePair(&amp;rdb, &amp;key, o, expire, now) == -1) goto werr; &#125; dictReleaseIterator(di); &#125; di = NULL; /* So that we don't release it again on error. */ /* EOF opcode * * 写入 EOF 代码 */ if (rdbSaveType(&amp;rdb, REDIS_RDB_OPCODE_EOF) == -1) goto werr; /* CRC64 checksum. It will be zero if checksum computation is disabled, the * loading code skips the check in this case. * * CRC64 校验和。 * * 如果校验和功能已关闭，那么 rdb.cksum 将为 0 ， * 在这种情况下， RDB 载入时会跳过校验和检查。 */ cksum = rdb.cksum; memrev64ifbe(&amp;cksum); rioWrite(&amp;rdb, &amp;cksum, 8); /* Make sure data will not remain on the OS's output buffers */ // 冲洗缓存，确保数据已写入磁盘 if (fflush(fp) == EOF) goto werr; if (fsync(fileno(fp)) == -1) goto werr; if (fclose(fp) == EOF) goto werr; /* Use RENAME to make sure the DB file is changed atomically only * if the generate DB file is ok. * * 使用 RENAME ，原子性地对临时文件进行改名，覆盖原来的 RDB 文件。 */ if (rename(tmpfile, filename) == -1) &#123; redisLog(REDIS_WARNING, \"Error moving temp DB file on the final destination: %s\", strerror(errno)); unlink(tmpfile); return REDIS_ERR; &#125; // 写入完成，打印日志 redisLog(REDIS_NOTICE, \"DB saved on disk\"); // 清零数据库脏状态 server.dirty = 0; // 记录最后一次完成 SAVE 的时间 server.lastsave = time(NULL); // 记录最后一次执行 SAVE 的状态 server.lastbgsave_status = REDIS_OK; return REDIS_OK; werr: // 关闭文件 fclose(fp); // 删除文件 unlink(tmpfile); redisLog(REDIS_WARNING, \"Write error saving DB on disk: %s\", strerror(errno)); if (di) dictReleaseIterator(di); return REDIS_ERR;&#125; 首先创建临时文件 123456789char tmpfile[256];snprintf(tmpfile, 256, \"temp-%d.rdb\", (int) getpid());fp = fopen(tmpfile, \"w\");if (!fp) &#123; redisLog(REDIS_WARNING, \"Failed opening .rdb for saving: %s\", strerror(errno)); return REDIS_ERR;&#125; 然后初始化 IO 12345678910111213141516171819FILE *fp;rio rdb;rioInitWithFile(&amp;rdb, fp);if (server.rdb_checksum) &#123; rdb.update_cksum = rioGenericUpdateChecksum;&#125;/* * 初始化文件流 */void rioInitWithFile(rio *r, FILE *fp) &#123; *r = rioFileIO; r-&gt;io.file.fp = fp; r-&gt;io.file.buffered = 0; r-&gt;io.file.autosync = 0;&#125; rio 的结构是这样的： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263 typedef struct _rio rio; /* * RIO API 接口和状态 */ struct _rio &#123; size_t (*read)(struct _rio *, void *buf, size_t len); size_t (*write)(struct _rio *, const void *buf, size_t len); off_t (*tell)(struct _rio *); void (*update_cksum)(struct _rio *, const void *buf, size_t len); // 当前校验和 uint64_t cksum; /* number of bytes read or written */ size_t processed_bytes; /* maximum single read or write chunk size */ size_t max_processing_chunk; union &#123; struct &#123; // 缓存指针 sds ptr; // 偏移量 off_t pos; &#125; buffer; struct &#123; // 被打开文件的指针 FILE *fp; // 最近一次 fsync() 以来，写入的字节量 off_t buffered; /* Bytes written since last fsync. */ // 写入多少字节之后，才会自动执行一次 fsync() off_t autosync; /* fsync after 'autosync' bytes written. */ &#125; file; &#125; io; &#125;; ``` 这里面主要是一些函数指针和变量- 接下来对 rdb 进行赋值, rioFileIO 是一个 static const 常量0 ```c *r = rioFileIO; /* * 流为文件时所使用的结构 */ static const rio rioFileIO = &#123; rioFileRead, // 读函数 rioFileWrite, // 写函数 rioFileTell, // 偏移量函数 NULL, /* update_checksum */ 0, /* current checksum */ 0, /* bytes read or written */ 0, /* read/write chunk size */ &#123; &#123; NULL, 0 &#125; &#125; /* union for io-specific vars */ &#125;; 几个函数指针是一层封装 1234567891011121314151617181920212223242526272829303132333435363738/* * 从文件 r 中读取 len 字节到 buf 中。返回值为读取的字节数。 */static size_t rioFileRead(rio *r, void *buf, size_t len) &#123; return fread(buf, len, 1, r-&gt;io.file.fp);&#125;/* Returns 1 or 0 for success/failure. * * 将长度为 len 的内容 buf 写入到文件 r 中。 * * 成功返回 1 ，失败返回 0 。 */static size_t rioFileWrite(rio *r, const void *buf, size_t len) &#123; size_t retval; retval = fwrite(buf, len, 1, r-&gt;io.file.fp); r-&gt;io.file.buffered += len; // 检查写入的字节数，看是否需要执行自动 sync if (r-&gt;io.file.autosync &amp;&amp; r-&gt;io.file.buffered &gt;= r-&gt;io.file.autosync) &#123; fflush(r-&gt;io.file.fp); aof_fsync(fileno(r-&gt;io.file.fp)); r-&gt;io.file.buffered = 0; &#125; return retval;&#125;/* Returns read/write position in file. * * 返回文件当前的偏移量 */static off_t rioFileTell(rio *r) &#123; return ftello(r-&gt;io.file.fp);&#125; 最后再把 rdb 的几个字段填充上 123r-&gt;io.file.fp = fp;r-&gt;io.file.buffered = 0;r-&gt;io.file.autosync = 0; 经过这一通操作， rio 对象和文件 File 就绑定上了。 写入 RDB 版本号 123456char magic[10];snprintf(magic, sizeof(magic), \"REDIS%04d\", REDIS_RDB_VERSION);if (rdbWriteRaw(&amp;rdb, magic, 9) == -1) &#123; goto werr;&#125; 首先把版本号写到字符串里面 1snprintf(magic, sizeof(magic), \"REDIS%04d\", 然后把字符串写到 rdb 文件中 12345678910111213141516171819202122232425262728293031323334/* * 将长度为 len 的字符数组 p 写入到 rdb 中。写入成功返回 len ，失败返回 -1 。 */static int rdbWriteRaw(rio *rdb, void *p, size_t len) &#123; if (rdb &amp;&amp; rioWrite(rdb, p, len) == 0) return -1; return len;&#125;/* * 将 buf 中的 len 字节写入到 r 中。写入成功返回实际写入的字节数，写入失败返回 -1 。 */static inline size_t rioWrite(rio *r, const void *buf, size_t len) &#123; while (len) &#123; size_t bytes_to_write = (r-&gt;max_processing_chunk &amp;&amp; r-&gt;max_processing_chunk &lt; len) ? r-&gt;max_processing_chunk : len; if (r-&gt;update_cksum) &#123; r-&gt;update_cksum(r,buf,bytes_to_write); &#125; if (r-&gt;write(r,buf,bytes_to_write) == 0) &#123; return 0; &#125; buf = (char*)buf + bytes_to_write; len -= bytes_to_write; r-&gt;processed_bytes += bytes_to_write; &#125; return 1;&#125; rioWrite 这个方法本身逻辑也挺简单的，主要是委托给了 r-&gt;write(r, buf, bytes_to_write) == 0 做底层的写操作。这个方法的主要作用是控制一次写 bytes 的数量不能大于 chunk_size 遍历数据库 123456789101112131415161718192021222324252627282930313233343536373839for (j = 0; j &lt; server.dbnum; j++) &#123; redisDb *db = server.db + j; dict *d = db-&gt;dict; if (dictSize(d) == 0) &#123; continue; &#125; di = dictGetSafeIterator(d); if (!di) &#123; fclose(fp); return REDIS_ERR; &#125; if (rdbSaveType(&amp;rdb, REDIS_RDB_OPCODE_SELECTDB) == -1) &#123; goto werr; &#125; if (rdbSaveLen(&amp;rdb, j) == -1) &#123; goto werr; &#125; // 遍历数据库，并写入每个键值对的数据 while ((de = dictNext(di)) != NULL) &#123; sds keystr = dictGetKey(de); robj key, *o = dictGetVal(de); long long expire; initStaticStringObject(key, keystr); expire = getExpire(db, &amp;key); if (rdbSaveKeyValuePair(&amp;rdb, &amp;key, o, expire, now) == -1) &#123; goto werr; &#125; &#125; dictReleaseIterator(di);&#125;di = NULL; 基本逻辑是把数据库中的每个键值对遍历一遍，然后写入，重点看 rdbSaveKeyValuePair 方法 123456789101112131415161718192021222324252627282930313233343536/* * 将键值对的键、值、过期时间和类型写入到 RDB 中。 * 出错返回 -1 。成功保存返回 1 ，当键已经过期时，返回 0 。 */int rdbSaveKeyValuePair( rio *rdb, robj *key, robj *val, long long expiretime, long long now) &#123; // 保存键的过期时间 if (expiretime != -1) &#123; // 不写入已经过期的键 if (expiretime &lt; now) &#123; return 0; &#125; if (rdbSaveType(rdb, REDIS_RDB_OPCODE_EXPIRETIME_MS) == -1) &#123; return -1; &#125; if (rdbSaveMillisecondTime(rdb, expiretime) == -1) &#123; return -1; &#125; &#125; // 保存类型，键，值 if (rdbSaveObjectType(rdb, val) == -1) &#123; return -1; &#125; if (rdbSaveStringObject(rdb, key) == -1) &#123; return -1; &#125; if (rdbSaveObject(rdb, val) == -1) &#123; return -1; &#125; return 1;&#125; 保存键的 expire 时间 rdbSaveType(rdb, REDIS_RDB_OPCODE_EXPIRETIME_MS) rdbSaveMillisecondTime(rdb, expiretime) 保存键的 type rdbSaveObjectType(rdb, val) 保存键 rdbSaveStringObject(rdb, key) 保存值 rdbSaveObject(rdb, val) 要注意的是，到目前未知，所有 write 的过程，底层调用的都是这个方法 123456789101112131415161718static size_t rioFileWrite(rio *r, const void *buf, size_t len) &#123; size_t retval; retval = fwrite(buf, len, 1, r-&gt;io.file.fp); r-&gt;io.file.buffered += len; // 检查写入的字节数，看是否需要执行自动 sync if (r-&gt;io.file.autosync &amp;&amp; r-&gt;io.file.buffered &gt;= r-&gt;io.file.autosync) &#123; fflush(r-&gt;io.file.fp); aof_fsync(fileno(r-&gt;io.file.fp)); r-&gt;io.file.buffered = 0; &#125; return retval;&#125; 在这个方法中，是否强制操作系统 fsync 取决于 开了 autosync + 待写字节数够了 写入 EOF 信息 1234// 写入 EOF 代码if (rdbSaveType(&amp;rdb, REDIS_RDB_OPCODE_EOF) == -1) &#123; goto werr;&#125; 写校验和 123cksum = rdb.cksum;memrev64ifbe(&amp;cksum);rioWrite(&amp;rdb, &amp;cksum, 8); 冲洗缓存，确保数据已写入磁盘 123456789if (fflush(fp) == EOF) &#123; goto werr;&#125;if (fsync(fileno(fp)) == -1) &#123; goto werr;&#125;if (fclose(fp) == EOF) &#123; goto werr;&#125; 使用 RENAME ，原子性地对临时文件进行改名，覆盖原来的 RDB 文件 12345if (rename(tmpfile, filename) == -1) &#123; redisLog(REDIS_WARNING, \"Error moving temp DB file on the final destination: %s\", strerror(errno)); unlink(tmpfile); return REDIS_ERR;&#125; 最后设置一些参数 12345678// 清零数据库脏状态server.dirty = 0;// 记录最后一次完成 SAVE 的时间server.lastsave = time(NULL);// 记录最后一次执行 SAVE 的状态server.lastbgsave_status = REDIS_OK; int rdbSaveBackground(char *filename)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778int rdbSaveBackground(char *filename) &#123; pid_t childpid; long long start; // 如果 BGSAVE 已经在执行，那么出错 if (server.rdb_child_pid != -1) &#123; return REDIS_ERR; &#125; // 记录 BGSAVE 执行前的数据库被修改次数 server.dirty_before_bgsave = server.dirty; // 最近一次尝试执行 BGSAVE 的时间 server.lastbgsave_try = time(NULL); // fork() 开始前的时间，记录 fork() 返回耗时用 start = ustime(); if ((childpid = fork()) == 0) &#123; int retval; /* Child */ // 关闭网络连接 fd closeListeningSockets(0); // 设置进程的标题，方便识别 redisSetProcTitle(\"redis-rdb-bgsave\"); // 执行保存操作 retval = rdbSave(filename); // 打印 copy-on-write 时使用的内存数 if (retval == REDIS_OK) &#123; size_t private_dirty = zmalloc_get_private_dirty(); if (private_dirty) &#123; redisLog(REDIS_NOTICE, \"RDB: %zu MB of memory used by copy-on-write\", private_dirty / (1024 * 1024)); &#125; &#125; // 向父进程发送信号 exitFromChild((retval == REDIS_OK) ? 0 : 1); &#125; else &#123; /* Parent */ // 计算 fork() 执行的时间 server.stat_fork_time = ustime() - start; // 如果 fork() 出错，那么报告错误 if (childpid == -1) &#123; server.lastbgsave_status = REDIS_ERR; redisLog(REDIS_WARNING, \"Can't save in background: fork: %s\", strerror(errno)); return REDIS_ERR; &#125; // 打印 BGSAVE 开始的日志 redisLog(REDIS_NOTICE, \"Background saving started by pid %d\", childpid); // 记录数据库开始 BGSAVE 的时间 server.rdb_save_time_start = time(NULL); // 记录负责执行 BGSAVE 的子进程 ID server.rdb_child_pid = childpid; // 关闭自动 rehash updateDictResizePolicy(); return REDIS_OK; &#125; return REDIS_OK; /* unreached */&#125; 先记录各种参数 12345678910111213// 如果 BGSAVE 已经在执行，那么出错if (server.rdb_child_pid != -1) &#123; return REDIS_ERR;&#125;// 记录 BGSAVE 执行前的数据库被修改次数server.dirty_before_bgsave = server.dirty;// 最近一次尝试执行 BGSAVE 的时间server.lastbgsave_try = time(NULL);// fork() 开始前的时间，记录 fork() 返回耗时用start = ustime(); fork 出子进程用于 BGSAVE 123456789101112131415161718192021222324252627282930if ((childpid = fork()) == 0) &#123; int retval; /* Child */ // 关闭网络连接 fd closeListeningSockets(0); // 设置进程的标题，方便识别 redisSetProcTitle(\"redis-rdb-bgsave\"); // 执行保存操作 retval = rdbSave(filename); // 打印 copy-on-write 时使用的内存数 if (retval == REDIS_OK) &#123; size_t private_dirty = zmalloc_get_private_dirty(); if (private_dirty) &#123; redisLog(REDIS_NOTICE, \"RDB: %zu MB of memory used by copy-on-write\", private_dirty / (1024 * 1024)); &#125; &#125; // 向父进程发送信号 exitFromChild((retval == REDIS_OK) ? 0 : 1);&#125; 关闭网络连接 fd 1234567891011121314151617181920212223242526closeListeningSockets(0);// 关闭监听套接字void closeListeningSockets(int unlink_unix_socket) &#123; int j; for (j = 0; j &lt; server.ipfd_count; j++) &#123; close(server.ipfd[j]); &#125; if (server.sofd != -1) &#123; close(server.sofd); &#125; if (server.cluster_enabled) &#123; for (j = 0; j &lt; server.cfd_count; j++) &#123; close(server.cfd[j]); &#125; &#125; if (unlink_unix_socket &amp;&amp; server.unixsocket) &#123; redisLog(REDIS_NOTICE, \"Removing the unix socket file.\"); unlink(server.unixsocket); /* don't care if this fails */ &#125;&#125; 关的 fd 包括 connfd, listenfd, 和 用于集群的 fd 设置进程的标题，方便识别 1redisSetProcTitle(\"redis-rdb-bgsave\"); 在子进程中执行保存 rdb 文件的操作 1retval = rdbSave(filename); 这个和直接执行 SAVE 操作的方法是同一个，只不过放到了子进程中 向父进程发送信号 123456789exitFromChild((retval == REDIS_OK) ? 0 : 1);void exitFromChild(int retcode) &#123; #ifdef COVERAGE_TEST exit(retcode); #else _exit(retcode); #endif&#125; backgroundSaveDoneHandler 函数用于处理 BGSAVE 完成时发送的信号 12345678910111213141516171819202122232425262728293031323334void backgroundSaveDoneHandler(int exitcode, int bysignal) &#123; // BGSAVE 成功 if (!bysignal &amp;&amp; exitcode == 0) &#123; redisLog(REDIS_NOTICE, \"Background saving terminated with success\"); server.dirty = server.dirty - server.dirty_before_bgsave; server.lastsave = time(NULL); server.lastbgsave_status = REDIS_OK; // BGSAVE 出错 &#125; else if (!bysignal &amp;&amp; exitcode != 0) &#123; redisLog(REDIS_WARNING, \"Background saving error\"); server.lastbgsave_status = REDIS_ERR; // BGSAVE 被中断 &#125; else &#123; redisLog(REDIS_WARNING, \"Background saving terminated by signal %d\", bysignal); // 移除临时文件 rdbRemoveTempFile(server.rdb_child_pid); if (bysignal != SIGUSR1) server.lastbgsave_status = REDIS_ERR; &#125; // 更新服务器状态 server.rdb_child_pid = -1; server.rdb_save_time_last = time(NULL) - server.rdb_save_time_start; server.rdb_save_time_start = -1; // 处理正在等待 BGSAVE 完成的那些 slave updateSlavesWaitingBgsave(exitcode == 0 ? REDIS_OK : REDIS_ERR);&#125; 这里面一共分了三种情况： 返回值为 0，且没有被中断 返回值非 0，且没有被中断 被中断：此时要释放 tmp file 123456789101112rdbRemoveTempFile(server.rdb_child_pid);/* * 移除 BGSAVE 所产生的临时文件，BGSAVE 执行被中断时使用 */void rdbRemoveTempFile(pid_t childpid) &#123; char tmpfile[256]; snprintf(tmpfile, 256, \"temp-%d.rdb\", (int) childpid); unlink(tmpfile);&#125; 父进程比较简单，基本上都是统计性质的工作 1234567891011121314151617181920212223242526272829else &#123; /* Parent */ // 计算 fork() 执行的时间 server.stat_fork_time = ustime() - start; // 如果 fork() 出错，那么报告错误 if (childpid == -1) &#123; server.lastbgsave_status = REDIS_ERR; redisLog(REDIS_WARNING, \"Can't save in background: fork: %s\", strerror(errno)); return REDIS_ERR; &#125; // 打印 BGSAVE 开始的日志 redisLog(REDIS_NOTICE, \"Background saving started by pid %d\", childpid); // 记录数据库开始 BGSAVE 的时间 server.rdb_save_time_start = time(NULL); // 记录负责执行 BGSAVE 的子进程 ID server.rdb_child_pid = childpid; // 关闭自动 rehash updateDictResizePolicy(); return REDIS_OK;&#125;","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"Redis","slug":"开源组件/Redis","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/"},{"name":"源码阅读","slug":"开源组件/Redis/源码阅读","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"},{"name":"C","slug":"C","permalink":"http://yoursite.com/tags/C/"}]},{"title":"Redis 源码阅读7_数据库实现相关_db","slug":"Redis源码阅读7_数据库实现相关_db","date":"2020-03-11T12:21:42.905Z","updated":"2020-03-11T12:22:47.000Z","comments":true,"path":"开源组件/Redis/源码阅读/Redis源码阅读7_数据库实现相关_db/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/Redis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB7_%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%9E%E7%8E%B0%E7%9B%B8%E5%85%B3_db/","excerpt":"","text":"Redis 的数据库实现。 涉及文件 结构体 API robj *lookupKeyReadOrReply(redisClient *c, robj *key, robj *reply) robj *lookupKeyWriteOrReply(redisClient *c, robj *key, robj *reply) void delCommand(redisClient *c) 涉及文件redis.h db.c 结构体1234567891011121314151617181920212223242526typedef struct redisDb &#123; // 数据库键空间，保存着数据库中的所有键值对 dict *dict; /* The keyspace for this DB */ // 键的过期时间，字典的键为键，字典的值为过期事件 UNIX 时间戳 dict *expires; /* Timeout of keys with a timeout set */ // 正处于阻塞状态的键 dict *blocking_keys; /* Keys with clients waiting for data (BLPOP) */ // 可以解除阻塞的键 dict *ready_keys; /* Blocked keys that received a PUSH */ // 正在被 WATCH 命令监视的键 dict *watched_keys; /* WATCHED keys for MULTI/EXEC CAS */ struct evictionPoolEntry *eviction_pool; /* Eviction pool of keys */ // 数据库号码 int id; /* Database ID */ // 数据库的键的平均 TTL ，统计信息 long long avg_ttl; /* Average TTL, just for stats */&#125; redisDb; API robj *lookupKeyReadOrReply(redisClient *c, robj *key, robj *reply) robj *lookupKeyWriteOrReply(redisClient *c, robj *key, robj *reply) void setKey(redisDb *db, robj *key, robj *val) void setExpire(redisDb *db, robj *key, long long when) void xxxCommand(redisClient* c); robj *lookupKeyReadOrReply(redisClient *c, robj *key, robj *reply)12345678910111213141516171819/* * 为执行读取操作而从数据库中查找返回 key 的值。 * * 如果 key 存在，那么返回 key 的值对象。 * * 如果 key 不存在，那么向客户端发送 reply 参数中的信息，并返回 NULL 。 */robj *lookupKeyReadOrReply(redisClient *c, robj *key, robj *reply) &#123; // 查找 robj *o = lookupKeyRead(c-&gt;db, key); // 决定是否发送信息 if (!o) &#125;&#123; addReply(c, reply); &#125; return o;&#125; addReply 在 “../3_数据类型实现/2_t_xxx.md” 看过了，这里重点看 lookupKeyRead： 12345678910111213141516171819202122232425/* * 为执行读取操作而取出键 key 在数据库 db 中的值。 * * 并根据是否成功找到值，更新服务器的命中/不命中信息。 * * 找到时返回值对象，没找到返回 NULL 。 */robj *lookupKeyRead(redisDb *db, robj *key) &#123; robj *val; // 检查 key 释放已经过期 expireIfNeeded(db, key); // 从数据库中取出键的值 val = lookupKey(db, key); // 更新命中/不命中信息 if (val == NULL) server.stat_keyspace_misses++; else server.stat_keyspace_hits++; // 返回值 return val;&#125; expireIfNeeded 也看过了， lookupKey 也挺简单的： 1234567891011121314151617181920212223242526272829/* * 从数据库 db 中取出键 key 的值（对象） * * 如果 key 的值存在，那么返回该值；否则，返回 NULL 。 */robj *lookupKey(redisDb *db, robj *key) &#123; // 查找键空间 dictEntry *de = dictFind(db-&gt;dict, key-&gt;ptr); // 节点存在 if (de) &#123; // 取出值 robj *val = dictGetVal(de); /* Update the access time for the ageing algorithm. * Don't do it if we have a saving child, as this will trigger * a copy on write madness. */ // 更新时间信息（只在不存在子进程时执行，防止破坏 copy-on-write 机制） if (server.rdb_child_pid == -1 &amp;&amp; server.aof_child_pid == -1) val-&gt;lru = LRU_CLOCK(); // 返回值 return val; &#125; else &#123; // 节点不存在 return NULL; &#125;&#125; robj *lookupKeyWriteOrReply(redisClient *c, robj *key, robj *reply)12345678910111213141516171819202122232425262728293031323334/* * 为执行写入操作而从数据库中查找返回 key 的值。 * * 如果 key 存在，那么返回 key 的值对象。 * * 如果 key 不存在，那么向客户端发送 reply 参数中的信息，并返回 NULL 。 */robj *lookupKeyWriteOrReply(redisClient *c, robj *key, robj *reply) &#123; robj *o = lookupKeyWrite(c-&gt;db, key); if (!o) &#123; addReply(c, reply); &#125; return o;&#125;/* * 为执行写入操作而取出键 key 在数据库 db 中的值。 * * 和 lookupKeyRead 不同，这个函数不会更新服务器的命中/不命中信息。 * * 找到时返回值对象，没找到返回 NULL 。 */robj *lookupKeyWrite(redisDb *db, robj *key) &#123; // 删除过期键 expireIfNeeded(db, key); // 查找并返回 key 的值对象 return lookupKey(db, key);&#125; 和 lookupKeyReadOrReply 基本上是一样的，唯一的不同是 read 的情况下要更新 命中/不命中 信息 void delCommand(redisClient *c)12345678910111213141516171819202122232425262728void delCommand(redisClient *c) &#123; int deleted = 0, j; // 遍历所有输入键 for (j = 1; j &lt; c-&gt;argc; j++) &#123; // 先删除过期的键 expireIfNeeded(c-&gt;db, c-&gt;argv[j]); // 尝试删除键 if (dbDelete(c-&gt;db, c-&gt;argv[j])) &#123; // 删除键成功，发送通知 signalModifiedKey(c-&gt;db, c-&gt;argv[j]); notifyKeyspaceEvent(REDIS_NOTIFY_GENERIC, \"del\", c-&gt;argv[j], c-&gt;db-&gt;id); server.dirty++; // 成功删除才增加 deleted 计数器的值 deleted++; &#125; &#125; // 返回被删除键的数量 addReplyLongLong(c, deleted);&#125; 首先看看 dbDelete 方法，逻辑是删除 expires 和 dict 的 key 123456789101112131415161718192021222324252627/* Delete a key, value, and associated expiration entry if any, from the DB * * 从数据库中删除给定的键，键的值，以及键的过期时间。 * * 删除成功返回 1 ，因为键不存在而导致删除失败时，返回 0 。 */int dbDelete(redisDb *db, robj *key) &#123; /* Deleting an entry from the expires dict will not free the sds of * the key, because it is shared with the main dictionary. */ // 删除键的过期时间 if (dictSize(db-&gt;expires) &gt; 0) &#123; dictDelete(db-&gt;expires, key-&gt;ptr); &#125; // 删除键值对 if (dictDelete(db-&gt;dict, key-&gt;ptr) == DICT_OK) &#123; // 如果开启了集群模式，那么从槽中删除给定的键 if (server.cluster_enabled) &#123; slotToKeyDel(key); &#125; return 1; &#125; else &#123; // 键不存在 return 0; &#125;&#125; 然后发送通知，一个通知事务 WATCH ，一个通知 pub-subscribe ，具体没细看 123signalModifiedKey(c-&gt;db, c-&gt;argv[j]);notifyKeyspaceEvent( REDIS_NOTIFY_GENERIC, \"del\", c-&gt;argv[j], c-&gt;db-&gt;id); 最后把删除键的数目 addReply","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"Redis","slug":"开源组件/Redis","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/"},{"name":"源码阅读","slug":"开源组件/Redis/源码阅读","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"},{"name":"C","slug":"C","permalink":"http://yoursite.com/tags/C/"}]},{"title":"Redis 源码阅读6_数据类型_键类型","slug":"Redis源码阅读6_数据类型_键类型","date":"2020-03-11T12:19:58.954Z","updated":"2020-03-11T12:20:43.000Z","comments":true,"path":"开源组件/Redis/源码阅读/Redis源码阅读6_数据类型_键类型/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/Redis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB6_%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B_%E9%94%AE%E7%B1%BB%E5%9E%8B/","excerpt":"","text":"总共有 5 个相关文件 文件 内容 t_string.c 字符串键的实现 t_list.c 列表键的实现 t_hash.c 散列键的实现 t_set.c 集合键的实现 t_zset.c 中除 zsl 开头的函数之外的所有函数。 有序集合键的实现 这 5 个文件里面实现的是各种 Redis 的命令的具体实现，仔细看一个： t_string.c SET key value [NX] [XX] [EX &lt;seconds&gt;] [PX &lt;milliseconds&gt;] 这个命令的意思是： 对 key 设置值为 value EX 和 PX 指定了 expire 的时间 NX 选项代表只在键不存在时，才对键进行设置操作。执行 SET key value NX 的效果等同于执行 SETNX key value XX 选项代表只在键已经存在时， 才对键进行设置操作 123456789101112131415161718192021222324252627282930313233343536373839void setCommand(redisClient *c) &#123; int j; robj *expire = NULL; int unit = UNIT_SECONDS; int flags = REDIS_SET_NO_FLAGS; // 设置选项参数 for (j = 3; j &lt; c-&gt;argc; j++) &#123; char *a = c-&gt;argv[j]-&gt;ptr; robj *next = (j == c-&gt;argc - 1) ? NULL : c-&gt;argv[j + 1]; if ((a[0] == 'n' || a[0] == 'N') &amp;&amp; (a[1] == 'x' || a[1] == 'X') &amp;&amp; a[2] == '\\0') &#123; flags |= REDIS_SET_NX; &#125; else if ((a[0] == 'x' || a[0] == 'X') &amp;&amp; (a[1] == 'x' || a[1] == 'X') &amp;&amp; a[2] == '\\0') &#123; flags |= REDIS_SET_XX; &#125; else if ((a[0] == 'e' || a[0] == 'E') &amp;&amp; (a[1] == 'x' || a[1] == 'X') &amp;&amp; a[2] == '\\0' &amp;&amp; next) &#123; unit = UNIT_SECONDS; expire = next; j++; &#125; else if ((a[0] == 'p' || a[0] == 'P') &amp;&amp; (a[1] == 'x' || a[1] == 'X') &amp;&amp; a[2] == '\\0' &amp;&amp; next) &#123; unit = UNIT_MILLISECONDS; expire = next; j++; &#125; else &#123; addReply(c, shared.syntaxerr); return; &#125; &#125; // 尝试对值对象进行编码 c-&gt;argv[2] = tryObjectEncoding(c-&gt;argv[2]); setGenericCommand(c, flags, c-&gt;argv[1], c-&gt;argv[2], expire, unit, NULL, NULL);&#125; 首先是把命令中的参数解析一下，设置 expire , unit 和 flags 三个变量。 1234567891011121314151617181920212223242526// 设置选项参数for (j = 3; j &lt; c-&gt;argc; j++) &#123; char *a = c-&gt;argv[j]-&gt;ptr; robj *next = (j == c-&gt;argc - 1) ? NULL : c-&gt;argv[j + 1]; if ((a[0] == 'n' || a[0] == 'N') &amp;&amp; (a[1] == 'x' || a[1] == 'X') &amp;&amp; a[2] == '\\0') &#123; flags |= REDIS_SET_NX; &#125; else if ((a[0] == 'x' || a[0] == 'X') &amp;&amp; (a[1] == 'x' || a[1] == 'X') &amp;&amp; a[2] == '\\0') &#123; flags |= REDIS_SET_XX; &#125; else if ((a[0] == 'e' || a[0] == 'E') &amp;&amp; (a[1] == 'x' || a[1] == 'X') &amp;&amp; a[2] == '\\0' &amp;&amp; next) &#123; unit = UNIT_SECONDS; expire = next; j++; &#125; else if ((a[0] == 'p' || a[0] == 'P') &amp;&amp; (a[1] == 'x' || a[1] == 'X') &amp;&amp; a[2] == '\\0' &amp;&amp; next) &#123; unit = UNIT_MILLISECONDS; expire = next; j++; &#125; else &#123; addReply(c, shared.syntaxerr); return; &#125;&#125; 然后进入了 setGenericCommand 函数，这个 setGenericCommand() 函数实现了 SET 、 SETEX 、 PSETEX 和 SETNX 命令。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859void setGenericCommand( redisClient *c, int flags, robj *key, robj *val, robj *expire, int unit, robj *ok_reply, robj *abort_reply) &#123; long long milliseconds = 0; /* initialized to avoid any harmness warning */ // 取出过期时间 if (expire) &#123; // 取出 expire 参数的值 // T = O(N) if (getLongLongFromObjectOrReply(c, expire, &amp;milliseconds, NULL) != REDIS_OK) return; // expire 参数的值不正确时报错 if (milliseconds &lt;= 0) &#123; addReplyError(c, \"invalid expire time in SETEX\"); return; &#125; // 不论输入的过期时间是秒还是毫秒 // Redis 实际都以毫秒的形式保存过期时间 // 如果输入的过期时间为秒，那么将它转换为毫秒 if (unit == UNIT_SECONDS) milliseconds *= 1000; &#125; // 如果设置了 NX 或者 XX 参数，那么检查条件是否不符合这两个设置 // 在条件不符合时报错，报错的内容由 abort_reply 参数决定 if ((flags &amp; REDIS_SET_NX &amp;&amp; lookupKeyWrite(c-&gt;db, key) != NULL) || (flags &amp; REDIS_SET_XX &amp;&amp; lookupKeyWrite(c-&gt;db, key) == NULL)) &#123; addReply(c, abort_reply ? abort_reply : shared.nullbulk); return; &#125; // 将键值关联到数据库 setKey(c-&gt;db, key, val); // 将数据库设为脏 server.dirty++; // 为键设置过期时间 if (expire) &#123; setExpire(c-&gt;db, key, mstime() + milliseconds); &#125; // 发送事件通知 notifyKeyspaceEvent(REDIS_NOTIFY_STRING, \"set\", key, c-&gt;db-&gt;id); // 发送事件通知 if (expire) &#123; notifyKeyspaceEvent(REDIS_NOTIFY_GENERIC, \"expire\", key, c-&gt;db-&gt;id); &#125; // 设置成功，向客户端发送回复 // 回复的内容由 ok_reply 决定 addReply(c, ok_reply ? ok_reply : shared.ok);&#125; 首先把 expire 时间取出来，转换成 ms 形式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104if (expire) &#123; // 取出 expire 参数的值 // T = O(N) if (getLongLongFromObjectOrReply( c, expire, &amp;milliseconds, NULL) != REDIS_OK) &#123; return; &#125; // expire 参数的值不正确时报错 if (milliseconds &lt;= 0) &#123; addReplyError(c, \"invalid expire time in SETEX\"); return; &#125; // 不论输入的过期时间是秒还是毫秒 // Redis 实际都以毫秒的形式保存过期时间 // 如果输入的过期时间为秒，那么将它转换为毫秒 if (unit == UNIT_SECONDS) &#123; milliseconds *= 1000; &#125;&#125;/* * 尝试从对象 o 中取出整数值， * 或者尝试将对象 o 中的值转换为整数值， * 并将这个得出的整数值保存到 *target 。 * * 如果取出/转换成功的话，返回 REDIS_OK 。 * 否则，返回 REDIS_ERR ，并向客户端发送一条出错回复。 * * T = O(N) */int getLongLongFromObjectOrReply( redisClient *c, robj *o, long long *target, const char *msg) &#123; long long value; // T = O(N) if (getLongLongFromObject(o, &amp;value) != REDIS_OK) &#123; if (msg != NULL) &#123; addReplyError(c, (char *) msg); &#125; else &#123; addReplyError(c, \"value is not an integer or out of range\"); &#125; return REDIS_ERR; &#125; *target = value; return REDIS_OK;&#125;/* * 尝试从对象 o 中取出整数值， * 或者尝试将对象 o 所保存的值转换为整数值， * 并将这个整数值保存到 *target 中。 * * 如果 o 为 NULL ，那么将 *target 设为 0 。 * * 如果对象 o 中的值不是整数，并且不能转换为整数，那么函数返回 REDIS_ERR 。 * * 成功取出或者成功进行转换时，返回 REDIS_OK 。 * * T = O(N) */int getLongLongFromObject(robj *o, long long *target) &#123; long long value; char *eptr; if (o == NULL) &#123; // o 为 NULL 时，将值设为 0 。 value = 0; &#125; else &#123; // 确保对象为 REDIS_STRING 类型 redisAssertWithInfo(NULL, o, o-&gt;type == REDIS_STRING); if (sdsEncodedObject(o)) &#123; errno = 0; // T = O(N) value = strtoll(o-&gt;ptr, &amp;eptr, 10); if (isspace(((char *) o-&gt;ptr)[0]) || eptr[0] != '\\0' || errno == ERANGE) return REDIS_ERR; &#125; else if (o-&gt;encoding == REDIS_ENCODING_INT) &#123; // 对于 REDIS_ENCODING_INT 编码的整数值 // 直接将它的值保存到 value 中 value = (long) o-&gt;ptr; &#125; else &#123; redisPanic(\"Unknown string encoding\"); &#125; &#125; // 保存值到指针 if (target) &#123; *target = value; &#125; // 返回结果标识符 return REDIS_OK;&#125; 这个函数没啥可看的，就是把字符串转为数字 如果设置了 NX 或者 XX 参数，那么要进行条件检查 123456789101112131415161718192021if ((flags &amp; REDIS_SET_NX &amp;&amp; lookupKeyWrite(c-&gt;db, key) != NULL) || (flags &amp; REDIS_SET_XX &amp;&amp; lookupKeyWrite(c-&gt;db, key) == NULL)) &#123; addReply(c, abort_reply ? abort_reply : shared.nullbulk); return;&#125;/* * 为执行写入操作而取出键 key 在数据库 db 中的值。 * * 和 lookupKeyRead 不同，这个函数不会更新服务器的命中/不命中信息。 * * 找到时返回值对象，没找到返回 NULL 。 */robj *lookupKeyWrite(redisDb *db, robj *key) &#123; // 删除过期键 expireIfNeeded(db, key); // 查找并返回 key 的值对象 return lookupKey(db, key);&#125; 删除过期键 12345678910111213141516171819202122232425262728293031int expireIfNeeded(redisDb *db, robj *key) &#123; mstime_t when = getExpire(db, key); mstime_t now; if (when &lt; 0) &#123; return 0; /* No expire for this key */ &#125; if (server.loading) &#123; return 0; &#125; now = server.lua_caller ? server.lua_time_start : mstime(); if (server.masterhost != NULL) &#123; return now &gt; when; &#125; if (now &lt;= when) &#123; return 0; &#125; server.stat_expiredkeys++; propagateExpire(db, key); notifyKeyspaceEvent(REDIS_NOTIFY_EXPIRED, \"expired\", key, db-&gt;id); return dbDelete(db, key);&#125; 原理是 redisServer 有两个字典，过期字典报错过期信息，用于读；然后进行各种情况判断，如果确定过期且该删除键的话，把键从 redisServer 真正的字典中删除 寻找 key 的值对象 1234567891011121314151617181920212223242526/* * 从数据库 db 中取出键 key 的值（对象） * * 如果 key 的值存在，那么返回该值；否则，返回 NULL 。 */robj *lookupKey(redisDb *db, robj *key) &#123; // 查找键空间 dictEntry *de = dictFind(db-&gt;dict, key-&gt;ptr); // 节点存在 if (de) &#123; // 取出值 robj *val = dictGetVal(de); // 更新时间信息（只在不存在子进程时执行，防止破坏 copy-on-write 机制） if (server.rdb_child_pid == -1 &amp;&amp; server.aof_child_pid == -1) val-&gt;lru = LRU_CLOCK(); // 返回值 return val; &#125; else &#123; // 节点不存在 return NULL; &#125;&#125; dictFind 方法在字典里研究过，这里注意的是由于访问过，所以会更新 lru 信息 在数据库中设置键值 1234567891011121314151617setKey(c-&gt;db, key, val);void setKey(redisDb *db, robj *key, robj *val) &#123; // 添加或覆写数据库中的键值对 if (lookupKeyWrite(db, key) == NULL) &#123; dbAdd(db, key, val); &#125; else &#123; dbOverwrite(db, key, val); &#125; incrRefCount(val); removeExpire(db, key); signalModifiedKey(db, key);&#125; 这个方法里面主要干了几件大事： 添加或覆写数据库中的键值对 (dictAdd 或 dictReplace) 值对象的引用计数会被增加 移除键的过期时间 监视键 key 的客户端会收到键已经被修改的通知 将数据库设为脏 1server.dirty++; 为键设置过期时间 12345678910111213141516if (expire) &#123; setExpire(c-&gt;db, key, mstime() + milliseconds);&#125;void setExpire(redisDb *db, robj *key, long long when) &#123; dictEntry *kde, *de; kde = dictFind(db-&gt;dict, key-&gt;ptr); redisAssertWithInfo(NULL, key, kde != NULL); de = dictReplaceRaw(db-&gt;expires, dictGetKey(kde)); dictSetSignedIntegerVal(de, when);&#125; 发送事件通知，针对的是 发布-订阅 高级功能 12345678// 发送事件通知notifyKeyspaceEvent(REDIS_NOTIFY_STRING, \"set\", key, c-&gt;db-&gt;id);// 发送事件通知if (expire) &#123; notifyKeyspaceEvent(REDIS_NOTIFY_GENERIC, \"expire\", key, c-&gt;db-&gt;id);&#125; 向客户端发送回复 1234567891011121314151617181920212223242526272829303132addReply(c, ok_reply ? ok_reply : shared.ok);void addReply(redisClient *c, robj *obj) &#123; if (prepareClientToWrite(c) != REDIS_OK) &#123; return; &#125; if (sdsEncodedObject(obj)) &#123; if (_addReplyToBuffer(c, obj-&gt;ptr, sdslen(obj-&gt;ptr)) != REDIS_OK) _addReplyObjectToList(c, obj); &#125; else if (obj-&gt;encoding == REDIS_ENCODING_INT) &#123; if (listLength(c-&gt;reply) == 0 &amp;&amp; (sizeof(c-&gt;buf) - c-&gt;bufpos) &gt;= 32) &#123; char buf[32]; int len; len = ll2string(buf, sizeof(buf), (long) obj-&gt;ptr); if (_addReplyToBuffer(c, buf, len) == REDIS_OK) return; &#125; obj = getDecodedObject(obj); if (_addReplyToBuffer(c, obj-&gt;ptr, sdslen(obj-&gt;ptr)) != REDIS_OK) _addReplyObjectToList(c, obj); decrRefCount(obj); &#125; else &#123; redisPanic(\"Wrong obj-&gt;encoding in addReply()\"); &#125;&#125; 回复的内容可以往两个位置写，一个是空间较小的固定 buffer，一个是链表。 首先尝试向固定大小的 buffer 中写 12345678910111213141516171819202122232425/* * 尝试将回复添加到 c-&gt;buf 中 */int _addReplyToBuffer(redisClient *c, char *s, size_t len) &#123; size_t available = sizeof(c-&gt;buf) - c-&gt;bufpos; if (c-&gt;flags &amp; REDIS_CLOSE_AFTER_REPLY) &#123; return REDIS_OK; &#125; if (listLength(c-&gt;reply) &gt; 0) &#123; return REDIS_ERR; &#125; if (len &gt; available) &#123; return REDIS_ERR; &#125; // 复制内容到 c-&gt;buf 里面 memcpy(c-&gt;buf + c-&gt;bufpos, s, len); c-&gt;bufpos += len; return REDIS_OK;&#125; 这是固定 buffer 的结构： char buf[REDIS_REPLY_CHUNK_BYTES]; RedisClient 里面有个 bufpos 指示可用的位置，只有空间足够时才往里面写 尝试失败了，就向链表式 buffer 里写 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* * 将回复对象（一个 SDS ）添加到 c-&gt;reply 回复链表中 */void _addReplyObjectToList(redisClient *c, robj *o) &#123; robj *tail; // 客户端即将被关闭，无须再发送回复 if (c-&gt;flags &amp; REDIS_CLOSE_AFTER_REPLY) &#123; return; &#125; // 链表中无缓冲块，直接将对象追加到链表中 if (listLength(c-&gt;reply) == 0) &#123; incrRefCount(o); listAddNodeTail(c-&gt;reply, o); // 链表中已有缓冲块，尝试将回复添加到块内 // 如果当前的块不能容纳回复的话，那么新建一个块 c-&gt;reply_bytes += getStringObjectSdsUsedMemory(o); &#125; else &#123; // 取出表尾的 SDS tail = listNodeValue(listLast(c-&gt;reply)); /* Append to this object when possible. */ // 如果表尾 SDS 的已用空间加上对象的长度，小于 REDIS_REPLY_CHUNK_BYTES // 那么将新对象的内容拼接到表尾 SDS 的末尾 if (tail-&gt;ptr != NULL &amp;&amp; tail-&gt;encoding == REDIS_ENCODING_RAW &amp;&amp; sdslen(tail-&gt;ptr) + sdslen(o-&gt;ptr) &lt;= REDIS_REPLY_CHUNK_BYTES) &#123; c-&gt;reply_bytes -= zmalloc_size_sds(tail-&gt;ptr); tail = dupLastObjectIfNeeded(c-&gt;reply); // 拼接 tail-&gt;ptr = sdscatlen(tail-&gt;ptr, o-&gt;ptr, sdslen(o-&gt;ptr)); c-&gt;reply_bytes += zmalloc_size_sds(tail-&gt;ptr); // 直接将对象追加到末尾 &#125; else &#123; incrRefCount(o); listAddNodeTail(c-&gt;reply, o); c-&gt;reply_bytes += getStringObjectSdsUsedMemory(o); &#125; &#125; // 检查回复缓冲区的大小，如果超过系统限制的话，那么关闭客户端 asyncCloseClientOnOutputBufferLimitReached(c);&#125; 这是链表 buffer 的结构 list *reply;，和 固定 buffer 不同的是，固定 buffer 只是一个数组，链表里可能有多个元素，每个元素都是一个 sds。 逻辑是：尽量复用链表尾的 sds，如果它空间不够的话，那就再开个 sds 要注意的是 addReply 只负责把要回复的内容往 buffer 中写好，socket 发的过程跟它无关。","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"Redis","slug":"开源组件/Redis","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/"},{"name":"源码阅读","slug":"开源组件/Redis/源码阅读","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"},{"name":"C","slug":"C","permalink":"http://yoursite.com/tags/C/"}]},{"title":"Redis 源码阅读5_数据类型_object","slug":"Redis源码阅读5_数据类型_object","date":"2020-03-11T12:18:28.175Z","updated":"2020-03-11T12:19:29.000Z","comments":true,"path":"开源组件/Redis/源码阅读/Redis源码阅读5_数据类型_object/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/Redis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB5_%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B_object/","excerpt":"","text":"Redis 的对象（类型）系统实现。 涉及文件 结构体 API 通用 robj *createObject(int type, void *ptr); STRING LIST 各种 free 方法 void freeStringObject(robj *o); void freeListObject(robj *o); void freeSetObject(robj *o); void freeZsetObject(robj *o); void freeHashObject(robj *o); 涉及文件object.c redis.h 结构体1234567891011121314151617181920#define REDIS_LRU_BITS 24typedef struct redisObject &#123; // 类型 unsigned type:4; // 编码 unsigned encoding:4; // 对象最后一次被访问的时间 unsigned lru:REDIS_LRU_BITS; /* lru time (relative to server.lruclock) */ // 引用计数 int refcount; // 指向实际值的指针 void *ptr;&#125; robj; 使用了 c 语言中的按位分配变量，节省空间 API通用robj *createObject(int type, void *ptr);12345678910111213141516/* * 创建一个新 robj 对象 */robj *createObject(int type, void *ptr) &#123; robj *o = zmalloc(sizeof(*o)); o-&gt;type = type; o-&gt;encoding = REDIS_ENCODING_RAW; o-&gt;ptr = ptr; o-&gt;refcount = 1; /* Set the LRU to the current lruclock (minutes resolution). */ o-&gt;lru = LRU_CLOCK(); return o;&#125; STRING robj *createStringObject(char *ptr, size_t len) robj *createRawStringObject(char *ptr, size_t len) robj *createEmbeddedStringObject(char *ptr, size_t len) robj *createStringObjectFromLongLong(long long value) robj *createStringObjectFromLongDouble(long double value) robj *dupStringObject(robj *o) STRING 类型的底层结构一共有三种： REDIS_ENCODING_RAW: SDS REDIS_ENCODING_EMBSTR：SDS，但是经过了 embstr 编码 REDIS_ENCODING_INT:整数值 随便看一个 1234567891011121314151617181920212223// 创建一个 REDIS_ENCODING_EMBSTR 编码的字符对象// 这个字符串对象中的 sds 会和字符串对象的 redisObject 结构一起分配// 因此这个字符也是不可修改的robj *createEmbeddedStringObject(char *ptr, size_t len) &#123; robj *o = zmalloc(sizeof(robj) + sizeof(struct sdshdr) + len + 1); struct sdshdr *sh = (void *) (o + 1); o-&gt;type = REDIS_STRING; o-&gt;encoding = REDIS_ENCODING_EMBSTR; o-&gt;ptr = sh + 1; o-&gt;refcount = 1; o-&gt;lru = LRU_CLOCK(); sh-&gt;len = len; sh-&gt;free = 0; if (ptr) &#123; memcpy(sh-&gt;buf, ptr, len); sh-&gt;buf[len] = '\\0'; &#125; else &#123; memset(sh-&gt;buf, 0, len + 1); &#125; return o;&#125; 这个方法把字符串对象和sds的内存分配到一起，然后把字符串拷贝到 sds 的 buf 中 LIST robj *createListObject(void) robj *createZiplistObject(void) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/* * 创建一个 LINKEDLIST 编码的列表对象 */robj *createListObject(void) &#123; list *l = listCreate(); robj *o = createObject(REDIS_LIST, l); listSetFreeMethod(l, decrRefCountVoid); o-&gt;encoding = REDIS_ENCODING_LINKEDLIST; return o;&#125;/* * 作用于特定数据结构的释放函数包装 */void decrRefCountVoid(void *o) &#123; decrRefCount(o);&#125;/* * 为对象的引用计数减一 * * 当对象的引用计数降为 0 时，释放对象。 */void decrRefCount(robj *o) &#123; if (o-&gt;refcount &lt;= 0) redisPanic(\"decrRefCount against refcount &lt;= 0\"); // 释放对象 if (o-&gt;refcount == 1) &#123; switch (o-&gt;type) &#123; case REDIS_STRING: freeStringObject(o); break; case REDIS_LIST: freeListObject(o); break; case REDIS_SET: freeSetObject(o); break; case REDIS_ZSET: freeZsetObject(o); break; case REDIS_HASH: freeHashObject(o); break; default: redisPanic(\"Unknown object type\"); break; &#125; zfree(o); // 减少计数 &#125; else &#123; o-&gt;refcount--; &#125;&#125; 挺简单的，重点是把 释放对象 的函数设置好 各种 Create 基本大同小异，大概都是先 new 一个底层结构，然后设置好 type 和 encoding。 看看 SET 的字典实现 123456789101112131415161718192021222324252627282930313233343536373839/* * 创建一个 SET 编码的集合对象 */robj *createSetObject(void) &#123; dict *d = dictCreate(&amp;setDictType, NULL); robj *o = createObject(REDIS_SET, d); o-&gt;encoding = REDIS_ENCODING_HT; return o;&#125;/* * 创建一个新的字典 * * T = O(1) */dict *dictCreate(dictType *type, void *privDataPtr) &#123; dict *d = zmalloc(sizeof(*d)); _dictInit(d, type, privDataPtr); return d;&#125;extern dictType setDictType;/* Sets type hash table */dictType setDictType = &#123; dictEncObjHash, /* hash function */ NULL, /* key dup */ NULL, /* val dup */ dictEncObjKeyCompare, /* key compare */ dictRedisObjectDestructor, /* key destructor */ NULL /* val destructor */&#125;; 各种 free 方法1234567891011121314151617181920212223242526272829303132333435363738/* * 为对象的引用计数减一 * * 当对象的引用计数降为 0 时，释放对象。 */void decrRefCount(robj *o) &#123; if (o-&gt;refcount &lt;= 0) redisPanic(\"decrRefCount against refcount &lt;= 0\"); // 释放对象 if (o-&gt;refcount == 1) &#123; switch (o-&gt;type) &#123; case REDIS_STRING: freeStringObject(o); break; case REDIS_LIST: freeListObject(o); break; case REDIS_SET: freeSetObject(o); break; case REDIS_ZSET: freeZsetObject(o); break; case REDIS_HASH: freeHashObject(o); break; default: redisPanic(\"Unknown object type\"); break; &#125; zfree(o); // 减少计数 &#125; else &#123; o-&gt;refcount--; &#125;&#125; void freeStringObject(robj *o);12345678910111213/* * 释放字符串对象 */void freeStringObject(robj *o) &#123; if (o-&gt;encoding == REDIS_ENCODING_RAW) &#123; sdsfree(o-&gt;ptr); &#125;&#125;void sdsfree(sds s) &#123; if (s == NULL) return; zfree(s - sizeof(struct sdshdr));&#125; void freeListObject(robj *o);1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/* * 释放列表对象 */void freeListObject(robj *o) &#123; switch (o-&gt;encoding) &#123; case REDIS_ENCODING_LINKEDLIST: listRelease((list *) o-&gt;ptr); break; case REDIS_ENCODING_ZIPLIST: zfree(o-&gt;ptr); break; default: redisPanic(\"Unknown list encoding type\"); &#125;&#125;/* * 释放整个链表，以及链表中所有节点 * * T = O(N) */void listRelease(list *list) &#123; unsigned long len; listNode *current, *next; // 指向头指针 current = list-&gt;head; // 遍历整个链表 len = list-&gt;len; while (len--) &#123; next = current-&gt;next; // 如果有设置值释放函数，那么调用它 if (list-&gt;free) &#123; list-&gt;free(current-&gt;value); &#125; // 释放节点结构 zfree(current); current = next; &#125; // 释放链表结构 zfree(list);&#125; void freeSetObject(robj *o);12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/* * 释放集合对象 */void freeSetObject(robj *o) &#123; switch (o-&gt;encoding) &#123; case REDIS_ENCODING_HT: dictRelease((dict *) o-&gt;ptr); break; case REDIS_ENCODING_INTSET: zfree(o-&gt;ptr); break; default: redisPanic(\"Unknown set encoding type\"); &#125;&#125;/* * 删除并释放整个字典 * * T = O(N) */void dictRelease(dict *d) &#123; // 删除并清空两个哈希表 _dictClear(d, &amp;d-&gt;ht[0], NULL); _dictClear(d, &amp;d-&gt;ht[1], NULL); // 释放节点结构 zfree(d);&#125;int _dictClear(dict *d, dictht *ht, void(callback)(void *)) &#123; unsigned long i; /* Free all the elements */ // 遍历整个哈希表 // T = O(N) for (i = 0; i &lt; ht-&gt;size &amp;&amp; ht-&gt;used &gt; 0; i++) &#123; dictEntry *he, *nextHe; if (callback &amp;&amp; (i &amp; 65535) == 0) callback(d-&gt;privdata); // 跳过空索引 if ((he = ht-&gt;table[i]) == NULL) continue; // 遍历整个链表 // T = O(1) while (he) &#123; nextHe = he-&gt;next; // 删除键 dictFreeKey(d, he); // 删除值 dictFreeVal(d, he); // 释放节点 zfree(he); // 更新已使用节点计数 ht-&gt;used--; // 处理下个节点 he = nextHe; &#125; &#125; /* Free the table and the allocated cache structure */ // 释放哈希表结构 zfree(ht-&gt;table); /* Re-initialize the table */ // 重置哈希表属性 _dictReset(ht); return DICT_OK; /* never fails */&#125; void freeZsetObject(robj *o);123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/* * 释放有序集合对象 */void freeZsetObject(robj *o) &#123; zset *zs; switch (o-&gt;encoding) &#123; case REDIS_ENCODING_SKIPLIST: zs = o-&gt;ptr; dictRelease(zs-&gt;dict); zslFree(zs-&gt;zsl); zfree(zs); break; case REDIS_ENCODING_ZIPLIST: zfree(o-&gt;ptr); break; default: redisPanic(\"Unknown sorted set encoding\"); &#125;&#125;/* * 释放给定跳跃表，以及表中的所有节点 * * T = O(N) */void zslFree(zskiplist *zsl) &#123; zskiplistNode *node = zsl-&gt;header-&gt;level[0].forward, *next; // 释放表头 zfree(zsl-&gt;header); // 释放表中所有节点 // T = O(N) while (node) &#123; next = node-&gt;level[0].forward; zslFreeNode(node); node = next; &#125; // 释放跳跃表结构 zfree(zsl);&#125; void freeHashObject(robj *o);1234567891011121314151617181920/* * 释放哈希对象 */void freeHashObject(robj *o) &#123; switch (o-&gt;encoding) &#123; case REDIS_ENCODING_HT: dictRelease((dict *) o-&gt;ptr); break; case REDIS_ENCODING_ZIPLIST: zfree(o-&gt;ptr); break; default: redisPanic(\"Unknown hash encoding type\"); break; &#125;&#125;","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"Redis","slug":"开源组件/Redis","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/"},{"name":"源码阅读","slug":"开源组件/Redis/源码阅读","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"},{"name":"C","slug":"C","permalink":"http://yoursite.com/tags/C/"}]},{"title":"Redis 源码阅读4_数据结构_skiplist","slug":"Redis源码阅读4_数据结构_skiplist","date":"2020-03-11T12:16:37.455Z","updated":"2020-03-11T12:17:25.000Z","comments":true,"path":"开源组件/Redis/源码阅读/Redis源码阅读4_数据结构_skiplist/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/Redis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB4_%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_skiplist/","excerpt":"","text":"跳跃表 涉及文件 结构体 zskiplistNode zskiplist API zskiplist *zslCreate(void); zskiplistNode *zslInsert(zskiplist *zsl, double score, robj *obj); int zslDelete(zskiplist *zsl, double score, robj *obj); 涉及文件redis.h t_zset.c 结构体zskiplistNode1234567891011121314151617181920212223242526/* * 跳跃表节点 */typedef struct zskiplistNode &#123; // 成员对象 robj *obj; // 分值 double score; // 后退指针 struct zskiplistNode *backward; // 层 struct zskiplistLevel &#123; // 前进指针 struct zskiplistNode *forward; // 跨度 unsigned int span; &#125; level[];&#125; zskiplistNode; zskiplist123456789101112131415/* * 跳跃表 */typedef struct zskiplist &#123; // 表头节点和表尾节点 struct zskiplistNode *header, *tail; // 表中节点的数量 unsigned long length; // 表中层数最大的节点的层数 int level;&#125; zskiplist; APIzskiplist *zslCreate(void); void zslFree(zskiplist *zsl); zskiplistNode *zslInsert(zskiplist *zsl, double score, robj *obj); unsigned char *zzlInsert(unsigned char *zl, robj *ele, double score); int zslDelete(zskiplist *zsl, double score, robj *obj); zskiplist *zslCreate(void);1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/* * 创建并返回一个新的跳跃表 * * T = O(1) */zskiplist *zslCreate(void) &#123; int j; zskiplist *zsl; // 分配空间 zsl = zmalloc(sizeof(*zsl)); // 设置高度和起始层数 zsl-&gt;level = 1; zsl-&gt;length = 0; // 初始化表头节点 // T = O(1) zsl-&gt;header = zslCreateNode(ZSKIPLIST_MAXLEVEL, 0, NULL); for (j = 0; j &lt; ZSKIPLIST_MAXLEVEL; j++) &#123; zsl-&gt;header-&gt;level[j].forward = NULL; zsl-&gt;header-&gt;level[j].span = 0; &#125; zsl-&gt;header-&gt;backward = NULL; // 设置表尾 zsl-&gt;tail = NULL; return zsl;&#125;/* * 创建一个层数为 level 的跳跃表节点， * 并将节点的成员对象设置为 obj ，分值设置为 score 。 * * 返回值为新创建的跳跃表节点 * * T = O(1) */zskiplistNode *zslCreateNode(int level, double score, robj *obj) &#123; // 分配空间 zskiplistNode *zn = zmalloc(sizeof(*zn) + level * sizeof(struct zskiplistLevel)); // 设置属性 zn-&gt;score = score; zn-&gt;obj = obj; return zn;&#125; 各种初始化，把 header 结点创建出来填充好 zskiplistNode *zslInsert(zskiplist *zsl, double score, robj *obj);123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118/* * 创建一个成员为 obj ，分值为 score 的新节点， * 并将这个新节点插入到跳跃表 zsl 中。 * * 函数的返回值为新节点。 * * T_wrost = O(N^2), T_avg = O(N log N) */zskiplistNode *zslInsert(zskiplist *zsl, double score, robj *obj) &#123; zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x; unsigned int rank[ZSKIPLIST_MAXLEVEL]; int i, level; redisAssert(!isnan(score)); // 在各个层查找节点的插入位置 // T_wrost = O(N^2), T_avg = O(N log N) x = zsl-&gt;header; for (i = zsl-&gt;level - 1; i &gt;= 0; i--) &#123; /* store rank that is crossed to reach the insert position */ // 如果 i 不是 zsl-&gt;level-1 层 // 那么 i 层的起始 rank 值为 i+1 层的 rank 值 // 各个层的 rank 值一层层累积 // 最终 rank[0] 的值加一就是新节点的前置节点的排位 // rank[0] 会在后面成为计算 span 值和 rank 值的基础 rank[i] = i == (zsl-&gt;level - 1) ? 0 : rank[i + 1]; // 沿着前进指针遍历跳跃表 // T_wrost = O(N^2), T_avg = O(N log N) while (x-&gt;level[i].forward &amp;&amp; (x-&gt;level[i].forward-&gt;score &lt; score || // 比对分值 (x-&gt;level[i].forward-&gt;score == score &amp;&amp; // 比对成员， T = O(N) compareStringObjects(x-&gt;level[i].forward-&gt;obj, obj) &lt; 0))) &#123; // 记录沿途跨越了多少个节点 rank[i] += x-&gt;level[i].span; // 移动至下一指针 x = x-&gt;level[i].forward; &#125; // 记录将要和新节点相连接的节点 update[i] = x; &#125; /* we assume the key is not already inside, since we allow duplicated * scores, and the re-insertion of score and redis object should never * happen since the caller of zslInsert() should test in the hash table * if the element is already inside or not. * * zslInsert() 的调用者会确保同分值且同成员的元素不会出现， * 所以这里不需要进一步进行检查，可以直接创建新元素。 */ // 获取一个随机值作为新节点的层数 // T = O(N) level = zslRandomLevel(); // 如果新节点的层数比表中其他节点的层数都要大 // 那么初始化表头节点中未使用的层，并将它们记录到 update 数组中 // 将来也指向新节点 if (level &gt; zsl-&gt;level) &#123; // 初始化未使用层 // T = O(1) for (i = zsl-&gt;level; i &lt; level; i++) &#123; rank[i] = 0; update[i] = zsl-&gt;header; update[i]-&gt;level[i].span = zsl-&gt;length; &#125; // 更新表中节点最大层数 zsl-&gt;level = level; &#125; // 创建新节点 x = zslCreateNode(level, score, obj); // 将前面记录的指针指向新节点，并做相应的设置 // T = O(1) for (i = 0; i &lt; level; i++) &#123; // 设置新节点的 forward 指针 x-&gt;level[i].forward = update[i]-&gt;level[i].forward; // 将沿途记录的各个节点的 forward 指针指向新节点 update[i]-&gt;level[i].forward = x; /* update span covered by update[i] as x is inserted here */ // 计算新节点跨越的节点数量 x-&gt;level[i].span = update[i]-&gt;level[i].span - (rank[0] - rank[i]); // 更新新节点插入之后，沿途节点的 span 值 // 其中的 +1 计算的是新节点 update[i]-&gt;level[i].span = (rank[0] - rank[i]) + 1; &#125; /* increment span for untouched levels */ // 未接触的节点的 span 值也需要增一，这些节点直接从表头指向新节点 // T = O(1) for (i = level; i &lt; zsl-&gt;level; i++) &#123; update[i]-&gt;level[i].span++; &#125; // 设置新节点的后退指针 x-&gt;backward = (update[0] == zsl-&gt;header) ? NULL : update[0]; if (x-&gt;level[0].forward) x-&gt;level[0].forward-&gt;backward = x; else zsl-&gt;tail = x; // 跳跃表的节点计数增一 zsl-&gt;length++; return x;&#125; 首先是找对于每一个 level，新插入的结点应该放在哪：具体做法是从最高层(跨度最大)，向下出溜，每一层的位置记录在 update 数组中，这个位置是恰好比要插入结点要一点点的位置 12345678910111213141516171819202122232425262728293031// 在各个层查找节点的插入位置// T_wrost = O(N^2), T_avg = O(N log N)x = zsl-&gt;header;for (i = zsl-&gt;level - 1; i &gt;= 0; i--) &#123; /* store rank that is crossed to reach the insert position */ // 如果 i 不是 zsl-&gt;level-1 层 // 那么 i 层的起始 rank 值为 i+1 层的 rank 值 // 各个层的 rank 值一层层累积 // 最终 rank[0] 的值加一就是新节点的前置节点的排位 // rank[0] 会在后面成为计算 span 值和 rank 值的基础 rank[i] = i == (zsl-&gt;level - 1) ? 0 : rank[i + 1]; // 沿着前进指针遍历跳跃表 // T_wrost = O(N^2), T_avg = O(N log N) while (x-&gt;level[i].forward &amp;&amp; (x-&gt;level[i].forward-&gt;score &lt; score || // 比对分值 (x-&gt;level[i].forward-&gt;score == score &amp;&amp; // 比对成员， T = O(N) compareStringObjects(x-&gt;level[i].forward-&gt;obj, obj) &lt; 0))) &#123; // 记录沿途跨越了多少个节点 rank[i] += x-&gt;level[i].span; // 移动至下一指针 x = x-&gt;level[i].forward; &#125; // 记录将要和新节点相连接的节点 update[i] = x;&#125; 然后创建新结点，随机一个 level 值，把链重新穿好 12345678910111213141516171819202122232425262728293031323334353637383940level = zslRandomLevel();// 如果新节点的层数比表中其他节点的层数都要大// 那么初始化表头节点中未使用的层，并将它们记录到 update 数组中// 将来也指向新节点if (level &gt; zsl-&gt;level) &#123; // 初始化未使用层 // T = O(1) for (i = zsl-&gt;level; i &lt; level; i++) &#123; rank[i] = 0; update[i] = zsl-&gt;header; update[i]-&gt;level[i].span = zsl-&gt;length; &#125; // 更新表中节点最大层数 zsl-&gt;level = level;&#125;// 创建新节点x = zslCreateNode(level, score, obj);// 将前面记录的指针指向新节点，并做相应的设置// T = O(1)for (i = 0; i &lt; level; i++) &#123; // 设置新节点的 forward 指针 x-&gt;level[i].forward = update[i]-&gt;level[i].forward; // 将沿途记录的各个节点的 forward 指针指向新节点 update[i]-&gt;level[i].forward = x; /* update span covered by update[i] as x is inserted here */ // 计算新节点跨越的节点数量 x-&gt;level[i].span = update[i]-&gt;level[i].span - (rank[0] - rank[i]); // 更新新节点插入之后，沿途节点的 span 值 // 其中的 +1 计算的是新节点 update[i]-&gt;level[i].span = (rank[0] - rank[i]) + 1;&#125; 最后做点收尾工作，设置一下各种东西 int zslDelete(zskiplist *zsl, double score, robj *obj);123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* * 从跳跃表 zsl 中删除包含给定节点 score 并且带有指定对象 obj 的节点。 * * T_wrost = O(N^2), T_avg = O(N log N) */int zslDelete(zskiplist *zsl, double score, robj *obj) &#123; zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x; int i; // 遍历跳跃表，查找目标节点，并记录所有沿途节点 // T_wrost = O(N^2), T_avg = O(N log N) x = zsl-&gt;header; for (i = zsl-&gt;level - 1; i &gt;= 0; i--) &#123; // 遍历跳跃表的复杂度为 T_wrost = O(N), T_avg = O(log N) while (x-&gt;level[i].forward &amp;&amp; (x-&gt;level[i].forward-&gt;score &lt; score || // 比对分值 (x-&gt;level[i].forward-&gt;score == score &amp;&amp; // 比对对象，T = O(N) compareStringObjects(x-&gt;level[i].forward-&gt;obj, obj) &lt; 0))) // 沿着前进指针移动 x = x-&gt;level[i].forward; // 记录沿途节点 update[i] = x; &#125; /* We may have multiple elements with the same score, what we need * is to find the element with both the right score and object. * * 检查找到的元素 x ，只有在它的分值和对象都相同时，才将它删除。 */ x = x-&gt;level[0].forward; if (x &amp;&amp; score == x-&gt;score &amp;&amp; equalStringObjects(x-&gt;obj, obj)) &#123; // T = O(1) zslDeleteNode(zsl, x, update); // T = O(1) zslFreeNode(x); return 1; &#125; else &#123; return 0; /* not found */ &#125; return 0; /* not found */&#125; 首先和 insert 差不多，也是先找到各层的待删除位置 1234567891011121314151617x = zsl-&gt;header;for (i = zsl-&gt;level - 1; i &gt;= 0; i--) &#123; // 遍历跳跃表的复杂度为 T_wrost = O(N), T_avg = O(log N) while (x-&gt;level[i].forward &amp;&amp; (x-&gt;level[i].forward-&gt;score &lt; score || // 比对分值 (x-&gt;level[i].forward-&gt;score == score &amp;&amp; // 比对对象，T = O(N) compareStringObjects(x-&gt;level[i].forward-&gt;obj, obj) &lt; 0))) // 沿着前进指针移动 x = x-&gt;level[i].forward; // 记录沿途节点 update[i] = x;&#125; 由于最底层的间隔是 1，所以被删除的结点如果存在的话，只能是 x = x-&gt;level[0].forward; 如果是它的话，就拆链 zslDeleteNode 123456789101112131415161718192021222324252627282930void zslDeleteNode(zskiplist *zsl, zskiplistNode *x, zskiplistNode **update) &#123; int i; // 更新所有和被删除节点 x 有关的节点的指针，解除它们之间的关系 // T = O(1) for (i = 0; i &lt; zsl-&gt;level; i++) &#123; if (update[i]-&gt;level[i].forward == x) &#123; update[i]-&gt;level[i].span += x-&gt;level[i].span - 1; update[i]-&gt;level[i].forward = x-&gt;level[i].forward; &#125; else &#123; update[i]-&gt;level[i].span -= 1; &#125; &#125; // 更新被删除节点 x 的前进和后退指针 if (x-&gt;level[0].forward) &#123; x-&gt;level[0].forward-&gt;backward = x-&gt;backward; &#125; else &#123; zsl-&gt;tail = x-&gt;backward; &#125; // 更新跳跃表最大层数（只在被删除节点是跳跃表中最高的节点时才执行） // T = O(1) while (zsl-&gt;level &gt; 1 &amp;&amp; zsl-&gt;header-&gt;level[zsl-&gt;level - 1].forward == NULL) zsl-&gt;level--; // 跳跃表节点计数器减一 zsl-&gt;length--;&#125; 最后把 结点本身 free 掉 12345678910/* * 释放给定的跳跃表节点 * * T = O(1) */void zslFreeNode(zskiplistNode *node) &#123; decrRefCount(node-&gt;obj); zfree(node);&#125; 由于 redis 用了 引用计数，所以 free之前还要看看持有的 robj 是否需要回收","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"Redis","slug":"开源组件/Redis","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/"},{"name":"源码阅读","slug":"开源组件/Redis/源码阅读","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"},{"name":"C","slug":"C","permalink":"http://yoursite.com/tags/C/"},{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"Redis 源码阅读3_数据结构_dict","slug":"Redis源码阅读3_数据结构_dict","date":"2020-03-11T12:14:52.388Z","updated":"2020-03-11T12:16:11.000Z","comments":true,"path":"开源组件/Redis/源码阅读/Redis源码阅读3_数据结构_dict/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/Redis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB3_%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_dict/","excerpt":"","text":"字典，实现了一个内存哈希表，它支持插入、删除、替换、查找和获取随机元素等操作。哈希表会自动在表的大小的二次方之间进行调整。键的冲突通过链表来解决。 涉及文件 结构体 dictEntry dictType dictht dict API 宏定义 Prototypes dict *dictCreate(dictType *type, void *privDataPtr); int dictReplace(dict *d, void *key, void *val); 涉及文件dict.h 和 dict.c 结构体dictEntry12345678910111213141516171819/* * 哈希表节点 */typedef struct dictEntry &#123; // 键 void *key; // 值 union &#123; void *val; uint64_t u64; int64_t s64; &#125; v; // 指向下个哈希表节点，形成链表 struct dictEntry *next;&#125; dictEntry; dictType123456789101112131415161718192021222324/* * 字典类型特定函数 */typedef struct dictType &#123; // 计算哈希值的函数 unsigned int (*hashFunction)(const void *key); // 复制键的函数 void *(*keyDup)(void *privdata, const void *key); // 复制值的函数 void *(*valDup)(void *privdata, const void *obj); // 对比键的函数 int (*keyCompare)(void *privdata, const void *key1, const void *key2); // 销毁键的函数 void (*keyDestructor)(void *privdata, void *key); // 销毁值的函数 void (*valDestructor)(void *privdata, void *obj);&#125; dictType; dictht123456789101112131415161718192021/* * 哈希表 * * 每个字典都使用两个哈希表，从而实现渐进式 rehash 。 */typedef struct dictht &#123; // 哈希表数组 dictEntry **table; // 哈希表大小 unsigned long size; // 哈希表大小掩码，用于计算索引值 // 总是等于 size - 1 unsigned long sizemask; // 该哈希表已有节点的数量 unsigned long used;&#125; dictht; dict12345678910111213141516171819202122/* * 字典 */typedef struct dict &#123; // 类型特定函数 dictType *type; // 私有数据 void *privdata; // 哈希表 dictht ht[2]; // rehash 索引 // 当 rehash 不在进行时，值为 -1 int rehashidx; /* rehashing not in progress if rehashidx == -1 */ // 目前正在运行的安全迭代器的数量 int iterators; /* number of iterators currently running */&#125; dict; API宏定义1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// 释放给定字典节点的值#define dictFreeVal(d, entry) \\ if ((d)-&gt;type-&gt;valDestructor) \\ (d)-&gt;type-&gt;valDestructor((d)-&gt;privdata, (entry)-&gt;v.val)// 设置给定字典节点的值#define dictSetVal(d, entry, _val_) do &#123; \\ if ((d)-&gt;type-&gt;valDup) \\ entry-&gt;v.val = (d)-&gt;type-&gt;valDup((d)-&gt;privdata, _val_); \\ else \\ entry-&gt;v.val = (_val_); \\&#125; while(0)// 将一个有符号整数设为节点的值#define dictSetSignedIntegerVal(entry, _val_) \\ do &#123; entry-&gt;v.s64 = _val_; &#125; while(0)// 将一个无符号整数设为节点的值#define dictSetUnsignedIntegerVal(entry, _val_) \\ do &#123; entry-&gt;v.u64 = _val_; &#125; while(0)// 释放给定字典节点的键#define dictFreeKey(d, entry) \\ if ((d)-&gt;type-&gt;keyDestructor) \\ (d)-&gt;type-&gt;keyDestructor((d)-&gt;privdata, (entry)-&gt;key)// 设置给定字典节点的键#define dictSetKey(d, entry, _key_) do &#123; \\ if ((d)-&gt;type-&gt;keyDup) \\ entry-&gt;key = (d)-&gt;type-&gt;keyDup((d)-&gt;privdata, _key_); \\ else \\ entry-&gt;key = (_key_); \\&#125; while(0)// 比对两个键#define dictCompareKeys(d, key1, key2) \\ (((d)-&gt;type-&gt;keyCompare) ? \\ (d)-&gt;type-&gt;keyCompare((d)-&gt;privdata, key1, key2) : \\ (key1) == (key2))// 计算给定键的哈希值#define dictHashKey(d, key) (d)-&gt;type-&gt;hashFunction(key)// 返回获取给定节点的键#define dictGetKey(he) ((he)-&gt;key)// 返回获取给定节点的值#define dictGetVal(he) ((he)-&gt;v.val)// 返回获取给定节点的有符号整数值#define dictGetSignedIntegerVal(he) ((he)-&gt;v.s64)// 返回给定节点的无符号整数值#define dictGetUnsignedIntegerVal(he) ((he)-&gt;v.u64)// 返回给定字典的大小#define dictSlots(d) ((d)-&gt;ht[0].size+(d)-&gt;ht[1].size)// 返回字典的已有节点数量#define dictSize(d) ((d)-&gt;ht[0].used+(d)-&gt;ht[1].used)// 查看字典是否正在 rehash#define dictIsRehashing(ht) ((ht)-&gt;rehashidx != -1) Prototypesdict *dictCreate(dictType *type, void *privDataPtr); int dictExpand(dict *d, unsigned long size); int dictAdd(dict *d, void *key, void *val); dictEntry *dictAddRaw(dict *d, void *key); int dictReplace(dict *d, void *key, void *val); dictEntry *dictReplaceRaw(dict *d, void *key); int dictDelete(dict *d, const void *key); int dictDeleteNoFree(dict *d, const void *key); void dictRelease(dict *d); dictEntry *dictFind(dict *d, const void *key); void *dictFetchValue(dict *d, const void *key); int dictResize(dict *d); dictIterator *dictGetIterator(dict *d); dictIterator *dictGetSafeIterator(dict *d); dictEntry *dictNext(dictIterator *iter); void dictReleaseIterator(dictIterator *iter); dictEntry *dictGetRandomKey(dict *d); int dictGetRandomKeys(dict d, dictEntry *des, int count); void dictPrintStats(dict *d); unsigned int dictGenHashFunction(const void *key, int len); unsigned int dictGenCaseHashFunction(const unsigned char *buf, int len); void dictEmpty(dict *d, void(callback)(void *)); void dictEnableResize(void); void dictDisableResize(void); int dictRehash(dict *d, int n); int dictRehashMilliseconds(dict *d, int ms); void dictSetHashFunctionSeed(unsigned int initval); unsigned int dictGetHashFunctionSeed(void); unsigned long dictScan(dict *d, unsigned long v, dictScanFunction *fn, void *privdata); dict *dictCreate(dictType *type, void *privDataPtr);1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/* * 创建一个新的字典 * * T = O(1) */dict *dictCreate(dictType *type, void *privDataPtr) &#123; dict *d = zmalloc(sizeof(*d)); _dictInit(d, type, privDataPtr); return d;&#125;/* * 初始化哈希表 * * T = O(1) */int _dictInit(dict *d, dictType *type, void *privDataPtr) &#123; // 初始化两个哈希表的各项属性值 // 但暂时还不分配内存给哈希表数组 _dictReset(&amp;d-&gt;ht[0]); _dictReset(&amp;d-&gt;ht[1]); // 设置类型特定函数 d-&gt;type = type; // 设置私有数据 d-&gt;privdata = privDataPtr; // 设置哈希表 rehash 状态 d-&gt;rehashidx = -1; // 设置字典的安全迭代器数量 d-&gt;iterators = 0; return DICT_OK;&#125;/* * 重置（或初始化）给定哈希表的各项属性值 * * * T = O(1) */static void _dictReset(dictht *ht) &#123; ht-&gt;table = NULL; ht-&gt;size = 0; ht-&gt;sizemask = 0; ht-&gt;used = 0;&#125; 字典的构造函数，需要依赖 dictType *type 和 void *privDataPtr：一个是 字典类型特定函数 ，另一个是私有数据指针。 设置各种属性，但是哈希表数组的内存不分配。 int dictReplace(dict *d, void *key, void *val);/* 将给定的键值对添加到字典中，如果键已经存在，那么删除旧有的键值对。 如果键值对为全新添加，那么返回 1 。 如果键值对是通过对原有的键值对更新得来的，那么返回 0 。 T = O(N) /int dictReplace(dict *d, void *key, void *val) { dictEntry *entry, auxentry; /* Try to add the element. If the key does not exists dictAdd will suceed. */// 尝试直接将键值对添加到字典// 如果键 key 不存在的话，添加会成功// T = O(N)if (dictAdd(d, key, val) == DICT_OK) return 1; /* It already exists, get the entry /// 运行到这里，说明键 key 已经存在，那么找出包含这个 key 的节点// T = O(1)entry = dictFind(d, key);/ Set the new value and free the old one. Note that it is important to do that in this order, as the value may just be exactly the same as the previous one. In this context, think to reference counting, you want to increment (set), and then decrement (free), and not the reverse. */// 先保存原有的值的指针auxentry = *entry;// 然后设置新的值// T = O(1)dictSetVal(d, entry, val);// 然后释放旧值// T = O(1)dictFreeVal(d, &amp;auxentry); return 0;} 首先，尝试直接将键值对添加到字典：如果键 key 不存在的话，添加会成功 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/* * 尝试将给定键值对添加到字典中 * * 只有给定键 key 不存在于字典时，添加操作才会成功 * * 添加成功返回 DICT_OK ，失败返回 DICT_ERR * * 最坏 T = O(N) ，平滩 O(1) */int dictAdd(dict *d, void *key, void *val) &#123; // 尝试添加键到字典，并返回包含了这个键的新哈希节点 // T = O(N) dictEntry *entry = dictAddRaw(d, key); // 键已存在，添加失败 if (!entry) &#123; return DICT_ERR; &#125; // 键不存在，设置节点的值 // T = O(1) dictSetVal(d, entry, val); // 添加成功 return DICT_OK;&#125;/* * 尝试将键插入到字典中 * * 如果键已经在字典存在，那么返回 NULL * * 如果键不存在，那么程序创建新的哈希节点， * 将节点和键关联，并插入到字典，然后返回节点本身。 * * T = O(N) */dictEntry *dictAddRaw(dict *d, void *key) &#123; int index; dictEntry *entry; dictht *ht; // 如果条件允许的话，进行单步 rehash // T = O(1) if (dictIsRehashing(d)) &#123; _dictRehashStep(d); &#125; // 计算键在哈希表中的索引值 // 如果值为 -1 ，那么表示键已经存在 // T = O(N) if ((index = _dictKeyIndex(d, key)) == -1) return NULL; // T = O(1) // 如果字典正在 rehash ，那么将新键添加到 1 号哈希表 // 否则，将新键添加到 0 号哈希表 ht = dictIsRehashing(d) ? &amp;d-&gt;ht[1] : &amp;d-&gt;ht[0]; // 为新节点分配空间 entry = zmalloc(sizeof(*entry)); // 将新节点插入到链表表头 entry-&gt;next = ht-&gt;table[index]; ht-&gt;table[index] = entry; // 更新哈希表已使用节点数量 ht-&gt;used++; /* Set the hash entry fields. */ // 设置新节点的键 // T = O(1) dictSetKey(d, entry, key); return entry;&#125; 首先找找 key 是否已存在 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* * 返回可以将 key 插入到哈希表的索引位置 * 如果 key 已经存在于哈希表，那么返回 -1 * * * 注意，如果字典正在进行 rehash ，那么总是返回 1 号哈希表的索引。 * 因为在字典进行 rehash 时，新节点总是插入到 1 号哈希表。 * * T = O(N) */static int _dictKeyIndex(dict *d, const void *key) &#123; unsigned int h, idx, table; dictEntry *he; /* Expand the hash table if needed */ // 单步 rehash // T = O(N) if (_dictExpandIfNeeded(d) == DICT_ERR) return -1; /* Compute the key hash value */ // 计算 key 的哈希值 h = dictHashKey(d, key); // T = O(1) for (table = 0; table &lt;= 1; table++) &#123; // 计算索引值 idx = h &amp; d-&gt;ht[table].sizemask; // 查找 key 是否存在 // T = O(1) he = d-&gt;ht[table].table[idx]; while (he) &#123; if (dictCompareKeys(d, key, he-&gt;key)) return -1; he = he-&gt;next; &#125; // 如果运行到这里时，说明 0 号哈希表中所有节点都不包含 key // 如果这时 rehahs 正在进行，那么继续对 1 号哈希表进行 rehash if (!dictIsRehashing(d)) break; &#125; // 返回索引值 return idx;&#125; 基本思路是先 hash，然后从 ht[0] 和 ht[1] 里面顺着链找 如果 key 没找到的话，那就创建一个新的结点，并且把新结点作为链表的头 头插法 12345// 为新节点分配空间entry = zmalloc(sizeof(*entry));// 将新节点插入到链表表头entry-&gt;next = ht-&gt;table[index];ht-&gt;table[index] = entry; 然后把 key 设置好就行了 如果键不存在，那么创建了一个新的 entry 以后，再把 val 设置好就行了 如果键存在，那就找一找包含这个 key 的结点在哪 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* * 返回字典中包含键 key 的节点 * * 找到返回节点，找不到返回 NULL * * T = O(1) */dictEntry *dictFind(dict *d, const void *key) &#123; dictEntry *he; unsigned int h, idx, table; // 字典（的哈希表）为空 if (d-&gt;ht[0].size == 0) return NULL; /* We don't have a table at all */ // 如果条件允许的话，进行单步 rehash if (dictIsRehashing(d)) _dictRehashStep(d); // 计算键的哈希值 h = dictHashKey(d, key); // 在字典的哈希表中查找这个键 // T = O(1) for (table = 0; table &lt;= 1; table++) &#123; // 计算索引值 idx = h &amp; d-&gt;ht[table].sizemask; // 遍历给定索引上的链表的所有节点，查找 key he = d-&gt;ht[table].table[idx]; // T = O(1) while (he) &#123; if (dictCompareKeys(d, key, he-&gt;key)) return he; he = he-&gt;next; &#125; // 如果程序遍历完 0 号哈希表，仍然没找到指定的键的节点 // 那么程序会检查字典是否在进行 rehash ， // 然后才决定是直接返回 NULL ，还是继续查找 1 号哈希表 if (!dictIsRehashing(d)) return NULL; &#125; // 进行到这里时，说明两个哈希表都没找到 return NULL;&#125; 最后替换一下 123456789101112131415161718192021// 先保存原有的值的指针auxentry = *entry;// 然后设置新的值// T = O(1)dictSetVal(d, entry, val);// 然后释放旧值// T = O(1)dictFreeVal(d, &amp;auxentry);// 设置给定字典节点的值#define dictSetVal(d, entry, _val_) do &#123; \\ if ((d)-&gt;type-&gt;valDup) \\ entry-&gt;v.val = (d)-&gt;type-&gt;valDup((d) -&gt;privdata, _val_); \\ else \\ entry-&gt;v.val = (_val_); \\&#125; while(0)// 释放给定字典节点的值#define dictFreeVal(d, entry) \\ if ((d)-&gt;type-&gt;valDestructor) \\ (d)-&gt;type-&gt;valDestructor((d)-&gt;privdata, (entry)-&gt;v.val) 这里面也是有操作的，要先 SetVal 再 FreeVal，因为新值和旧值有可能是一个，所以要先保存一份，dup，然后再 free","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"Redis","slug":"开源组件/Redis","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/"},{"name":"源码阅读","slug":"开源组件/Redis/源码阅读","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"},{"name":"C","slug":"C","permalink":"http://yoursite.com/tags/C/"},{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"Redis 源码阅读2_数据结构_adlist","slug":"Redis源码阅读2_数据结构_adlist","date":"2020-03-11T12:13:18.328Z","updated":"2020-03-11T12:14:10.000Z","comments":true,"path":"开源组件/Redis/源码阅读/Redis源码阅读2_数据结构_adlist/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/Redis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB2_%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_adlist/","excerpt":"","text":"双端链表 涉及文件 结构体 API list *listCreate(void); void listRelease(list *list); list *listInsertNode(list *list, listNode *old_node, void *value, int after); 涉及文件adlist.h 和 adlist.c 结构体123456789101112131415/* * 双端链表节点 */typedef struct listNode &#123; // 前置节点 struct listNode *prev; // 后置节点 struct listNode *next; // 节点的值 void *value;&#125; listNode; 双链表结构 123456789101112131415161718192021222324/* * 双端链表结构 */typedef struct list &#123; // 表头节点 listNode *head; // 表尾节点 listNode *tail; // 节点值复制函数 void *(*dup)(void *ptr); // 节点值释放函数 void (*free)(void *ptr); // 节点值对比函数 int (*match)(void *ptr, void *key); // 链表所包含的节点数量 unsigned long len;&#125; list; list 中包含三个函数指针 dup、free 和 match ，用于多态实现功能。 注： 函数指针的声明方法为：返回值类型 ( * 指针变量名) ([形参列表]); int func(int x); /* 声明一个函数 */ int (f) (int x); / 声明一个函数指针 */ f=func; /* 将func函数的首地址赋给指针f */ 或者使用下面的方法将函数地址赋给函数指针： f = &func; API首先是一大堆简单的宏定义： 1234567891011121314151617181920212223242526272829/* Functions implemented as macros */// 返回给定链表所包含的节点数量// T = O(1)#define listLength(l) ((l)-&gt;len)// 返回给定链表的表头节点// T = O(1)#define listFirst(l) ((l)-&gt;head)// 返回给定链表的表尾节点// T = O(1)#define listLast(l) ((l)-&gt;tail)// 返回给定节点的前置节点// T = O(1)#define listPrevNode(n) ((n)-&gt;prev)// 返回给定节点的后置节点// T = O(1)#define listNextNode(n) ((n)-&gt;next)// 返回给定节点的值// T = O(1)#define listNodeValue(n) ((n)-&gt;value)// 将链表 l 的值复制函数设置为 m// T = O(1)#define listSetDupMethod(l, m) ((l)-&gt;dup = (m))// 将链表 l 的值释放函数设置为 m// T = O(1)#define listSetFreeMethod(l, m) ((l)-&gt;free = (m))// 将链表的对比函数设置为 m// T = O(1)#define listSetMatchMethod(l, m) ((l)-&gt;match = (m)) 然后是真正的部分： list *listCreate(void); void listRelease(list *list); list *listAddNodeHead(list *list, void *value); list *listAddNodeTail(list *list, void *value); list *listInsertNode(list *list, listNode *old_node, void *value, int after); void listDelNode(list *list, listNode *node); listIter *listGetIterator(list *list, int direction); listNode *listNext(listIter *iter); void listReleaseIterator(listIter *iter); list *listDup(list *orig); listNode *listSearchKey(list *list, void *key); listNode *listIndex(list *list, long index); void listRewind(list *list, listIter *li); void listRewindTail(list *list, listIter *li); void listRotate(list *list); list *listCreate(void);1234567891011121314151617181920212223/* * 创建一个新的链表 * * 创建成功返回链表，失败返回 NULL 。 * * T = O(1) */list *listCreate(void) &#123; struct list *list; // 分配内存 if ((list = zmalloc(sizeof(*list))) == NULL) return NULL; // 初始化属性 list-&gt;head = list-&gt;tail = NULL; list-&gt;len = 0; list-&gt;dup = NULL; list-&gt;free = NULL; list-&gt;match = NULL; return list;&#125; 分配内存和各种初始化，不要被 zmalloc(sizeof(*list))) 骗了， list 是一个指针， list 又变成了 list 本身，所以 `sizeof(list)和sizeof(struct list)` 是一样的。 void listRelease(list *list);123456789101112131415161718192021222324252627282930&#x2F;* * 释放整个链表，以及链表中所有节点 * * T &#x3D; O(N) *&#x2F;void listRelease(list *list) &#123; unsigned long len; listNode *current, *next; &#x2F;&#x2F; 指向头指针 current &#x3D; list-&gt;head; &#x2F;&#x2F; 遍历整个链表 len &#x3D; list-&gt;len; while (len--) &#123; next &#x3D; current-&gt;next; &#x2F;&#x2F; 如果有设置值释放函数，那么调用它 if (list-&gt;free) &#123; list-&gt;free(current-&gt;value); &#125; &#x2F;&#x2F; 释放节点结构 zfree(current); current &#x3D; next; &#125; &#x2F;&#x2F; 释放链表结构 zfree(list);&#125; 先通过调用多态的 free 函数，释放链表中的结点；然后释放链表结构 zfree(list) list *listInsertNode(list *list, listNode *old_node, void *value, int after);1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/* * 创建一个包含值 value 的新节点，并将它插入到 old_node 的之前或之后 * * 如果 after 为 0 ，将新节点插入到 old_node 之前。 * 如果 after 为 1 ，将新节点插入到 old_node 之后。 * * T = O(1) */list *listInsertNode(list *list, listNode *old_node, void *value, int after) &#123; listNode *node; // 创建新节点 if ((node = zmalloc(sizeof(*node))) == NULL) return NULL; // 保存值 node-&gt;value = value; // 将新节点添加到给定节点之后 if (after) &#123; node-&gt;prev = old_node; node-&gt;next = old_node-&gt;next; // 给定节点是原表尾节点 if (list-&gt;tail == old_node) &#123; list-&gt;tail = node; &#125; // 将新节点添加到给定节点之前 &#125; else &#123; node-&gt;next = old_node; node-&gt;prev = old_node-&gt;prev; // 给定节点是原表头节点 if (list-&gt;head == old_node) &#123; list-&gt;head = node; &#125; &#125; // 更新新节点的前置指针 if (node-&gt;prev != NULL) &#123; node-&gt;prev-&gt;next = node; &#125; // 更新新节点的后置指针 if (node-&gt;next != NULL) &#123; node-&gt;next-&gt;prev = node; &#125; // 更新链表节点数 list-&gt;len++; return list;&#125; 挺简单的，就是拆链，插入，重组链","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"Redis","slug":"开源组件/Redis","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/"},{"name":"源码阅读","slug":"开源组件/Redis/源码阅读","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"},{"name":"C","slug":"C","permalink":"http://yoursite.com/tags/C/"},{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"Redis 源码阅读1_数据结构_sds","slug":"Redis源码阅读1_数据结构_sds","date":"2020-03-11T12:10:26.585Z","updated":"2020-03-11T12:11:55.000Z","comments":true,"path":"开源组件/Redis/源码阅读/Redis源码阅读1_数据结构_sds/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/Redis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB1_%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_sds/","excerpt":"","text":"简单动态字符串 sds 涉及文件 结构体 API sds sdsnew(const char *init); sds sdscat(sds s, const char *t); void sdsclear(sds s); 涉及文件sds.h 和 sds.c 结构体1234567891011121314/* * 保存字符串对象的结构 */struct sdshdr &#123; // buf 中已占用空间的长度 int len; // buf 中剩余可用空间的长度 int free; // 数据空间 char buf[];&#125;; char buf[] 是一个柔性数组结构，统计 sizeof(struct sdshdr) 时不包含 buf（因为也确实不知道） APIsds sdsnewlen(const void *init, size_t initlen); sds sdsnew(const char *init); sds sdsempty(void); size_t sdslen(const sds s); sds sdsdup(const sds s); void sdsfree(sds s); size_t sdsavail(const sds s); sds sdsgrowzero(sds s, size_t len); sds sdscatlen(sds s, const void *t, size_t len); sds sdscat(sds s, const char *t); sds sdscatsds(sds s, const sds t); sds sdscpylen(sds s, const char *t, size_t len); sds sdscpy(sds s, const char *t); sds sdscatvprintf(sds s, const char *fmt, va_list ap); sds sdscatfmt(sds s, char const *fmt, …); sds sdstrim(sds s, const char *cset); void sdsrange(sds s, int start, int end); void sdsupdatelen(sds s); void sdsclear(sds s); int sdscmp(const sds s1, const sds s2); sds *sdssplitlen(const char *s, int len, const char *sep, int seplen, int *count); void sdsfreesplitres(sds *tokens, int count); void sdstolower(sds s); void sdstoupper(sds s); sds sdsfromlonglong(long long value); sds sdscatrepr(sds s, const char *p, size_t len); sds *sdssplitargs(const char *line, int *argc); sds sdsmapchars(sds s, const char *from, const char *to, size_t setlen); sds sdsjoin(char **argv, int argc, char *sep); /* Low level functions exposed to the user API */sds sdsMakeRoomFor(sds s, size_t addlen); void sdsIncrLen(sds s, int incr); sds sdsRemoveFreeSpace(sds s); size_t sdsAllocSize(sds s); sds sdsnew(const char *init);123456789101112131415161718192021222324&#x2F;* * 类型别名，用于指向 sdshdr 的 buf 属性 *&#x2F;typedef char *sds;&#x2F;* * 根据给定字符串 init ，创建一个包含同样字符串的 sds * * 参数 * init ：如果输入为 NULL ，那么创建一个空白 sds * 否则，新创建的 sds 中包含和 init 内容相同字符串 * * 返回值 * sds ：创建成功返回 sdshdr 相对应的 sds * 创建失败返回 NULL * * 复杂度 * T &#x3D; O(N) *&#x2F;sds sdsnew(const char *init) &#123; size_t initlen &#x3D; (init &#x3D;&#x3D; NULL) ? 0 : strlen(init); return sdsnewlen(init, initlen);&#125; 首先用 string.h 获取字符串长度，然后看 sdsnewlen 方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/* * 根据给定的初始化字符串 init 和字符串长度 initlen * 创建一个新的 sds * * 参数 * init ：初始化字符串指针 * initlen ：初始化字符串的长度 * * 返回值 * sds ：创建成功返回 sdshdr 相对应的 sds * 创建失败返回 NULL * * 复杂度 * T = O(N) */sds sdsnewlen(const void *init, size_t initlen) &#123; struct sdshdr *sh; // 根据是否有初始化内容，选择适当的内存分配方式 // T = O(N) if (init) &#123; // zmalloc 不初始化所分配的内存 sh = zmalloc(sizeof(struct sdshdr) + initlen + 1); &#125; else &#123; // zcalloc 将分配的内存全部初始化为 0 sh = zcalloc(sizeof(struct sdshdr) + initlen + 1); &#125; // 内存分配失败，返回 if (sh == NULL) return NULL; // 设置初始化长度 sh-&gt;len = initlen; // 新 sds 不预留任何空间 sh-&gt;free = 0; // 如果有指定初始化内容，将它们复制到 sdshdr 的 buf 中 // T = O(N) if (initlen &amp;&amp; init) memcpy(sh-&gt;buf, init, initlen); // 以 \\0 结尾 sh-&gt;buf[initlen] = '\\0'; // 返回 buf 部分，而不是整个 sdshdr return (char *) sh-&gt;buf;&#125; 首先进行内存的分配/初始化，zmalloc 和 zcalloc 方法在 zmalloc.h / zmalloc.c 中定义和实现，具体细节未看； 然后把 sdshdr 的 len 和 free 属性设置好，用 string.h 中的 memcpy 把原始字符串的内容拷贝一份到 buf 中，然后给末尾添加上 ‘\\0’； 最后返回的是 sds 的 buf 部分 sds sdscat(sds s, const char *t);1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/* * 将给定字符串 t 追加到 sds 的末尾 * * 返回值 * sds ：追加成功返回新 sds ，失败返回 NULL * * 复杂度 * T = O(N) */sds sdscat(sds s, const char *t) &#123; return sdscatlen(s, t, strlen(t));&#125;/* * 将长度为 len 的字符串 t 追加到 sds 的字符串末尾 * * 返回值 * sds ：追加成功返回新 sds ，失败返回 NULL * * 复杂度 * T = O(N) */sds sdscatlen(sds s, const void *t, size_t len) &#123; struct sdshdr *sh; // 原有字符串长度 size_t curlen = sdslen(s); // 扩展 sds 空间 // T = O(N) s = sdsMakeRoomFor(s, len); // 内存不足？直接返回 if (s == NULL) &#123; return NULL; &#125; // 复制 t 中的内容到字符串后部 // T = O(N) sh = (void *) (s - (sizeof(struct sdshdr))); memcpy(s + curlen, t, len); // 更新属性 sh-&gt;len = curlen + len; sh-&gt;free = sh-&gt;free - len; // 添加新结尾符号 s[curlen + len] = '\\0'; // 返回新 sds return s;&#125; 首先获取 sds 当前的长度，但是这个也是有点操作的： 1234static inline size_t sdslen(const sds s) &#123; struct sdshdr *sh = (void *) (s - (sizeof(struct sdshdr))); return sh-&gt;len;&#125; sdshdr 中的 len 属性记录了长度，但是 s 指向的是 buf 部分的首地址，而 sizeof 不包含 buf 部分，所以 struct sdshdr *sh = (void *) (s - (sizeof(struct sdshdr))); 这个操作可以让指针恰好指向 sdshdr 的开头 扩展 sds 的空间，这个也是有点操作的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/* * 对 sds 中 buf 的长度进行扩展，确保在函数执行之后， * buf 至少会有 addlen + 1 长度的空余空间 * （额外的 1 字节是为 \\0 准备的） * * 返回值 * sds ：扩展成功返回扩展后的 sds * 扩展失败返回 NULL * * 复杂度 * T = O(N) */sds sdsMakeRoomFor(sds s, size_t addlen) &#123; struct sdshdr *sh, *newsh; // 获取 s 目前的空余空间长度 size_t free = sdsavail(s); size_t len, newlen; // s 目前的空余空间已经足够，无须再进行扩展，直接返回 if (free &gt;= addlen) &#123; return s; &#125; // 获取 s 目前已占用空间的长度 len = sdslen(s); sh = (void *) (s - (sizeof(struct sdshdr))); // s 最少需要的长度 newlen = (len + addlen); // 根据新长度，为 s 分配新空间所需的大小 if (newlen &lt; SDS_MAX_PREALLOC) // 如果新长度小于 SDS_MAX_PREALLOC // 那么为它分配两倍于所需长度的空间 newlen *= 2; else // 否则，分配长度为目前长度加上 SDS_MAX_PREALLOC newlen += SDS_MAX_PREALLOC; // T = O(N) newsh = zrealloc(sh, sizeof(struct sdshdr) + newlen + 1); // 内存不足，分配失败，返回 if (newsh == NULL) return NULL; // 更新 sds 的空余长度 newsh-&gt;free = newlen - len; // 返回 sds return newsh-&gt;buf;&#125; sdshdr 的 free 属性记录了 剩余空间大小，如果空间足够就啥也不干； 如果空间不够，要做的事情是 zrealloc，这个方法目标是新分配内存+把原来的 sdshdr 整体拷贝过去 + 释放原有空间 现在空间肯定是够了，把字符串放在原来的空间后面，加个 ‘\\0’，然后更新 len 和 free 属性 void sdsclear(sds s);12345678910111213141516171819/* * 在不释放 SDS 的字符串空间的情况下， * 重置 SDS 所保存的字符串为空字符串。 * * 复杂度 * T = O(1) */void sdsclear(sds s) &#123; // 取出 sdshdr struct sdshdr *sh = (void *) (s - (sizeof(struct sdshdr))); // 重新计算属性 sh-&gt;free += sh-&gt;len; sh-&gt;len = 0; // 将结束符放到最前面（相当于惰性地删除 buf 中的内容） sh-&gt;buf[0] = '\\0';&#125; 惰性空间释放，把属性更新一下，结束符放在 buf 最前面","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"Redis","slug":"开源组件/Redis","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/"},{"name":"源码阅读","slug":"开源组件/Redis/源码阅读","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/Redis/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"},{"name":"C","slug":"C","permalink":"http://yoursite.com/tags/C/"},{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"muduo 多线程模型：一个 Sudoku 服务器演变","slug":"muduo多线程模型：一个Sudoku服务器演变","date":"2020-03-09T03:01:57.280Z","updated":"2020-03-09T03:02:43.000Z","comments":true,"path":"开源组件/muduo/muduo多线程模型：一个Sudoku服务器演变/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/muduo%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B%EF%BC%9A%E4%B8%80%E4%B8%AASudoku%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%BC%94%E5%8F%98/","excerpt":"","text":"原文链接 https://blog.csdn.net/solstice/article/details/6548228 本文以一个 Sudoku Solver 为例，回顾了并发网络服务程序的多种设计方案，并介绍了使用 muduo 网络库编写多线程服务器的两种最常用手法。以往的例子展现了 Muduo 在编写单线程并发网络服务程序方面的能力与便捷性，今天我们看一看它在多线程方面的表现。 本文代码见：http://code.google.com/p/muduo/source/browse/trunk/examples/sudoku/ 下载：http://muduo.googlecode.com/files/muduo-0.2.5-alpha.tar.gz 关于数独的求解算法见 https://blog.csdn.net/Solstice/article/details/2096209 一、Sudoku Solver 协议 基本实现 二、常见的并发网络服务程序设计方案 三、结语 四、代码 方案 5：单线程 Reactor 方案 8：Reactor + Thread Pool 方案 9：Multiple Reactors 一、Sudoku Solver假设有这么一个网络编程任务：写一个求解数独的程序 (Sudoku Solver)，并把它做成一个网络服务。 Sudoku Solver 是我喜爱的网络编程例子，它曾经出现在《分布式系统部署、监控与进程管理的几重境界》、《Muduo 设计与实现之一：Buffer 类的设计》、《〈多线程服务器的适用场合〉例释与答疑》等文中，它也可以看成是 echo 服务的一个变种（《谈一谈网络编程学习经验》把 echo 列为三大 TCP 网络编程案例之一）。 写这么一个程序在网络编程方面的难度不高，跟写 echo 服务差不多（从网络连接读入一个 Sudoku 题目，算出答案，再发回给客户），挑战在于怎样做才能发挥现在多核硬件的能力？在谈这个问题之前，让我们先写一个基本的单线程版。 协议一个简单的以 /r/n 分隔的文本行协议，使用 TCP 长连接，客户端在不需要服务时主动断开连接。 请求：[id:]〈81digits〉/r/n 响应：[id:]〈81digits〉/r/n 或者 [id:]NoSolution/r/n 其中 [id:] 表示可选的 id，用于区分先后的请求，以支持 Parallel Pipelining，响应中会回显请求中的 id。Parallel Pipelining 的意义见赖勇浩的《以小见大——那些基于 protobuf 的五花八门的 RPC（2） 》，或者见我写的《分布式系统的工程化开发方法》第 54 页关于 out-of-order RPC 的介绍。 〈81digits〉是 Sudoku 的棋盘，9x9 个数字，未知数字以 0 表示。 如果 Sudoku 有解，那么响应是填满数字的棋盘；如果无解，则返回 NoSolution。 例子1： 123请求：000000010400000000020000000000050407008000300001090000300400200050100000000806000&#x2F;r&#x2F;n响应：693784512487512936125963874932651487568247391741398625319475268856129743274836159&#x2F;r&#x2F;n 例子2： 123请求：a:000000010400000000020000000000050407008000300001090000300400200050100000000806000&#x2F;r&#x2F;n响应：a:693784512487512936125963874932651487568247391741398625319475268856129743274836159&#x2F;r&#x2F;n 例子3： 123请求：b:000000010400000000020000000000050407008000300001090000300400200050100000000806005&#x2F;r&#x2F;n响应：b:NoSolution&#x2F;r&#x2F;n 基于这个文本协议，我们可以用 telnet 模拟客户端来测试 sudoku solver，不需要单独编写 sudoku client。SudokuSolver 的默认端口号是 9981，因为它有 9x9=81 个格子。 基本实现Sudoku 的求解算法见《谈谈数独(Sudoku)》一文，这不是本文的重点。假设我们已经有一个函数能求解 Sudoku，它的原型如下 string solveSudoku(const string&amp; puzzle); 函数的输入是上文的”〈81digits〉”，输出是”〈81digits〉”或”NoSolution”。这个函数是个 pure function，同时也是线程安全的。 有了这个函数，我们以《Muduo 网络编程示例之零：前言》中的 EchoServer 为蓝本，稍作修改就能得到 SudokuServer。这里只列出最关键的 onMessage() 函数，完整的代码见 http://code.google.com/p/muduo/source/browse/trunk/examples/sudoku/server_basic.cc 。onMessage() 的主要功能是处理协议格式，并调用 solveSudoku() 求解问题。 server_basic.cc 1234567891011121314151617181920212223242526void onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp) &#123; LOG_DEBUG &lt;&lt; conn-&gt;name(); size_t len = buf-&gt;readableBytes(); while (len &gt;= kCells + 2) &#123; const char *crlf = buf-&gt;findCRLF(); if (crlf) &#123; string request(buf-&gt;peek(), crlf); buf-&gt;retrieveUntil(crlf + 2); len = buf-&gt;readableBytes(); if (!processRequest(conn, request)) &#123; conn-&gt;send(\"Bad Request!\\r\\n\"); conn-&gt;shutdown(); break; &#125; &#125; else if (len &gt; 100) &#123; conn-&gt;send(\"Id too long!\\r\\n\"); conn-&gt;shutdown(); break; &#125; else &#123; break; &#125; &#125;&#125; server_basic.cc 是一个并发服务器，可以同时服务多个客户连接。但是它是单线程的，无法发挥多核硬件的能力。 Sudoku 是一个计算密集型的任务（见《Muduo 设计与实现之一：Buffer 类的设计》中关于其性能的分析），其瓶颈在 CPU。为了让这个单线程 server_basic 程序充分利用 CPU 资源，一个简单的办法是在同一台机器上部署多个 server_basic 进程，让每个进程占用不同的端口，比如在一台 8 核机器上部署 8 个 server_basic 进程，分别占用 9981、9982、……、9988 端口。这样做其实是把难题推给了客户端，因为客户端(s)要自己做负载均衡。再想得远一点，在 8 个 server_basic 前面部署一个 load balancer？似乎小题大做了。 能不能在一个端口上提供服务，并且又能发挥多核处理器的计算能力呢？当然可以，办法不止一种。 二、常见的并发网络服务程序设计方案W. Richard Stevens 的 UNP2e 第 27 章 Client-Server Design Alternatives 介绍了十来种当时（90 年代末）流行的编写并发网络程序的方案。UNP3e 第 30 章，内容未变，还是这几种。以下简称 UNP CSDA 方案。UNP 这本书主要讲解阻塞式网络编程，在非阻塞方面着墨不多，仅有一章。正确使用 non-blocking IO 需要考虑的问题很多，不适宜直接调用 Sockets API，而需要一个功能完善的网络库支撑。 随着 2000 年前后第一次互联网浪潮的兴起，业界对高并发 http 服务器的强烈需求大大推动了这一领域的研究，目前高性能 httpd 普遍采用的是单线程 reactor 方式。另外一个说法是 IBM Lotus 使用 TCP 长连接协议，而把 Lotus 服务端移植到 Linux 的过程中 IBM 的工程师们大大提高了 Linux 内核在处理并发连接方面的可伸缩性，因为一个公司可能有上万人同时上线，连接到同一台跑着 Lotus server 的 Linux 服务器。 可伸缩网络编程这个领域其实近十年来没什么新东西，POSA2 已经作了相当全面的总结，另外以下几篇文章也值得参考。 http://bulk.fefe.de/scalable-networking.pdf http://www.kegel.com/c10k.html http://gee.cs.oswego.edu/dl/cpjslides/nio.pdf 下表是陈硕总结的 10 种常见方案。其中“多连接互通”指的是如果开发 chat 服务，多个客户连接之间是否能方便地交换数据（chat 也是《谈一谈网络编程学习经验》中举的三大 TCP 网络编程案例之一）。对于 echo/http/sudoku 这类“连接相互独立”的服务程序，这个功能无足轻重，但是对于 chat 类服务至关重要。“顺序性”指的是在 http/sudoku 这类请求-响应服务中，如果客户连接顺序发送多个请求，那么计算得到的多个响应是否按相同的顺序发还给客户（这里指的是在自然条件下，不含刻意同步）。 方案 model UNP 对应 阻塞/非阻塞 多进程？ 多线程？ IO 复用？ 长连接？ 并发性 多核？ 开销 多连接互通？ 顺序性 线程数确定？ 特点 0 accept + read/write 0 阻塞 no no no no 无 no 低 no yes yes 一次服务一个客户 1 accept + fork 1 阻塞 yes no no yes 低 yes 高 no yes no process-per-connection 2 accept + thread 6 阻塞 no yes no yes 中 yes 中 yes yes no thread-per-connection 3 prefork 2/3/4/5 阻塞 yes no no yes 低 yes 高 no yes no 见 UNP 4 pre threaded 7/8 阻塞 no yes no yes 中 yes 中 yes yes no 见 UNP 5 poll(reactor) sec 6.8 非阻塞 no no yes yes 高 no 低 yes yes yes 单线程 reactor 6 reactor + thread-per-task 无 非阻塞 no yes yes yes 中 yes 中 yes no no thread-per-request 7 reactor + worker thread 无 非阻塞 no yes yes yes 中 yes 中 yes yes no worker-thread-per-connection 8 reactor + thread pool 无 非阻塞 no yes yes yes 高 yes 低 yes no yes 主线程 io + 工作线程计算 9 multiple reactors 无 非阻塞 no yes yes yes 高 yes 低 yes yes yes one-loop-per-thread UNP CSDA 方案归入 0~5。5 也是目前用得很多的单线程 reactor 方案，muduo 对此提供了很好的支持。6 和 7 其实不是实用的方案，只是作为过渡品。8 和 9 是本文重点介绍的方案，其实这两个方案已经在《多线程服务器的常用编程模型》一文中提到过，只不过当时我还没有写 muduo，无法用具体的代码示例来说明。 在对比各方案之前，我们先看看基本的 micro benchmark 数据（前三项由 lmbench 测得）： fork()+exit(): 160us pthread_create()+pthread_join(): 12us context switch : 1.5us sudoku resolve: 100us (根据题目难度不同，浮动范围 20~200us) 接下来看一下几种方案： 方案 0：这其实不是并发服务器，而是 iterative 服务器，因为它一次只能服务一个客户。代码见 UNP figure 1.9，UNP 以此为对比其他方案的基准点。这个方案不适合长连接，到是很适合 daytime 这种 write-only 服务。 方案 1：这是传统的 Unix 并发网络编程方案，UNP 称之为 child-per-client 或 fork()-per-client，另外也俗称 process-per-connection。这种方案适合并发连接数不大的情况。至今仍有一些网络服务程序用这种方式实现，比如 PostgreSQL 和 Perforce 的服务端。这种方案适合“计算响应的工作量远大于 fork() 的开销”这种情况，比如数据库服务器。这种方案适合长连接，但不太适合短连接，因为 fork() 开销大于求解 sudoku 的用时。 方案 2：这是传统的 Java 网络编程方案 thread-per-connection，在 Java 1.4 引入 NIO 之前，Java 网络服务程序多采用这种方案。它的初始化开销比方案 1 要小很多。这种方案的伸缩性受到线程数的限制，一两百个还行，几千个的话对操作系统的 scheduler 恐怕是个不小的负担。 方案 3：这是针对方案 1 的优化，UNP 详细分析了几种变化，包括对 accept 惊群问题的考虑。 方案 4：这是对方案 2 的优化，UNP 详细分析了它的几种变化。 以上几种方案都是阻塞式网络编程，程序（thread-of-control）通常阻塞在 read() 上，等待数据到达。但是 TCP 是个全双工协议，同时支持 read() 和 write() 操作，当一个线程/进程阻塞在 read() 上，但程序又想给这个 TCP 连接发数据，那该怎么办？比如说 echo client，既要从 stdin 读，又要从网络读，当程序正在阻塞地读网络的时候，如何处理键盘输入？又比如 proxy，既要把连接 a 收到的数据发给连接 b，又要把从连接 b 收到的数据发给连接 a，那么到底读哪个？（proxy 是《谈一谈网络编程学习经验》中举的三大 TCP 网络编程案例之一。） 一种方法是用两个线程/进程，一个负责读，一个负责写。UNP 也在实现 echo client 时介绍了这种方案。另外见 Python Pinhole 的代码：http://code.activestate.com/recipes/114642/ ((另一种方法))是使用 IO multiplexing，也就是 select/poll/epoll/kqueue 这一系列的“多路选择器”，让一个 thread-of-control 能处理多个连接。“IO 复用”其实复用的不是 IO 连接，而是复用线程。使用 select/poll 几乎肯定要配合 non-blocking IO，而使用 non-blocking IO 肯定要使用应用层 buffer，原因见《Muduo 设计与实现之一：Buffer 类的设计》。这就不是一件轻松的事儿了，如果每个程序都去搞一套自己的 IO multiplexing 机制（本质是 event-driven 事件驱动），这是一种很大的浪费。感谢 Doug Schmidt 为我们总结出了 Reactor 模式，让 event-driven 网络编程有章可循。继而出现了一些通用的 reactor 框架/库，比如 libevent、muduo、Netty、twisted、POE 等等，有了这些库，我想基本不用去编写阻塞式的网络程序了（特殊情况除外，比如 proxy 流量限制）。 单线程 reactor 的程序结构是（图片取自 Doug Lea 的演讲）： 方案 5：基本的单线程 reactor 方案，即前面的 server_basic.cc 程序。本文以它作为对比其他方案的基准点。这种方案的优点是由网络库搞定数据收发，程序只关心业务逻辑；缺点在前面已经谈了：适合 IO 密集的应用，不太适合 CPU 密集的应用，因为较难发挥多核的威力。 方案 6：这是一个过渡方案，收到 Sudoku 请求之后，不在 reactor 线程计算，而是创建一个新线程去计算，以充分利用多核 CPU。这是非常初级的多线程应用，因为它为每个请求（而不是每个连接）创建了一个新线程。这个开销可以用线程池来避免，即方案 8。这个方案还有一个特点是 out-of-order，即同时创建多个线程去计算同一个连接上收到的多个请求，那么算出结果的次序是不确定的，可能第 2 个 Sudoku 比较简单，比第 1 个先算出结果。这也是为什么我们在一开始设计协议的时候使用了 id，以便客户端区分 response 对应的是哪个 request。 方案 7：为了让返回结果的顺序确定，我们可以为每个连接创建一个计算线程，每个连接上的请求固定发给同一个线程去算，先到先得。这也是一个过渡方案，因为并发连接数受限于线程数目，这个方案或许还不如直接使用阻塞 IO 的 thread-per-connection 方案2。方案 7 与方案 6 的另外一个区别是一个 client 的最大 CPU 占用率，在方案 6 中，一个 connection 上发来的一长串突发请求(burst requests) 可以占满全部 8 个 core；而在方案 7 中，由于每个连接上的请求固定由同一个线程处理，那么它最多占用 12.5% 的 CPU 资源。这两种方案各有优劣，取决于应用场景的需要，到底是公平性重要还是突发性能重要。这个区别在方案 8 和方案 9 中同样存在，需要根据应用来取舍。 方案 8：为了弥补方案 6 中为每个请求创建线程的缺陷，我们使用固定大小线程池，程序结构如下图。全部的 IO 工作都在一个 reactor 线程完成，而计算任务交给 thread pool。如果计算任务彼此独立，而且 IO 的压力不大，那么这种方案是非常适用的。Sudoku Solver 正好符合。代码见：http://code.google.com/p/muduo/source/browse/trunk/examples/sudoku/server_threadpool.cc 后文给出了它与方案 9 的区别。 如果 IO 的压力比较大，一个 reactor 忙不过来，可以试试 multiple reactors 的方案 9。 方案 9：这是 muduo 内置的多线程方案，也是 Netty 内置的多线程方案。这种方案的特点是 one loop per thread，有一个 main reactor 负责 accept 连接，然后把连接挂在某个 sub reactor 中（muduo 采用 round-robin 的方式来选择 sub reactor），这样该连接的所有操作都在那个 sub reactor 所处的线程中完成。多个连接可能被分派到多个线程中，以充分利用 CPU。Muduo 采用的是固定大小的 reactor pool，池子的大小通常根据 CPU 核数确定，也就是说线程数是固定的，这样程序的总体处理能力不会随连接数增加而下降。另外，由于一个连接完全由一个线程管理，那么请求的顺序性有保证，突发请求也不会占满全部 8 个核（如果需要优化突发请求，可以考虑方案 10）。这种方案把 IO 分派给多个线程，防止出现一个 reactor 的处理能力饱和。与方案 8 的线程池相比，方案 9 减少了进出 thread pool 的两次上下文切换。我认为这是一个适应性很强的多线程 IO 模型，因此把它作为 muduo 的默认线程模型。 方案 10：把方案 8 和方案 90 混合，既使用多个 reactors 来处理 IO，又使用线程池来处理计算。这种方案适合既有突发 IO （利用多线程处理多个连接上的 IO），又有突发计算的应用（利用线程池把一个连接上的计算任务分配给多个线程去做）。 这种其实方案看起来复杂，其实写起来很简单，只要把方案 8 的代码加一行 server_.setThreadNum(numThreads); 就行，这里就不举例了。 三、结语我在《多线程服务器的常用编程模型》一文中说 总结起来，我推荐的多线程服务端编程模式为：event loop per thread + thread pool。 event loop 用作 non-blocking IO 和定时器。thread pool 用来做计算，具体可以是任务队列或消费者-生产者队列。 当时（2010年2月）我还说“以这种方式写服务器程序，需要一个优质的基于 Reactor 模式的网络库来支撑，我只用过in-house的产品，无从比较并推荐市面上常见的 C++ 网络库，抱歉。” 现在有了 muduo 网络库，我终于能够用具体的代码示例把思想完整地表达出来。 四、代码方案 5：单线程 Reactorserver_basic.cc 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475class SudokuServer &#123;public: SudokuServer(EventLoop *loop, const InetAddress &amp;listenAddr) : server_(loop, listenAddr, \"SudokuServer\"), startTime_(Timestamp::now()) &#123; server_.setConnectionCallback( std::bind(&amp;SudokuServer::onConnection, this, _1)); server_.setMessageCallback( std::bind(&amp;SudokuServer::onMessage, this, _1, _2, _3)); &#125; void start() &#123; server_.start(); &#125;private: void onConnection(const TcpConnectionPtr &amp;conn) &#123; ... &#125; void onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp) &#123; LOG_DEBUG &lt;&lt; conn-&gt;name(); size_t len = buf-&gt;readableBytes(); while (len &gt;= kCells + 2) &#123; const char *crlf = buf-&gt;findCRLF(); if (crlf) &#123; string request(buf-&gt;peek(), crlf); buf-&gt;retrieveUntil(crlf + 2); len = buf-&gt;readableBytes(); if (!processRequest(conn, request)) &#123; conn-&gt;send(\"Bad Request!\\r\\n\"); conn-&gt;shutdown(); break; &#125; &#125; else if (len &gt; 100) // id + \":\" + kCells + \"\\r\\n\" &#123; conn-&gt;send(\"Id too long!\\r\\n\"); conn-&gt;shutdown(); break; &#125; else &#123; break; &#125; &#125; &#125; bool processRequest(const TcpConnectionPtr &amp;conn, const string &amp;request) &#123; string id; string puzzle; bool goodRequest = true; string::const_iterator colon = find(request.begin(), request.end(), ':'); if (colon != request.end()) &#123; id.assign(request.begin(), colon); puzzle.assign(colon + 1, request.end()); &#125; else &#123; puzzle = request; &#125; if (puzzle.size() == implicit_cast&lt;size_t&gt;(kCells)) &#123; LOG_DEBUG &lt;&lt; conn-&gt;name(); string result = solveSudoku(puzzle); if (id.empty()) &#123; conn-&gt;send(result + \"\\r\\n\"); &#125; else &#123; conn-&gt;send(id + \":\" + result + \"\\r\\n\"); &#125; &#125; else &#123; goodRequest = false; &#125; return goodRequest; &#125; TcpServer server_; Timestamp startTime_;&#125;; 方案 8：Reactor + Thread Poolserver_threadpool.cc 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586class SudokuServer &#123;public: SudokuServer(EventLoop *loop, const InetAddress &amp;listenAddr, int numThreads) : server_(loop, listenAddr, \"SudokuServer\"), numThreads_(numThreads), startTime_(Timestamp::now()) &#123; server_.setConnectionCallback( std::bind(&amp;SudokuServer::onConnection, this, _1)); server_.setMessageCallback( std::bind(&amp;SudokuServer::onMessage, this, _1, _2, _3)); &#125; void start() &#123; LOG_INFO &lt;&lt; \"starting \" &lt;&lt; numThreads_ &lt;&lt; \" threads.\"; threadPool_.start(numThreads_); server_.start(); &#125;private: void onConnection(const TcpConnectionPtr &amp;conn) &#123; ... &#125; void onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp) &#123; LOG_DEBUG &lt;&lt; conn-&gt;name(); size_t len = buf-&gt;readableBytes(); while (len &gt;= kCells + 2) &#123; const char *crlf = buf-&gt;findCRLF(); if (crlf) &#123; string request(buf-&gt;peek(), crlf); buf-&gt;retrieveUntil(crlf + 2); len = buf-&gt;readableBytes(); if (!processRequest(conn, request)) &#123; conn-&gt;send(\"Bad Request!\\r\\n\"); conn-&gt;shutdown(); break; &#125; &#125; else if (len &gt; 100) &#123; conn-&gt;send(\"Id too long!\\r\\n\"); conn-&gt;shutdown(); break; &#125; else &#123; break; &#125; &#125; &#125; bool processRequest(const TcpConnectionPtr &amp;conn, const string &amp;request) &#123; string id; string puzzle; bool goodRequest = true; string::const_iterator colon = find(request.begin(), request.end(), ':'); if (colon != request.end()) &#123; id.assign(request.begin(), colon); puzzle.assign(colon + 1, request.end()); &#125; else &#123; puzzle = request; &#125; if (puzzle.size() == implicit_cast&lt;size_t&gt;(kCells)) &#123; threadPool_.run(std::bind(&amp;solve, conn, puzzle, id)); &#125; else &#123; goodRequest = false; &#125; return goodRequest; &#125; static void solve(const TcpConnectionPtr &amp;conn, const string &amp;puzzle, const string &amp;id) &#123; LOG_DEBUG &lt;&lt; conn-&gt;name(); string result = solveSudoku(puzzle); if (id.empty()) &#123; conn-&gt;send(result + \"\\r\\n\"); &#125; else &#123; conn-&gt;send(id + \":\" + result + \"\\r\\n\"); &#125; &#125; TcpServer server_; ThreadPool threadPool_; int numThreads_; Timestamp startTime_;&#125;; 方案 9：Multiple Reactorsserver_multiloop.cc 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879class SudokuServer &#123;public: SudokuServer(EventLoop *loop, const InetAddress &amp;listenAddr, int numThreads) : server_(loop, listenAddr, \"SudokuServer\"), numThreads_(numThreads), startTime_(Timestamp::now()) &#123; server_.setConnectionCallback( std::bind(&amp;SudokuServer::onConnection, this, _1)); server_.setMessageCallback( std::bind(&amp;SudokuServer::onMessage, this, _1, _2, _3)); server_.setThreadNum(numThreads); &#125; void start() &#123; LOG_INFO &lt;&lt; \"starting \" &lt;&lt; numThreads_ &lt;&lt; \" threads.\"; server_.start(); &#125;private: void onConnection(const TcpConnectionPtr &amp;conn) &#123; ... &#125; void onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp) &#123; LOG_DEBUG &lt;&lt; conn-&gt;name(); size_t len = buf-&gt;readableBytes(); while (len &gt;= kCells + 2) &#123; const char *crlf = buf-&gt;findCRLF(); if (crlf) &#123; string request(buf-&gt;peek(), crlf); buf-&gt;retrieveUntil(crlf + 2); len = buf-&gt;readableBytes(); if (!processRequest(conn, request)) &#123; conn-&gt;send(\"Bad Request!\\r\\n\"); conn-&gt;shutdown(); break; &#125; &#125; else if (len &gt; 100) &#123; conn-&gt;send(\"Id too long!\\r\\n\"); conn-&gt;shutdown(); break; &#125; else &#123; break; &#125; &#125; &#125; bool processRequest(const TcpConnectionPtr &amp;conn, const string &amp;request) &#123; string id; string puzzle; bool goodRequest = true; string::const_iterator colon = find(request.begin(), request.end(), ':'); if (colon != request.end()) &#123; id.assign(request.begin(), colon); puzzle.assign(colon + 1, request.end()); &#125; else &#123; puzzle = request; &#125; if (puzzle.size() == implicit_cast&lt;size_t&gt;(kCells)) &#123; LOG_DEBUG &lt;&lt; conn-&gt;name(); string result = solveSudoku(puzzle); if (id.empty()) &#123; conn-&gt;send(result + \"\\r\\n\"); &#125; else &#123; conn-&gt;send(id + \":\" + result + \"\\r\\n\"); &#125; &#125; else &#123; goodRequest = false; &#125; return goodRequest; &#125; TcpServer server_; int numThreads_; Timestamp startTime_;&#125;;","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"Net","slug":"Net","permalink":"http://yoursite.com/tags/Net/"}]},{"title":"muduo 网络编程示例之二： Boost.Asio 的聊天服务器","slug":"muduo网络编程示例之二：Boost.Asio的聊天服务器","date":"2020-03-09T03:00:45.855Z","updated":"2020-03-09T03:01:30.000Z","comments":true,"path":"开源组件/muduo/muduo网络编程示例之二：Boost.Asio的聊天服务器/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/muduo%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%A4%BA%E4%BE%8B%E4%B9%8B%E4%BA%8C%EF%BC%9ABoost.Asio%E7%9A%84%E8%81%8A%E5%A4%A9%E6%9C%8D%E5%8A%A1%E5%99%A8/","excerpt":"","text":"原文链接 https://blog.csdn.net/Solstice/article/details/6172391 本文将介绍一个与 Boost.Asio 的示例代码中的聊天服务器功能类似的网络服务程序，包括客户端与服务端的 muduo 实现。这个例子的主要目的是介绍如何处理分包，并初步涉及 Muduo 的多线程功能。Muduo 的下载地址： http://muduo.googlecode.com/files/muduo-0.1.7-alpha.tar.gz ，SHA1 873567e43b3c2cae592101ea809b30ba730f2ee6，本文的完整代码可在线阅读http://code.google.com/p/muduo/source/browse/trunk/examples/asio/chat/ 。 一、TCP 分包 二、聊天服务 三、消息格式 四、打包的代码 五、分包的代码 六、编解码器 LengthHeaderCodec 七、服务端的实现 八、客户端的实现 九、简单测试 一、TCP 分包前面一篇《五个简单 TCP 协议》中处理的协议没有涉及分包，在 TCP 这种字节流协议上做应用层分包是网络编程的基本需求。分包指的是在发送一个消息(message)或一帧(frame)数据时，通过一定的处理，让接收方能从字节流中识别并截取（还原）出一个个消息。“粘包问题”是个伪问题。 对于短连接的 TCP 服务，分包不是一个问题，只要发送方主动关闭连接，就表示一条消息发送完毕，接收方 read() 返回 0，从而知道消息的结尾。例如前一篇文章里的 daytime 和 time 协议。 注：一方主动关闭 TCP 连接时，另一方 read() 返回 0 ，则代表对方已经关闭连接。 对于长连接的 TCP 服务，分包有四种方法： 消息长度固定，比如 muduo 的 roundtrip 示例就采用了固定的 16 字节消息； 使用特殊的字符或字符串作为消息的边界，例如 HTTP 协议的 headers 以 “/r/n” 为字段的分隔符； 在每条消息的头部加一个长度字段，这恐怕是最常见的做法，本文的聊天协议也采用这一办法； 利用消息本身的格式来分包，例如 XML 格式的消息中 ... 的配对，或者 JSON 格式中的 { ... } 的配对。解析这种消息格式通常会用到状态机。 在后文的代码讲解中还会仔细讨论用长度字段分包的常见陷阱。 二、聊天服务本文实现的聊天服务非常简单，由服务端程序和客户端程序组成，协议如下： 服务端程序中某个端口侦听 (listen) 新的连接； 客户端向服务端发起连接； 连接建立之后，客户端随时准备接收服务端的消息并在屏幕上显示出来； 客户端接受键盘输入，以回车为界，把消息发送给服务端； 服务端接收到消息之后，依次发送给每个连接到它的客户端；原来发送消息的客户端进程也会收到这条消息； 一个服务端进程可以同时服务多个客户端进程，当有消息到达服务端后，每个客户端进程都会收到同一条消息，服务端广播发送消息的顺序是任意的，不一定哪个客户端会先收到这条消息。 （可选）如果消息 A 先于消息 B 到达服务端，那么每个客户端都会先收到 A 再收到 B。 这实际上是一个简单的基于 TCP 的应用层广播协议，由服务端负责把消息发送给每个连接到它的客户端。参与“聊天”的既可以是人，也可以是程序。在以后的文章中，我将介绍一个稍微复杂的一点的例子 hub，它有“聊天室”的功能，客户端可以注册特定的 topic(s)，并往某个 topic 发送消息，这样代码更有意思。 三、消息格式本聊天服务的消息格式非常简单，“消息”本身是一个字符串，每条消息的有一个 4 字节的头部，以网络序存放字符串的长度。消息之间没有间隙，字符串也不一定以 ‘/0’ 结尾。比方说有两条消息 “hello” 和 “chenshuo”，那么打包后的字节流是： 10x00, 0x00, 0x00, 0x05, &#39;h&#39;, &#39;e&#39;, &#39;l&#39;, &#39;l&#39;, &#39;o&#39;, 0x00, 0x00, 0x00, 0x08, &#39;c&#39;, &#39;h&#39;, &#39;e&#39;, &#39;n&#39;, &#39;s&#39;, &#39;h&#39;, &#39;u&#39;, &#39;o&#39; 共 21 字节。 四、打包的代码这段代码把 const string&amp; message 打包为 muduo::net::Buffer，并通过 conn 发送。 muduo/examples/asio/chat/codec.h 12345678910void send(muduo::net::TcpConnection* conn, const string&amp; message) &#123; muduo::net::Buffer buf; buf.append(message.data(), message.size()); int32_t len = muduo::net::sockets::hostToNetwork32(static_cast(message.size())); buf.prepend(&amp;len, sizeof len); conn-&gt;send(&amp;buf);&#125; muduo::Buffer 有一个很好的功能，它在头部预留了 8 个字节的空间，这样第 6 行的 prepend() 操作就不需要移动已有的数据，效率较高。 五、分包的代码解析数据往往比生成数据复杂，分包打包也不例外。 muduo/examples/asio/chat/codec.h 12345678910111213141516171819void onMessage(const muduo::net::TcpConnectionPtr&amp; conn, muduo::net::Buffer* buf, muduo::Timestamp receiveTime) &#123; while (buf-&gt;readableBytes() &gt;= kHeaderLen) &#123; const void* data = buf-&gt;peek(); int32_t tmp = *static_cast&lt;const int32_t*&gt;(data); int32_t len = muduo::net::sockets::networkToHost32(tmp); if (len &gt; 65536 || len &lt; 0) &#123; LOG_ERROR &lt;&lt; \"Invalid length \" &lt;&lt; len; conn-&gt;shutdown(); &#125; else if (buf-&gt;readableBytes() &gt;= len + kHeaderLen) &#123; buf-&gt;retrieve(kHeaderLen); muduo::string message(buf-&gt;peek(), len); buf-&gt;retrieve(len); messageCallback_(conn, message, receiveTime); // 收到完整的消息，通知用户 &#125; else &#123; break; &#125; &#125;&#125; 上面这段代码第 7 行用了 while 循环来反复读取数据，直到 Buffer 中的数据不够一条完整的消息。请读者思考，如果换成 if (buf-&gt;readableBytes() &gt;= kHeaderLen) 会有什么后果。 以前面提到的两条消息的字节流为例： 0x00, 0x00, 0x00, 0x05, ‘h’, ‘e’, ‘l’, ‘l’, ‘o’, 0x00, 0x00, 0x00, 0x08, ‘c’, ‘h’, ‘e’, ‘n’, ‘s’, ‘h’, ‘u’, ‘o’ 假设数据最终都全部到达，onMessage() 至少要能正确处理以下各种数据到达的次序，每种情况下 messageCallback_ 都应该被调用两次： 每次收到一个字节的数据，onMessage() 被调用 21 次； 数据分两次到达，第一次收到 2 个字节，不足消息的长度字段； 数据分两次到达，第一次收到 4 个字节，刚好够长度字段，但是没有 body； 数据分两次到达，第一次收到 8 个字节，长度完整，但 body 不完整； 数据分两次到达，第一次收到 9 个字节，长度完整，body 也完整； 数据分两次到达，第一次收到 10 个字节，第一条消息的长度完整、body 也完整，第二条消息长度不完整； 请自行移动分割点，验证各种情况； 数据一次就全部到达，这时必须用 while 循环来读出两条消息，否则消息会堆积。 请读者验证 onMessage() 是否做到了以上几点。这个例子充分说明了 non-blocking read 必须和 input buffer 一起使用。这也解释了为什么该用 while 而不是 if 。 六、编解码器 LengthHeaderCodec有人评论 Muduo 的接收缓冲区不能设置回调函数的触发条件，确实如此。每当 socket 可读，Muduo 的 TcpConnection 会读取数据并存入 Input Buffer，然后回调用户的函数。不过，一个简单的间接层就能解决问题，让用户代码只关心“消息到达”而不是“数据到达”，如本例中的 LengthHeaderCodec 所展示的那一样。 12345678910111213141516171819202122#ifndef MUDUO_EXAMPLES_ASIO_CHAT_CODEC_H#define MUDUO_EXAMPLES_ASIO_CHAT_CODEC_H using muduo::Logger;class LengthHeaderCodec : boost::noncopyable &#123;public: typedef boost::function&lt;void (const muduo::net::TcpConnectionPtr&amp;, const muduo::string&amp; message, muduo::Timestamp)&gt; StringMessageCallback; explicit LengthHeaderCodec(const StringMessageCallback&amp; cb) : messageCallback_(cb) &#123; &#125; void onMessage(const muduo::net::TcpConnectionPtr&amp; conn, muduo::net::Buffer* buf, muduo::Timestamp receiveTime) &#123; 同上 &#125; void send(muduo::net::TcpConnection* conn, const muduo::string&amp; message) &#123; 同上 &#125;private: StringMessageCallback messageCallback_; const static size_t kHeaderLen = sizeof(int32_t);&#125;;#endif // MUDUO_EXAMPLES_ASIO_CHAT_CODEC_H 这段代码把以 Buffer* 为参数的 MessageCallback 转换成了以 const string&amp; 为参数的 StringMessageCallback，让用户代码不必关心分包操作。客户端和服务端都能从中受益。 七、服务端的实现聊天服务器的服务端代码小于 100 行，不到 asio 的一半。 请先阅读第 68 行起的数据成员的定义。除了经常见到的 EventLoop 和 TcpServer，ChatServer 还定义了 codec_ 和 std::set connections_ 作为成员，connections_ 是目前已建立的客户连接，在收到消息之后，服务器会遍历整个容器，把消息广播给其中每一个 TCP 连接。 首先，在构造函数里注册回调： 12345678910111213141516171819#include \"codec.h\"using namespace muduo;using namespace muduo::net;class ChatServer : boost::noncopyable &#123;public: ChatServer(EventLoop* loop, const InetAddress&amp; listenAddr): loop_(loop), server_(loop, listenAddr, \"ChatServer\"), codec_(boost::bind(&amp;ChatServer::onStringMessage, this, _1, _2, _3)) &#123; server_.setConnectionCallback(boost::bind(&amp;ChatServer::onConnection, this, _1)); server_.setMessageCallback(boost::bind(&amp;LengthHeaderCodec::onMessage, &amp;codec_, _1, _2, _3)); &#125; void start() &#123; server_.start(); &#125; 这里有几点值得注意，在以往的代码里是直接把本 class 的 onMessage() 注册给 server_；这里我们把 LengthHeaderCodec::onMessage() 注册给 server_，然后向 codec_ 注册了 ChatServer::onStringMessage()，等于说让 codec_ 负责解析消息，然后把完整的消息回调给 ChatServer。这正是我前面提到的“一个简单的间接层”，在不增加 Muduo 库的复杂度的前提下，提供了足够的灵活性让我们在用户代码里完成需要的工作。 另外，server_.start() 绝对不能在构造函数里调用，这么做将来会有线程安全的问题，见我在《当析构函数遇到多线程 ── C++ 中线程安全的对象回调》一文中的论述。 以下是处理连接的建立和断开的代码，注意它把新建的连接加入到 connections_ 容器中，把已断开的连接从容器中删除。这么做是为了避免内存和资源泄漏，TcpConnectionPtr 是 boost::shared_ptr，是 muduo 里唯一一个默认采用 shared_ptr 来管理生命期的对象。以后我们会谈到这么做的原因。 1234567891011private: void onConnection(const TcpConnectionPtr&amp; conn) &#123; LOG_INFO &lt;&lt; conn-&gt;localAddress().toHostPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;peerAddress().toHostPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); MutexLockGuard lock(mutex_); if (conn-&gt;connected()) &#123; connections_.insert(conn); &#125; else &#123; connections_.erase(conn); &#125; &#125; 以下是服务端处理消息的代码，它遍历整个 connections_ 容器，把消息打包发送给各个客户连接。 1234567void onStringMessage(const TcpConnectionPtr&amp;, const string&amp; message, Timestamp) &#123; MutexLockGuard lock(mutex_); for (ConnectionList::iterator it = connections_.begin(); it != connections_.end(); ++it) &#123; codec_.send(get_pointer(*it), message); &#125;&#125; 数据成员： 1234567typedef std::set ConnectionList;EventLoop* loop_;TcpServer server_;LengthHeaderCodec codec_;MutexLock mutex_;ConnectionList connections_;&#125;; main() 函数里边是例行公事的代码： 12345678910111213int main(int argc, char* argv[]) &#123; LOG_INFO &lt;&lt; \"pid = \" &lt;&lt; getpid(); if (argc &gt; 1) &#123; EventLoop loop; uint16_t port = static_cast(atoi(argv[1])); InetAddress serverAddr(port); ChatServer server(&amp;loop, serverAddr); server.start(); loop.loop(); &#125; else &#123; printf(\"Usage: %s port/n\", argv[0]); &#125;&#125; 如果你读过 asio 的对应代码，会不会觉得 Reactor 往往比 Proactor 容易使用？ 八、客户端的实现我有时觉得服务端的程序常常比客户端的更容易写，聊天服务器再次验证了我的看法。客户端的复杂性来自于它要读取键盘输入，而 EventLoop 是独占线程的，所以我用了两个线程，main() 函数所在的线程负责读键盘，另外用一个 EventLoopThread 来处理网络 IO。我暂时没有把标准输入输出融入 Reactor 的想法，因为服务器程序的 stdin 和 stdout 往往是重定向了的。 来看代码，首先，在构造函数里注册回调，并使用了跟前面一样的 LengthHeaderCodec 作为中间层，负责打包分包。 12345678910111213141516171819#include \"codec.h\"using namespace muduo;using namespace muduo::net;class ChatClient : boost::noncopyable &#123;public: ChatClient(EventLoop* loop, const InetAddress&amp; listenAddr) : loop_(loop), client_(loop, listenAddr, \"ChatClient\"), codec_(boost::bind(&amp;ChatClient::onStringMessage, this, _1, _2, _3)) &#123; client_.setConnectionCallback(boost::bind(&amp;ChatClient::onConnection, this, _1)); client_.setMessageCallback(boost::bind(&amp;LengthHeaderCodec::onMessage, &amp;codec_, _1, _2, _3)); client_.enableRetry(); &#125; void connect() &#123; client_.connect(); &#125; disconnect() 目前为空，客户端的连接由操作系统在进程终止时关闭。 123void disconnect() &#123; // client_.disconnect();&#125; write() 会由 main 线程调用，所以要加锁，这个锁不是为了保护 TcpConnection，而是保护 shared_ptr。 123456void write(const string&amp; message) &#123; MutexLockGuard lock(mutex_); if (connection_) &#123; codec_.send(get_pointer(connection_), message); &#125;&#125; onConnection() 会由 EventLoop 线程调用，所以要加锁以保护 shared_ptr。 123456789101112private: void onConnection(const TcpConnectionPtr&amp; conn) &#123; LOG_INFO &lt;&lt; conn-&gt;localAddress().toHostPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;peerAddress().toHostPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); MutexLockGuard lock(mutex_); if (conn-&gt;connected()) &#123; connection_ = conn; &#125; else &#123; connection_.reset(); &#125; &#125; 把收到的消息打印到屏幕，这个函数由 EventLoop 线程调用，但是不用加锁，因为 printf() 是线程安全的。注意这里不能用 cout，它不是线程安全的。 123void onStringMessage(const TcpConnectionPtr&amp;, const string&amp; message, Timestamp) &#123; printf(\"&lt;&lt;&lt; %s/n\", message.c_str());&#125; 数据成员： 123456EventLoop* loop_;TcpClient client_;LengthHeaderCodec codec_;MutexLock mutex_;TcpConnectionPtr connection_;&#125;; main() 函数里除了例行公事，还要启动 EventLoop 线程和读取键盘输入。 1234567891011121314151617181920int main(int argc, char* argv[]) &#123; LOG_INFO &lt;&lt; \"pid = \" &lt;&lt; getpid(); if (argc &gt; 2) &#123; EventLoopThread loopThread; uint16_t port = static_cast(atoi(argv[2])); InetAddress serverAddr(argv[1], port); ChatClient client(loopThread.startLoop(), serverAddr); // 注册到 EventLoopThread 的 EventLoop 上。 client.connect(); std::string line; while (std::getline(std::cin, line)) &#123; string message(line.c_str()); // 这里似乎多此一举，可直接发送 line。这里是 client.write(message); &#125; client.disconnect(); &#125; else &#123; printf(\"Usage: %s host_ip port/n\", argv[0]); &#125;&#125; 九、简单测试开三个命令行窗口，在第一个运行 $ ./asio_chat_server 3000 第二个运行 $ ./asio_chat_client 127.0.0.1 3000 第三个运行同样的命令 $ ./asio_chat_client 127.0.0.1 3000 这样就有两个客户端进程参与聊天。在第二个窗口里输入一些字符并回车，字符会出现在本窗口和第三个窗口中。","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"Net","slug":"Net","permalink":"http://yoursite.com/tags/Net/"},{"name":"Boost","slug":"Boost","permalink":"http://yoursite.com/tags/Boost/"}]},{"title":"为什么 muduo 的 shutdown() 没有直接关闭 TCP 连接？","slug":"为什么muduo的shutdown()没有直接关闭TCP连接？","date":"2020-03-09T02:59:18.134Z","updated":"2020-03-09T03:00:08.000Z","comments":true,"path":"开源组件/muduo/为什么muduo的shutdown()没有直接关闭TCP连接？/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/%E4%B8%BA%E4%BB%80%E4%B9%88muduo%E7%9A%84shutdown()%E6%B2%A1%E6%9C%89%E7%9B%B4%E6%8E%A5%E5%85%B3%E9%97%ADTCP%E8%BF%9E%E6%8E%A5%EF%BC%9F/","excerpt":"","text":"原文链接 https://blog.csdn.net/Solstice/article/details/6208634 问题： 相关代码 回答 问题：1在 simple 中的 daytime 示例中，服务端主动关闭时调用的是如下函数序列，这不是只是关闭了连接上的写操作吗，怎么是关闭了整个连接？ 相关代码daytime.cc 123456789void DaytimeServer::onConnection(const TcpConnectionPtr &amp;conn) &#123; LOG_INFO &lt;&lt; \"DaytimeServer - \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); if (conn-&gt;connected()) &#123; conn-&gt;send(Timestamp::now().toFormattedString() + \"\\n\"); conn-&gt;shutdown(); &#125;&#125; TcpConnection.cc 12345678910111213141516void TcpConnection::shutdown() &#123; // FIXME: use compare and swap if (state_ == kConnected) &#123; setState(kDisconnecting); // FIXME: shared_from_this()? loop_-&gt;runInLoop(std::bind(&amp;TcpConnection::shutdownInLoop, this)); &#125;&#125;void TcpConnection::shutdownInLoop() &#123; loop_-&gt;assertInLoopThread(); if (!channel_-&gt;isWriting()) &#123; // we are not writing socket_-&gt;shutdownWrite(); &#125;&#125; Socket.cc 123456789void Socket::shutdownWrite() &#123; sockets::shutdownWrite(sockfd_);&#125;void sockets::shutdownWrite(int sockfd) &#123; if (::shutdown(sockfd, SHUT_WR) &lt; 0) &#123; LOG_SYSERR &lt;&lt; \"sockets::shutdownWrite\"; &#125;&#125; 回答Muduo TcpConnection 没有提供 close，而只提供 shutdown ，这么做是为了收发数据的完整性。 TCP 是一个全双工协议，同一个文件描述符既可读又可写， shutdownWrite() 关闭了“写”方向的连接，保留了“读”方向，这称为 TCP half-close。如果直接 close(socket_fd)，那么 socket_fd 就不能读或写了。 用 shutdown 而不用 close 的效果是，如果对方已经发送了数据，这些数据还“在路上”，那么 muduo 不会漏收这些数据。换句话说，muduo 在 TCP 这一层面解决了“当你打算关闭网络连接的时候，如何得知对方有没有发了一些数据而你还没有收到？”这一问题。当然，这个问题也可以在上面的协议层解决，双方商量好不再互发数据，就可以直接断开连接。 等于说 muduo 把“主动关闭连接”这件事情分成两步来做，如果要主动关闭连接，它会先关本地“写”端，等对方关闭之后，再关本地“读”端。 1练习：阅读代码，回答“如果被动关闭连接，muduo 的行为如何？ 提示：muduo 在 read() 返回 0 的时候会回调 connection callback，这样客户代码就知道对方断开连接了。 Muduo 这种关闭连接的方式对对方也有要求，那就是对方 read() 到 0 字节之后会主动关闭连接（无论 shutdownWrite() 还是 close()），一般的网络程序都会这样，不是什么问题。当然，这么做有一个潜在的安全漏洞，万一对方故意不不关，那么 muduo 的连接就一直半开着，消耗系统资源。 完整的流程是：我们发完了数据，于是 shutdownWrite，发送 TCP FIN 分节，对方会读到 0 字节，然后对方通常会关闭连接，这样 muduo 会读到 0 字节，然后 muduo 关闭连接。（思考题，在 shutdown() 之后，muduo 回调 connection callback 的时间间隔大约是一个 round-trip time，为什么？） 另外，如果有必要，对方可以在 read() 返回 0 之后继续发送数据，这是直接利用了 half-close TCP 连接。muduo 会收到这些数据，通过 message callback 通知客户代码。 那么 muduo 什么时候真正 close socket 呢？在 TcpConnection 对象析构的时候。TcpConnection 持有一个 Socket 对象，Socket 是一个 RAII handler，它的析构函数会 close(sockfd_)。这样，如果发生 TcpConnection 对象泄漏，那么我们从 /proc/pid/fd/ 就能找到没有关闭的文件描述符，便于查错。 muduo 在 read() 返回 0 的时候会回调 connection callback，然后把 TcpConnection 的引用计数减一，如果 TcpConnection 的引用计数降到零，它就会析构了。 参考： 《TCP/IP 详解》第一卷第 18.5 节，TCP Half-Close。 《UNIX 网络编程》第一卷第三版第 6.6 节， shutdown() 函数。","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"TCP","slug":"TCP","permalink":"http://yoursite.com/tags/TCP/"}]},{"title":"muduo 网络编程示例之一：五个简单 TCP 协议","slug":"muduo网络编程示例之一：五个简单TCP协议","date":"2020-03-09T02:58:09.704Z","updated":"2020-03-09T02:58:54.000Z","comments":true,"path":"开源组件/muduo/muduo网络编程示例之一：五个简单TCP协议/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/muduo%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%A4%BA%E4%BE%8B%E4%B9%8B%E4%B8%80%EF%BC%9A%E4%BA%94%E4%B8%AA%E7%AE%80%E5%8D%95TCP%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"原文链接 https://blog.csdn.net/Solstice/article/details/6171905 本文将介绍第一个示例：五个简单 TCP 网络服务协议，包括 echo (RFC 862)、discard (RFC 863)、chargen (RFC 864)、daytime (RFC 867)、time (RFC 868)，以及 time 协议的客户端。各协议的功能简介如下： discard - 丢弃所有收到的数据； daytime - 服务端 accept 连接之后，以字符串形式发送当前时间，然后主动断开连接； time - 服务端 accept 连接之后，以二进制形式发送当前时间（从 Epoch 到现在的秒数），然后主动断开连接；我们需要一个客户程序来把收到的时间转换为字符串。 echo - 回显服务，把收到的数据发回客户端； chargen - 服务端 accept 连接之后，不停地发送测试数据。 以上五个协议使用不同的端口，可以放到同一个进程中实现，且不必使用多线程。完整的代码见 muduo/examples/simple，下载地址 http://muduo.googlecode.com/files/muduo-0.1.6-alpha.tar.gz 。 一、discard 二、daytime 三、time server client 四、echo 五、chargen server client 六、Five in one 一、discardDiscard 恐怕算是最简单的长连接 TCP 应用层协议，它只需要关注“三个半事件”中的“消息/数据到达”事件 main.cpp 12345678int main() &#123; LOG_INFO &lt;&lt; \"pid = \" &lt;&lt; getpid(); EventLoop loop; InetAddress listenAddr(2009); DiscardServer server(&amp;loop, listenAddr); server.start(); loop.loop();&#125; discard.h 12345678910111213141516class DiscardServer &#123;public: DiscardServer(muduo::net::EventLoop *loop, const muduo::net::InetAddress &amp;listenAddr); void start();private: void onConnection(const muduo::net::TcpConnectionPtr &amp;conn); void onMessage(const muduo::net::TcpConnectionPtr &amp;conn, muduo::net::Buffer *buf, muduo::Timestamp time); muduo::net::TcpServer server_;&#125;; discard.cc 1234567891011121314151617181920212223242526DiscardServer::DiscardServer(EventLoop *loop, const InetAddress &amp;listenAddr) : server_(loop, listenAddr, \"DiscardServer\") &#123; server_.setConnectionCallback( std::bind(&amp;DiscardServer::onConnection, this, _1)); server_.setMessageCallback( std::bind(&amp;DiscardServer::onMessage, this, _1, _2, _3));&#125;void DiscardServer::start() &#123; server_.start();&#125;void DiscardServer::onConnection(const TcpConnectionPtr &amp;conn) &#123; LOG_INFO &lt;&lt; \"DiscardServer - \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\");&#125;void DiscardServer::onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp time) &#123; string msg(buf-&gt;retrieveAllAsString()); LOG_INFO &lt;&lt; conn-&gt;name() &lt;&lt; \" discards \" &lt;&lt; msg.size() &lt;&lt; \" bytes received at \" &lt;&lt; time.toString();&#125; 二、daytimeDaytime 是短连接协议，在发送完当前时间后，由服务端主动断开连接。它只需要关注“三个半事件”中的“连接已建立”事件 仅关注两个回调函数，其他部分和 discard 大同小异 daytime.cc 1234567891011121314151617void DaytimeServer::onConnection(const TcpConnectionPtr &amp;conn) &#123; LOG_INFO &lt;&lt; \"DaytimeServer - \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); if (conn-&gt;connected()) &#123; conn-&gt;send(Timestamp::now().toFormattedString() + \"\\n\"); conn-&gt;shutdown(); &#125;&#125;void DaytimeServer::onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp time) &#123; string msg(buf-&gt;retrieveAllAsString()); LOG_INFO &lt;&lt; conn-&gt;name() &lt;&lt; \" discards \" &lt;&lt; msg.size() &lt;&lt; \" bytes received at \" &lt;&lt; time.toString();&#125; 比较值得注意的是 onConnection 方法，发送完数据以后，由 server 端 shutdown 这个 socket 三、timeTime 协议与 daytime 极为类似，只不过它返回的不是日期时间字符串，而是一个 32-bit 整数，表示从 1970-01-01 00:00:00Z 到现在的秒数。当然，这个协议有“2038 年问题”。服务端只需要关注“三个半事件”中的“连接已建立”事件。 serverserver 端和 daytime 差不多，也是 send 以后主动 shutdown time.cc 1234567891011121314151617181920void TimeServer::onConnection(const muduo::net::TcpConnectionPtr &amp;conn) &#123; LOG_INFO &lt;&lt; \"TimeServer - \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); if (conn-&gt;connected()) &#123; time_t now = ::time(NULL); int32_t be32 = sockets::hostToNetwork32(static_cast&lt;int32_t&gt;(now)); conn-&gt;send(&amp;be32, sizeof be32); conn-&gt;shutdown(); &#125;&#125;void TimeServer::onMessage(const muduo::net::TcpConnectionPtr &amp;conn, muduo::net::Buffer *buf, muduo::Timestamp time) &#123; string msg(buf-&gt;retrieveAllAsString()); LOG_INFO &lt;&lt; conn-&gt;name() &lt;&lt; \" discards \" &lt;&lt; msg.size() &lt;&lt; \" bytes received at \" &lt;&lt; time.toString();&#125; client因为 time 服务端发送的是二进制数据，不便直接阅读，我们编写一个客户端来解析并打印收到的 4 个字节数据。这个程序只需要关注“三个半事件”中的“消息/数据到达”事件。 main.cpp 12345678910111213int main(int argc, char *argv[]) &#123; LOG_INFO &lt;&lt; \"pid = \" &lt;&lt; getpid(); if (argc &gt; 1) &#123; EventLoop loop; InetAddress serverAddr(argv[1], 2037); TimeClient timeClient(&amp;loop, serverAddr); timeClient.connect(); loop.loop(); &#125; else &#123; printf(\"Usage: %s host_ip\\n\", argv[0]); &#125;&#125; timeclient.h 123456789101112131415161718192021222324252627282930313233343536373839404142434445class TimeClient : noncopyable &#123;public: TimeClient(EventLoop *loop, const InetAddress &amp;serverAddr) : loop_(loop), client_(loop, serverAddr, \"TimeClient\") &#123; client_.setConnectionCallback( std::bind(&amp;TimeClient::onConnection, this, _1)); client_.setMessageCallback( std::bind(&amp;TimeClient::onMessage, this, _1, _2, _3)); // client_.enableRetry(); &#125; void connect() &#123; client_.connect(); &#125;private: EventLoop *loop_; TcpClient client_; void onConnection(const TcpConnectionPtr &amp;conn) &#123; LOG_INFO &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); if (!conn-&gt;connected()) &#123; loop_-&gt;quit(); // // 如果连接断开，则终止主循环，退出程序 &#125; &#125; void onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp receiveTime) &#123; if (buf-&gt;readableBytes() &gt;= sizeof(int32_t)) &#123; const void *data = buf-&gt;peek(); int32_t be32 = *static_cast&lt;const int32_t *&gt;(data); buf-&gt;retrieve(sizeof(int32_t)); time_t time = sockets::networkToHost32(be32); Timestamp ts(implicit_cast&lt;uint64_t&gt;(time) * Timestamp::kMicroSecondsPerSecond); LOG_INFO &lt;&lt; \"Server time = \" &lt;&lt; time &lt;&lt; \", \" &lt;&lt; ts.toFormattedString(); &#125; else &#123; LOG_INFO &lt;&lt; conn-&gt;name() &lt;&lt; \" no enough data \" &lt;&lt; buf-&gt;readableBytes() &lt;&lt; \" at \" &lt;&lt; receiveTime.toFormattedString(); &#125; &#125;&#125;; 注意其中考虑到了如果数据没有一次性收全，已经收到的数据会暂存在 Buffer 里，以等待下一次机会，程序也不会阻塞。这样即便服务器一个字节一个字节地发送数据，代码还是能正常工作，这也是非阻塞网络编程必须在用户态使用接受缓冲的主要原因。 四、echoEcho 是我们遇到的第一个带交互的协议：服务端把客户端发过来的数据原封不动地传回去。它只需要关注“三个半事件”中的“消息/数据到达”事件。 echo.cc 12345678910111213141516void EchoServer::onConnection(const muduo::net::TcpConnectionPtr &amp;conn) &#123; LOG_INFO &lt;&lt; \"EchoServer - \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\");&#125;void EchoServer::onMessage(const muduo::net::TcpConnectionPtr &amp;conn, muduo::net::Buffer *buf, muduo::Timestamp time) &#123; muduo::string msg(buf-&gt;retrieveAllAsString()); LOG_INFO &lt;&lt; conn-&gt;name() &lt;&lt; \" echo \" &lt;&lt; msg.size() &lt;&lt; \" bytes, \" &lt;&lt; \"data received at \" &lt;&lt; time.toString(); conn-&gt;send(msg);&#125; 这段代码实现的不是行回显(line echo)服务，而是有一点数据就发送一点数据。这样可以避免客户端恶意地不发送换行字符，而服务端又必须缓存已经收到的数据，导致服务器内存暴涨。但这个程序还是有一个安全漏洞，即如果客户端故意不断发生数据，但从不接收，那么服务端的发送缓冲区会一直堆积，导致内存暴涨。解决办法可以参考下面的 chargen 协议。 五、chargenChargen 协议很特殊，它只发送数据，不接收数据。而且，它发送数据的速度不能快过客户端接收的速度，因此需要关注“三个半事件”中的半个“消息/数据发送完毕”事件(onWriteComplete)。 serverchargen.h 12345678910111213141516171819202122232425class ChargenServer &#123;public: ChargenServer(muduo::net::EventLoop *loop, const muduo::net::InetAddress &amp;listenAddr, bool print = false); void start();private: void onConnection(const muduo::net::TcpConnectionPtr &amp;conn); void onMessage(const muduo::net::TcpConnectionPtr &amp;conn, muduo::net::Buffer *buf, muduo::Timestamp time); void onWriteComplete(const muduo::net::TcpConnectionPtr &amp;conn); void printThroughput(); muduo::net::TcpServer server_; muduo::string message_; int64_t transferred_; muduo::Timestamp startTime_;&#125;; chargen.cc 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162ChargenServer::ChargenServer(EventLoop *loop, const InetAddress &amp;listenAddr, bool print) : server_(loop, listenAddr, \"ChargenServer\"), transferred_(0), startTime_(Timestamp::now()) &#123; server_.setConnectionCallback( std::bind(&amp;ChargenServer::onConnection, this, _1)); server_.setMessageCallback( std::bind(&amp;ChargenServer::onMessage, this, _1, _2, _3)); server_.setWriteCompleteCallback( std::bind(&amp;ChargenServer::onWriteComplete, this, _1)); if (print) &#123; loop-&gt;runEvery(3.0, std::bind(&amp;ChargenServer::printThroughput, this)); &#125; string line; for (int i = 33; i &lt; 127; ++i) &#123; line.push_back(char(i)); &#125; line += line; for (size_t i = 0; i &lt; 127 - 33; ++i) &#123; message_ += line.substr(i, 72) + '\\n'; &#125;&#125;void ChargenServer::start() &#123; server_.start();&#125;void ChargenServer::onConnection(const TcpConnectionPtr &amp;conn) &#123; LOG_INFO &lt;&lt; \"ChargenServer - \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); if (conn-&gt;connected()) &#123; // 在连接建立时发生第一次数据 conn-&gt;setTcpNoDelay(true); conn-&gt;send(message_); &#125;&#125;void ChargenServer::onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp time) &#123; string msg(buf-&gt;retrieveAllAsString()); LOG_INFO &lt;&lt; conn-&gt;name() &lt;&lt; \" discards \" &lt;&lt; msg.size() &lt;&lt; \" bytes received at \" &lt;&lt; time.toString();&#125;void ChargenServer::onWriteComplete(const TcpConnectionPtr &amp;conn) &#123; // 继续发送数据 transferred_ += message_.size(); conn-&gt;send(message_);&#125;void ChargenServer::printThroughput() &#123; Timestamp endTime = Timestamp::now(); double time = timeDifference(endTime, startTime_); printf(\"%4.3f MiB/s\\n\", static_cast&lt;double&gt;(transferred_) / time / 1024 / 1024); transferred_ = 0; startTime_ = endTime;&#125; clientchargenclient.cc 123456789101112131415161718192021222324252627282930313233class ChargenClient : noncopyable &#123;public: ChargenClient(EventLoop *loop, const InetAddress &amp;listenAddr) : loop_(loop), client_(loop, listenAddr, \"ChargenClient\") &#123; client_.setConnectionCallback( std::bind(&amp;ChargenClient::onConnection, this, _1)); client_.setMessageCallback( std::bind(&amp;ChargenClient::onMessage, this, _1, _2, _3)); // client_.enableRetry(); &#125; void connect() &#123; client_.connect(); &#125;private: void onConnection(const TcpConnectionPtr &amp;conn) &#123; LOG_INFO &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); if (!conn-&gt;connected()) loop_-&gt;quit(); &#125; void onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp receiveTime) &#123; buf-&gt;retrieveAll(); &#125; EventLoop *loop_; TcpClient client_;&#125;; 六、Five in one前面五个程序都用到了 EventLoop，这其实是个 Reactor，用于注册和分发 IO 事件。Muduo 遵循 one loop per thread 模型，多个服务端(TcpServer)和客户端(TcpClient)可以共享同一个 EventLoop，也可以分配到多个 EventLoop 上以发挥多核多线程的好处。这里我们把五个服务端用同一个 EventLoop 跑起来，程序还是单线程的，功能却强大了很多： 123456789101112131415161718192021int main() &#123; LOG_INFO &lt;&lt; \"pid = \" &lt;&lt; getpid(); EventLoop loop; // one loop shared by multiple servers ChargenServer chargenServer(&amp;loop, InetAddress(2019)); chargenServer.start(); DaytimeServer daytimeServer(&amp;loop, InetAddress(2013)); daytimeServer.start(); DiscardServer discardServer(&amp;loop, InetAddress(2009)); discardServer.start(); EchoServer echoServer(&amp;loop, InetAddress(2007)); echoServer.start(); TimeServer timeServer(&amp;loop, InetAddress(2037)); timeServer.start(); loop.loop();&#125; 以上几个协议的消息格式都非常简单，没有涉及 TCP 网络编程中常见的分包处理，在下一篇文章讲 Boost.Asio 的聊天服务器时我们再来讨论这个问题。","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"TCP","slug":"TCP","permalink":"http://yoursite.com/tags/TCP/"}]},{"title":"muduo 中的 net 组件","slug":"muduo中的net组件","date":"2020-03-09T02:57:01.062Z","updated":"2020-03-09T02:57:36.000Z","comments":true,"path":"开源组件/muduo/muduo中的net组件/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/muduo%E4%B8%AD%E7%9A%84net%E7%BB%84%E4%BB%B6/","excerpt":"","text":"类之间的耦合关系 最底层 Buffer：√ Socket：√ Timer：√ Channel ：和 EventLoop 相互耦合 √ 第二层 TimerId ：依赖 Timer √ Poller ：依赖 Channel，和 EventLoop 相互耦合 √ Connector ：依赖 Channel，和 EventLoop 相互耦合 √ Acceptor ：依赖 Channel 和 Socket，和 EventLoop 相互耦合 √ TimerQueue ：依赖 Timer 和 Channel，和 EventLoop 相互耦合 √ 第三层 EventLoop ：和 Channel、Poller 和 TimerQueue 相互耦合 √ 第四层 EventLoopThread ：依赖 EventLoop √ TcpConnection ：依赖 EventLoop、Socket、Channel 和 Buffer √ 第五层 EventLoopThreadPool ：依赖 EventLoopThread √ TcpServer ：依赖 EventLoop、Acceptor、EventLoopThread 和 TcpConnection √ TcpClient ：依赖 EventLoop 、 Connector 和 TcpConnection √ 最底层 1. Buffer 2. Socket 3. Timer 4. Channel 第二层 1. TimerId 2. Poller 3. Acceptor 4. TimerQueue 5. Connector 第三层 1. EventLoop 第四层 1. EventLoopThread 2. TcpConnection 第五层 1. EventLoopThreadPool 2. TcpServer 3. TcpClient 最底层1. Buffer见 “./5_Buffer类的设计.md” 2. Socketsocket_fd 的 wrapper，目标是提供了几种 Socket 的封装，例如 listen、bind 等。 12345678910111213141516171819202122namespace muduo &#123; namespace net &#123; class InetAddress; class Socket : noncopyable &#123; public: explicit Socket(int sockfd) : sockfd_(sockfd) &#123;&#125; void bindAddress(const InetAddress &amp;localaddr); void listen(); int accept(InetAddress *peeraddr); ... private: const int sockfd_; &#125;; &#125; // namespace net&#125; // namespace muduo 3. Timer用于时间事件的内部类 1234567891011121314151617181920212223242526272829303132333435363738394041namespace muduo &#123; namespace net &#123; class Timer : noncopyable &#123; public: Timer(TimerCallback cb, Timestamp when, double interval) : callback_(std::move(cb)), expiration_(when), interval_(interval), repeat_(interval &gt; 0.0), sequence_(s_numCreated_.incrementAndGet()) &#123;&#125; void run() const &#123; callback_(); &#125; void restart(Timestamp now) &#123; if (repeat_) &#123; expiration_ = addTime(now, interval_); &#125; else &#123; expiration_ = Timestamp::invalid(); &#125; &#125; Timestamp expiration() const; bool repeat() const; int64_t sequence() const; static int64_t numCreated(); private: const TimerCallback callback_; Timestamp expiration_; const double interval_; const bool repeat_; const int64_t sequence_; static AtomicInt64 s_numCreated_; &#125;; &#125; // namespace net&#125; // namespace muduo 4. Channel用于注册与响应 IO 事件 构造函数中和一个 EventLoop 绑定、和一个文件描述符 fd 绑定 123456789101112namespace muduo &#123; namespace net &#123; class EventLoop; class Channel : noncopyable &#123; public: Channel(EventLoop *loop, int fd); ... &#125;; &#125; // namespace net&#125; // namespace muduo IO 事件类型分为两种：ReadEventCallback 和 EventCallback；有四种回调函数类型：readCallback_、writeCallback_、closeCallback_ 和 errorCallback_，可以通过 setXXX 函数来注册（注意 std::move() 将左值转化为了右值引用以减少拷贝次数） 12345678910111213141516171819202122232425262728namespace muduo &#123; namespace net &#123; class Channel : noncopyable &#123; public: typedef std::function&lt;void()&gt; EventCallback; typedef std::function&lt;void(Timestamp)&gt; ReadEventCallback; void setReadCallback(ReadEventCallback cb) &#123; readCallback_ = std::move(cb); &#125; void setWriteCallback(EventCallback cb) &#123; writeCallback_ = std::move(cb); &#125; void setCloseCallback(EventCallback cb) &#123; closeCallback_ = std::move(cb); &#125; void setErrorCallback(EventCallback cb) &#123; errorCallback_ = std::move(cb); &#125; ... private: ReadEventCallback readCallback_; EventCallback writeCallback_; EventCallback closeCallback_; EventCallback errorCallback_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 设置事件读写能力，通过对 events_ 变量设置掩码完成，并且通知绑定的 EventLoop update 1234567891011121314151617181920212223242526272829303132333435363738394041424344namespace muduo &#123; namespace net &#123; class Channel : noncopyable &#123; public: void enableReading() &#123; events_ |= kReadEvent; update(); &#125; void disableReading() &#123; events_ &amp;= ~kReadEvent; update(); &#125; void enableWriting() &#123; events_ |= kWriteEvent; update(); &#125; void disableWriting() &#123; events_ &amp;= ~kWriteEvent; update(); &#125; void disableAll() &#123; events_ = kNoneEvent; update(); &#125; ... private: static const int kNoneEvent = 0; static const int kReadEvent = POLLIN | POLLPRI; static const int kWriteEvent = POLLOUT; int events_; update() &#123; addedToLoop_ = true; loop_-&gt;updateChannel(this); &#125; ... &#125;; &#125; // namespace net&#125; // namespace muduo 处理事件 revents_ 变量由 Poller 设置，表征事件的类型；处理事件时根据事件类型，调用注册好的几种回调函数 123456789101112131415161718192021222324252627282930313233void Channel::handleEventWithGuard(Timestamp receiveTime) &#123; eventHandling_ = true; LOG_TRACE &lt;&lt; reventsToString(); if ((revents_ &amp; POLLHUP) &amp;&amp; !(revents_ &amp; POLLIN)) &#123; if (logHup_) &#123; LOG_WARN &lt;&lt; \"fd = \" &lt;&lt; fd_ &lt;&lt; \" Channel::handle_event() POLLHUP\"; &#125; if (closeCallback_) &#123; closeCallback_(); &#125; &#125; if (revents_ &amp; POLLNVAL) &#123; LOG_WARN &lt;&lt; \"fd = \" &lt;&lt; fd_ &lt;&lt; \" Channel::handle_event() POLLNVAL\"; &#125; if (revents_ &amp; (POLLERR | POLLNVAL)) &#123; if (errorCallback_) &#123; errorCallback_(); &#125; &#125; if (revents_ &amp; (POLLIN | POLLPRI | POLLRDHUP)) &#123; if (readCallback_) &#123; readCallback_(receiveTime); &#125; &#125; if (revents_ &amp; POLLOUT) &#123; if (writeCallback_) &#123; writeCallback_(); &#125; &#125; eventHandling_ = false;&#125; 第二层1. TimerId用于取消 Timer 123456789101112131415161718192021222324namespace muduo &#123; namespace net &#123; class Timer; class TimerId : public muduo::copyable &#123; public: TimerId() : timer_(NULL), sequence_(0) &#123;&#125; TimerId(Timer *timer, int64_t seq) : timer_(timer), sequence_(seq) &#123; &#125; // default copy-ctor, dtor and assignment are okay friend class TimerQueue; private: Timer *timer_; int64_t sequence_; &#125;; &#125; // namespace net&#125; // namespace muduo 2. Poller IO 多路复用的虚基类，和一个 EventLoop 对象绑定，poll 方法必须在 loop 线程中调用。 一个 Poller 对象中可以包含多个 Channel，代表多个 IO 事件 有两个实现类：PollPoller 底层使用 poll；EPollPoller 底层使用 epoll 123456789101112131415161718192021222324252627282930313233343536373839404142namespace muduo &#123; namespace net &#123; class Channel; class Poller : noncopyable &#123; public: typedef std::vector&lt;Channel *&gt; ChannelList; Poller(EventLoop *loop); virtual ~Poller(); /// Polls the I/O events. /// Must be called in the loop thread. virtual Timestamp poll(int timeoutMs, ChannelList *activeChannels) = 0; /// Changes the interested I/O events. /// Must be called in the loop thread. virtual void updateChannel(Channel *channel) = 0; /// Remove the channel, when it destructs. /// Must be called in the loop thread. virtual void removeChannel(Channel *channel) = 0; virtual bool hasChannel(Channel *channel) const; static Poller *newDefaultPoller(EventLoop *loop); void assertInLoopThread() const &#123; ownerLoop_-&gt;assertInLoopThread(); &#125; protected: typedef std::map&lt;int, Channel *&gt; ChannelMap; ChannelMap channels_; private: EventLoop *ownerLoop_; &#125;; &#125; // namespace net&#125; // namespace muduo 3. Acceptor用于接受 TCP 连接 构造函数 12345678910111213141516171819202122232425262728293031namespace muduo &#123; namespace net &#123; class Acceptor : noncopyable &#123; public: Acceptor(EventLoop *loop, const InetAddress &amp;listenAddr, bool reuseport) : loop_(loop), acceptSocket_(sockets::createNonblockingOrDie(listenAddr.family())), acceptChannel_(loop, acceptSocket_.fd()), listenning_(false), idleFd_(::open(\"/dev/null\", O_RDONLY | O_CLOEXEC)) &#123; assert(idleFd_ &gt;= 0); acceptSocket_.setReuseAddr(true); acceptSocket_.setReusePort(reuseport); acceptSocket_.bindAddress(listenAddr); acceptChannel_.setReadCallback( std::bind(&amp;Acceptor::handleRead, this)); &#125; private: EventLoop *loop_; Socket acceptSocket_; Channel acceptChannel_; NewConnectionCallback newConnectionCallback_; bool listenning_; int idleFd_; &#125;; &#125; // namespace net&#125; // namespace muduo 每个 Acceptor 和一个 EventLoop 绑定，并且传入一个 InetAddress 对象用于构造 Socket 监听的端口。 在构造函数中，acceptSocket_ 对象初始化好，acceptChannel_ 也初始化好，并且注册 Read 事件的 CallBack 为 Acceptor::handleRead。 Acceptor::handleRead 的代码是： 123456789101112131415161718192021222324252627void Acceptor::handleRead() &#123; loop_-&gt;assertInLoopThread(); InetAddress peerAddr; //FIXME loop until no more int connfd = acceptSocket_.accept(&amp;peerAddr); if (connfd &gt;= 0) &#123; // string hostport = peerAddr.toIpPort(); // LOG_TRACE &lt;&lt; \"Accepts of \" &lt;&lt; hostport; if (newConnectionCallback_) &#123; newConnectionCallback_(connfd, peerAddr); &#125; else &#123; sockets::close(connfd); &#125; &#125; else &#123; LOG_SYSERR &lt;&lt; \"in Acceptor::handleRead\"; // Read the section named \"The special problem of // accept()ing when you can't\" in libev's doc. // By Marc Lehmann, author of libev. if (errno == EMFILE) &#123; ::close(idleFd_); idleFd_ = ::accept(acceptSocket_.fd(), NULL, NULL); ::close(idleFd_); idleFd_ = ::open(\"/dev/null\", O_RDONLY | O_CLOEXEC); &#125; &#125;&#125; 也就是说，一旦 acceptChannel_ 产生了 read 事件，它就会调用 Acceptor 注册好的 newConnectionCallback_ 回调函数。 listen 让 acceptSocket_ 开始 listen 某个端口（这个不是阻塞的），然后打开 acceptChannel_ 的读事件能力。 123456void Acceptor::listen() &#123; loop_-&gt;assertInLoopThread(); listenning_ = true; acceptSocket_.listen(); acceptChannel_.enableReading();&#125; 4. TimerQueue一个 Timer 队列，用于管理定时事件 构造函数 1234567891011121314151617181920212223242526272829303132333435namespace muduo &#123; namespace net &#123; class TimerQueue : noncopyable &#123; public: explicit TimerQueue(EventLoop *loop): loop_(loop), timerfd_(createTimerfd()), timerfdChannel_(loop, timerfd_), timers_(), callingExpiredTimers_(false) &#123; timerfdChannel_.setReadCallback(std::bind(&amp;TimerQueue::handleRead, this)); timerfdChannel_.enableReading(); &#125; ... private: typedef std::pair&lt;Timestamp, Timer *&gt; Entry; typedef std::set&lt;Entry&gt; TimerList; EventLoop *loop_; const int timerfd_; Channel timerfdChannel_; // Timer list sorted by expiration TimerList timers_; bool callingExpiredTimers_; /* atomic */ ... &#125;; &#125; // namespace net&#125; // namespace muduo 在构造函数中，首先将一个 TimerQueue 和一个 EventLoop 绑定，然后创建一个 timer 的 fd，然后用这个 fd 构造一个 timerfdChannel_。这个 timerfdChannel_ 会设置一个 Read 事件的 CallBack 回调函数，看一下这个回调函数 1234567891011121314151617void TimerQueue::handleRead() &#123; loop_-&gt;assertInLoopThread(); Timestamp now(Timestamp::now()); readTimerfd(timerfd_, now); std::vector&lt;Entry&gt; expired = getExpired(now); callingExpiredTimers_ = true; cancelingTimers_.clear(); // safe to callback outside critical section for (const Entry &amp;it : expired) &#123; it.second-&gt;run(); &#125; callingExpiredTimers_ = false; reset(expired, now);&#125; 这个回调函数的大致原理是：如果 timerfdChannel_ 变得可读，说明有时间事件到达了；此时找一下过期的 timer 们，执行一下 timer 的 callback 函数 addTimer 添加一个 Timer。 12345678910111213141516namespace muduo &#123; namespace net &#123; class TimerQueue : noncopyable &#123; public: TimerId addTimer(TimerCallback cb, Timestamp when, double interval) &#123; Timer *timer = new Timer(std::move(cb), when, interval); loop_-&gt;runInLoop(std::bind(&amp;TimerQueue::addTimerInLoop, this, timer)); return TimerId(timer, timer-&gt;sequence()); &#125; ... &#125;; 具体方式是先 new 一个 Timer 对象，然后向绑定的 EventLoop 对象添加。 1234567void EventLoop::runInLoop(Functor cb) &#123; if (isInLoopThread()) &#123; cb(); &#125; else &#123; queueInLoop(std::move(cb)); &#125;&#125; EventLoop 要么直接执行，要么稍后执行（对于添加 Timer 这个行为，很可能发生在其他线程），但是无论如何最终都会执行 addTimerInLoop 方法： 12345678void TimerQueue::addTimerInLoop(Timer *timer) &#123; loop_-&gt;assertInLoopThread(); bool earliestChanged = insert(timer); if (earliestChanged) &#123; resetTimerfd(timerfd_, timer-&gt;expiration()); &#125;&#125; 细节没有关注，整体思路就是更新 timer 列表 cancel 12345678910111213namespace muduo &#123; namespace net &#123; class TimerQueue : noncopyable &#123; public: void cancel(TimerId timerId) &#123; loop_-&gt;runInLoop(std::bind(&amp;TimerQueue::cancelInLoop, this, timerId)); &#125; ... &#125; &#125; // namespace net&#125; // namespace muduo 和 addTimer 一样，最终也是到 EventLoop 中执行，接下来看 cancelInLoop 12345678910111213141516void TimerQueue::cancelInLoop(TimerId timerId) &#123; loop_-&gt;assertInLoopThread(); assert(timers_.size() == activeTimers_.size()); ActiveTimer timer(timerId.timer_, timerId.sequence_); ActiveTimerSet::iterator it = activeTimers_.find(timer); if (it != activeTimers_.end()) &#123; size_t n = timers_.erase(Entry(it-&gt;first-&gt;expiration(), it-&gt;first)); assert(n == 1); (void) n; delete it-&gt;first; // FIXME: no delete please activeTimers_.erase(it); &#125; else if (callingExpiredTimers_) &#123; cancelingTimers_.insert(timer); &#125; assert(timers_.size() == activeTimers_.size());&#125; 基本思路也是更新 TimerQueue 中的 Timer 列表 5. Connector属于 TcpClient 的底层组件，用于发起 TCP 连接，它是 TcpClient 的成员，生命期由后者控制。 构造函数 12345678910111213141516171819202122232425namespace muduo &#123; namespace net &#123; class Connector : noncopyable, public std::enable_shared_from_this&lt;Connector&gt; &#123; public: Connector(EventLoop *loop, const InetAddress &amp;serverAddr) : loop_(loop), serverAddr_(serverAddr), connect_(false), state_(kDisconnected), retryDelayMs_(kInitRetryDelayMs) &#123; &#125; ... private: static const int kInitRetryDelayMs = 500; EventLoop *loop_; InetAddress serverAddr_; bool connect_; // atomic States state_; // FIXME: use atomic variable int retryDelayMs_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 绑定一个 EventLoop，指定一下 server 的地址和端口 start 可以在任何线程中调用 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253void Connector::start() &#123; connect_ = true; loop_-&gt;runInLoop(std::bind(&amp;Connector::startInLoop, this)); &#125;void Connector::startInLoop() &#123; loop_-&gt;assertInLoopThread(); assert(state_ == kDisconnected); if (connect_) &#123; connect(); &#125; else &#123; LOG_DEBUG &lt;&lt; \"do not connect\"; &#125;&#125;void Connector::connect() &#123; int sockfd = sockets::createNonblockingOrDie(serverAddr_.family()); int ret = sockets::connect(sockfd, serverAddr_.getSockAddr()); int savedErrno = (ret == 0) ? 0 : errno; switch (savedErrno) &#123; case 0: case EINPROGRESS: case EINTR: case EISCONN: connecting(sockfd); break; case EAGAIN: case EADDRINUSE: case EADDRNOTAVAIL: case ECONNREFUSED: case ENETUNREACH: retry(sockfd); break; case EACCES: case EPERM: case EAFNOSUPPORT: case EALREADY: case EBADF: case EFAULT: case ENOTSOCK: LOG_SYSERR &lt;&lt; \"connect error in Connector::startInLoop \" &lt;&lt; savedErrno; sockets::close(sockfd); break; default: LOG_SYSERR &lt;&lt; \"Unexpected error in Connector::startInLoop \" &lt;&lt; savedErrno; sockets::close(sockfd); // connectErrorCallback_(); break; &#125;&#125; start 方法从客户端开启一个 TCP 连接。基本做法是把 connect 放在 EventLoop 中执行，最终会开启 socket::connect 在 connect 方法中有很多种情况，最终归结为 3 大类：connecting、retry 和 close，close 就是 socket::close 就行了，重点看一下前两个。 先看 retry，retry 的操作是先把原来的 sockfd close 掉，然后设一个定时任务 runAfter，而这个延时时延每次翻倍，直到到达最大阈值。 12345678910111213void Connector::retry(int sockfd) &#123; sockets::close(sockfd); setState(kDisconnected); if (connect_) &#123; LOG_INFO &lt;&lt; \"Connector::retry - Retry connecting to \" &lt;&lt; serverAddr_.toIpPort() &lt;&lt; \" in \" &lt;&lt; retryDelayMs_ &lt;&lt; \" milliseconds. \"; loop_-&gt;runAfter(retryDelayMs_ / 1000.0, std::bind(&amp;Connector::startInLoop, shared_from_this())); retryDelayMs_ = std::min(retryDelayMs_ * 2, kMaxRetryDelayMs); &#125; else &#123; LOG_DEBUG &lt;&lt; \"do not connect\"; &#125;&#125; 另一种是 connecting，能进入 connecting 说明 connect 阻塞过程已经结束，接下来正式进入通讯过程，因此设置 channel 的两个回调。 1234567891011void Connector::connecting(int sockfd) &#123; setState(kConnecting); assert(!channel_); channel_.reset(new Channel(loop_, sockfd)); channel_-&gt;setWriteCallback( std::bind(&amp;Connector::handleWrite, this)); channel_-&gt;setErrorCallback( std::bind(&amp;Connector::handleError, this)); channel_-&gt;enableWriting();&#125; 首先看一下 WriteCallback，基本就是执行一下设置好的 newConnectionCallback_ 1234567891011121314151617181920212223242526void Connector::handleWrite() &#123; LOG_TRACE &lt;&lt; \"Connector::handleWrite \" &lt;&lt; state_; if (state_ == kConnecting) &#123; int sockfd = removeAndResetChannel(); int err = sockets::getSocketError(sockfd); if (err) &#123; LOG_WARN &lt;&lt; \"Connector::handleWrite - SO_ERROR = \" &lt;&lt; err &lt;&lt; \" \" &lt;&lt; strerror_tl(err); retry(sockfd); &#125; else if (sockets::isSelfConnect(sockfd)) &#123; LOG_WARN &lt;&lt; \"Connector::handleWrite - Self connect\"; retry(sockfd); &#125; else &#123; setState(kConnected); if (connect_) &#123; newConnectionCallback_(sockfd); &#125; else &#123; sockets::close(sockfd); &#125; &#125; &#125; else &#123; // what happened? assert(state_ == kDisconnected); &#125;&#125; 再看一下 ErrorCallback，基本操作就是 log 然后 retry。 123456789void Connector::handleError() &#123; LOG_ERROR &lt;&lt; \"Connector::handleError state=\" &lt;&lt; state_; if (state_ == kConnecting) &#123; int sockfd = removeAndResetChannel(); int err = sockets::getSocketError(sockfd); LOG_TRACE &lt;&lt; \"SO_ERROR = \" &lt;&lt; err &lt;&lt; \" \" &lt;&lt; strerror_tl(err); retry(sockfd); &#125;&#125; restart restart 只能在 loop thread 中被调用，目标是重新设置各项 connector 参数，然后重启这个 connector 1234567void Connector::restart() &#123; loop_-&gt;assertInLoopThread(); setState(kDisconnected); retryDelayMs_ = kInitRetryDelayMs; connect_ = true; startInLoop();&#125; 第三层1. EventLoop封装事件循环，也是事件分派的中心。它用 TimerQueue 作为计时器管理，用 Poller 作为 IO Multiplexing。 构造函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950namespace muduo &#123; namespace net &#123; class EventLoop : noncopyable &#123; public: EventLoop() : looping_(false), quit_(false), eventHandling_(false), callingPendingFunctors_(false), iteration_(0), threadId_(CurrentThread::tid()), poller_(Poller::newDefaultPoller(this)), timerQueue_(new TimerQueue(this)), wakeupFd_(createEventfd()), wakeupChannel_(new Channel(this, wakeupFd_)), currentActiveChannel_(NULL) &#123; LOG_DEBUG &lt;&lt; \"EventLoop created \" &lt;&lt; this &lt;&lt; \" in thread \" &lt;&lt; threadId_; if (t_loopInThisThread) &#123; LOG_FATAL &lt;&lt; \"Another EventLoop \" &lt;&lt; t_loopInThisThread &lt;&lt; \" exists in this thread \" &lt;&lt; threadId_; &#125; else &#123; t_loopInThisThread = this; &#125; wakeupChannel_-&gt;setReadCallback(std::bind(&amp;EventLoop::handleRead, this)); wakeupChannel_-&gt;enableReading(); &#125; ... private: bool looping_; /* atomic */ std::atomic&lt;bool&gt; quit_; bool eventHandling_; /* atomic */ bool callingPendingFunctors_; /* atomic */ int64_t iteration_; const pid_t threadId_; std::unique_ptr&lt;Poller&gt; poller_; std::unique_ptr&lt;TimerQueue&gt; timerQueue_; int wakeupFd_; std::unique_ptr&lt;Channel&gt; wakeupChannel_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 在构造函数中，构造了 Poller 和 TimerQueue。 另外就是构造了 wakeupChannel_，这个 channel 的 fd 是 eventfd，注册的 read 回调函数是下面这样，目前不知道这个 channel 是干啥的 1234567void EventLoop::handleRead() &#123; uint64_t one = 1; ssize_t n = sockets::read(wakeupFd_, &amp;one, sizeof one); if (n != sizeof one) &#123; LOG_ERROR &lt;&lt; \"EventLoop::handleRead() reads \" &lt;&lt; n &lt;&lt; \" bytes instead of 8\"; &#125;&#125; Channel 相关 一共有 3 个相关的函数 12345678910111213141516171819namespace muduo &#123; namespace net &#123; class EventLoop : noncopyable &#123; public: void updateChannel(Channel *channel); void removeChannel(Channel *channel); bool hasChannel(Channel *channel); ... private: std::unique_ptr&lt;Poller&gt; poller_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 这几个函数的本质是向内部的 poller_ 对象注册或注销 123456789101112131415161718192021void EventLoop::updateChannel(Channel *channel) &#123; assert(channel-&gt;ownerLoop() == this); assertInLoopThread(); poller_-&gt;updateChannel(channel);&#125;void EventLoop::removeChannel(Channel *channel) &#123; assert(channel-&gt;ownerLoop() == this); assertInLoopThread(); if (eventHandling_) &#123; assert(currentActiveChannel_ == channel || std::find(activeChannels_.begin(), activeChannels_.end(), channel) == activeChannels_.end()); &#125; poller_-&gt;removeChannel(channel);&#125;bool EventLoop::hasChannel(Channel *channel) &#123; assert(channel-&gt;ownerLoop() == this); assertInLoopThread(); return poller_-&gt;hasChannel(channel);&#125; Timer 相关 一共有 4 个相关方法： 123456789101112131415161718192021namespace muduo &#123; namespace net &#123; class EventLoop : noncopyable &#123; public: TimerId runAt(Timestamp time, TimerCallback cb); TimerId runAfter(double delay, TimerCallback cb); TimerId runEvery(double interval, TimerCallback cb); void cancel(TimerId timerId); ... private: std::unique_ptr&lt;TimerQueue&gt; timerQueue_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 这几个方法的目标是注册定时回调函数或取消，实现靠的是内部的 TimerQueue 对象： 1234567891011121314151617TimerId EventLoop::runAt(Timestamp time, TimerCallback cb) &#123; return timerQueue_-&gt;addTimer(std::move(cb), time, 0.0);&#125;TimerId EventLoop::runAfter(double delay, TimerCallback cb) &#123; Timestamp time(addTime(Timestamp::now(), delay)); return runAt(time, std::move(cb));&#125;TimerId EventLoop::runEvery(double interval, TimerCallback cb) &#123; Timestamp time(addTime(Timestamp::now(), interval)); return timerQueue_-&gt;addTimer(std::move(cb), time, interval);&#125;void EventLoop::cancel(TimerId timerId) &#123; return timerQueue_-&gt;cancel(timerId);&#125; loop EventLoop.h 123456789101112131415161718192021222324namespace muduo &#123; namespace net &#123; class EventLoop : noncopyable &#123; public: typedef std::function&lt;void()&gt; Functor; void loop(); ... private: bool looping_; /* atomic */ std::atomic&lt;bool&gt; quit_; int64_t iteration_; std::unique_ptr&lt;Poller&gt; poller_; // scratch variables ChannelList activeChannels_; Channel *currentActiveChannel_; ... &#125;; &#125; // namespace net&#125; // namespace muduo EventLoop.cc 12345678910111213141516171819202122232425262728293031323334const int kPollTimeMs = 10000;void EventLoop::loop() &#123; assert(!looping_); assertInLoopThread(); looping_ = true; quit_ = false; LOG_TRACE &lt;&lt; \"EventLoop \" &lt;&lt; this &lt;&lt; \" start looping\"; while (!quit_) &#123; activeChannels_.clear(); pollReturnTime_ = poller_-&gt;poll(kPollTimeMs, &amp;activeChannels_); ++iteration_; if (Logger::logLevel() &lt;= Logger::TRACE) &#123; printActiveChannels(); &#125; // TODO sort channel by priority eventHandling_ = true; for (Channel *channel : activeChannels_) &#123; currentActiveChannel_ = channel; currentActiveChannel_-&gt;handleEvent(pollReturnTime_); &#125; currentActiveChannel_ = NULL; eventHandling_ = false; doPendingFunctors(); &#125; LOG_TRACE &lt;&lt; \"EventLoop \" &lt;&lt; this &lt;&lt; \" stop looping\"; looping_ = false;&#125; loop 方法是一个死循环，它的主体是让 poller_ 监听一段时间的 IO 事件，然后调用已经注册好的这些 Channel 的各种 CallBack 函数进行处理；处理完这些事件以后，每一次循环会 doPendingFunctors 一次，看一下这个方法： 1234567891011121314void EventLoop::doPendingFunctors() &#123; std::vector&lt;Functor&gt; functors; callingPendingFunctors_ = true; &#123; MutexLockGuard lock(mutex_); functors.swap(pendingFunctors_); &#125; for (const Functor &amp;functor : functors) &#123; functor(); &#125; callingPendingFunctors_ = false;&#125; 这个函数的作用是有一些未执行的方法 pendingFunctors，在这个里面依次执行一下，pendingFunctors_ 是在以下的函数中被更新的： 123456789101112131415161718void EventLoop::runInLoop(Functor cb) &#123; if (isInLoopThread()) &#123; cb(); &#125; else &#123; queueInLoop(std::move(cb)); &#125;&#125;void EventLoop::queueInLoop(Functor cb) &#123; &#123; MutexLockGuard lock(mutex_); pendingFunctors_.push_back(std::move(cb)); &#125; if (!isInLoopThread() || callingPendingFunctors_) &#123; wakeup(); &#125;&#125; 外部调用的接口是 runInLoop，如果 cb 没有被立刻执行，那么它就会加入到 pendingFunctors_ 中，等待在每次循环中执行掉。 事件包括了两部分：刚才说的是 IO 事件，还有一部分是 Timer 定时的事件，Timer 事件其实也跟 doPendingFunctors 有关： 123456789101112TimerId EventLoop::runAt(Timestamp time, TimerCallback cb) &#123; return timerQueue_-&gt;addTimer(std::move(cb), time, 0.0);&#125;TimerId TimerQueue::addTimer(TimerCallback cb, Timestamp when, double interval) &#123; Timer *timer = new Timer(std::move(cb), when, interval); loop_-&gt;runInLoop( std::bind(&amp;TimerQueue::addTimerInLoop, this, timer)); return &#123;timer, timer-&gt;sequence()&#125;;&#125; 可以看到，在 addTimer 时，本质上也把回调函数加到了 EventLoop 的 runInLoop 中，只不过回调函数是 TimerQueue::addTimerInLoop。 Timer 一旦注册好以后，跟 IO 事件一样，因为 timer 本身也有一个 timerfd，到时以后会唤醒，本质上还是一个 fd 的 IO。 TimerQueue 和 刚才的 wakeupChannel_ 的本质其实是一样的，TimerQueue 内部也有一个 Channel，对应一个 timerfd TimerQueue.h 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152namespace muduo &#123; namespace net &#123; class TimerQueue : noncopyable &#123; private: const int timerfd_; Channel timerfdChannel_; ... &#125;; &#125; // namespace net&#125; // namespace muduonamespace muduo &#123; namespace net &#123; namespace detail &#123; int createTimerfd() &#123; int timerfd = ::timerfd_create(CLOCK_MONOTONIC, TFD_NONBLOCK | TFD_CLOEXEC); if (timerfd &lt; 0) &#123; LOG_SYSFATAL &lt;&lt; \"Failed in timerfd_create\"; &#125; return timerfd; &#125; ... &#125; // namespace detail &#125; // namespace net&#125; // namespace muduo``` `EventLoop.h````cppnamespace muduo &#123; namespace net &#123; class EventLoop : noncopyable &#123; private: int wakeupFd_; std::unique_ptr&lt;Channel&gt; wakeupChannel_; ... &#125;; &#125; // namespace net&#125; // namespace muduonamespace &#123; int createEventfd() &#123; int evtfd = ::eventfd(0, EFD_NONBLOCK | EFD_CLOEXEC); if (evtfd &lt; 0) &#123; LOG_SYSERR &lt;&lt; \"Failed in eventfd\"; abort(); &#125; return evtfd; &#125; 它们的底层都是一个系统调用，各产生一个 fd：timerfd 产生的是定时事件，eventfd 产生的事件，目标是让 poll 快速返回：向 eventfd 写入一个字节，就会产生一个可读事件，从而实现 poll 阻塞方法的快速返回。 无论是 TimerQueue 中的 timerfdChannel_，还是 EventLoop 中的 wakeupChannel_，都需要向 poller 中注册从而监听，这部分代码藏的比较深，在 Channel 的 enableReading 里面： 123456789101112131415void Channel::enableReading() &#123; events_ |= kReadEvent; update();&#125;void Channel::update() &#123; addedToLoop_ = true; loop_-&gt;updateChannel(this);&#125;void EventLoop::updateChannel(Channel *channel) &#123; assert(channel-&gt;ownerLoop() == this); assertInLoopThread(); poller_-&gt;updateChannel(channel);&#125; 套了好几层，最终还是向 poller 注册上了。。。 总之，这个 loop 方法是 Reactor 模式的核心：一个 loop 一个 Thread，先向 poller 注册好要监听的 Channel，Channel 又包括了 3 种类型：要监听的 IO 端口、timerfd 抽象成的时间事件 Channel 和 为了快速推出阻塞而额外留好的 eventfd 抽象成的 Channel。产生事件以后，poller 退出阻塞，调用各个 channel 上注册好的处理回调函数。另外，在 loop 期间其他线程加入的方法，先 pending，然后一并处理一下。 quit 12345678910111213141516171819202122232425namespace muduo &#123; namespace net &#123; class EventLoop : noncopyable &#123; public: void quit() &#123; quit_ = true; if (!isInLoopThread()) &#123; wakeup(); &#125; &#125; void wakeup() &#123; uint64_t one = 1; ssize_t n = sockets::write(wakeupFd_, &amp;one, sizeof one); if (n != sizeof one) &#123; LOG_ERROR &lt;&lt; \"EventLoop::wakeup() writes \" &lt;&lt; n &lt;&lt; \" bytes instead of 8\"; &#125; &#125; ... &#125;; &#125; // namespace net&#125; // namespace muduo quit 函数的目标是结束整个 loop：如果调用者就在当前线程，那么直接把 while 循环开始时的 quit 变量置为 true 即可；反之，为了快速退出 poller 的阻塞，向提前预留好的 wakeupfd 写一个字节，这样 poller 监听的 channel 就产生了可读事件，从而让阻塞退出。 第四层1. EventLoopThread启动一个线程，在其中运行 EventLoop::loop() 构造函数 1234567891011121314151617181920212223242526namespace muduo &#123; namespace net &#123; class EventLoopThread : noncopyable &#123; public: typedef std::function&lt;void(EventLoop *)&gt; ThreadInitCallback; explicit EventLoopThread(const ThreadInitCallback &amp;cb = ThreadInitCallback(), const string &amp;name = string()) : loop_(NULL), exiting_(false), thread_(std::bind(&amp;EventLoopThread::threadFunc, this), name), mutex_(), cond_(mutex_), callback_(cb) &#123; &#125; ... private: EventLoop *loop_ GUARDED_BY(mutex_); bool exiting_; Thread thread_; MutexLock mutex_; Condition cond_ GUARDED_BY(mutex_); ThreadInitCallback callback_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 其中，内部的 thread_ 对象注册的 func 是 123456789101112131415161718void EventLoopThread::threadFunc() &#123; EventLoop loop; if (callback_) &#123; callback_(&amp;loop); &#125; &#123; MutexLockGuard lock(mutex_); loop_ = &amp;loop; cond_.notify(); &#125; loop.loop(); //assert(exiting_); MutexLockGuard lock(mutex_); loop_ = NULL;&#125; 也就是说，在新开的 Thread 中，会先跑 ThreadInitCallBack，然后运行 loop.loop() startLoop 开始一个 loop，注意这里用 mutex 和 cond 保护 loop 的初始化 123456789101112131415EventLoop *EventLoopThread::startLoop() &#123; assert(!thread_.started()); thread_.start(); EventLoop *loop = NULL; &#123; MutexLockGuard lock(mutex_); while (loop_ == NULL) &#123; cond_.wait(); &#125; loop = loop_; &#125; return loop;&#125; 2. TcpConnection整个网络库的核心，封装一次 TCP 连接。每个 TcpConnection 必须归某个 EventLoop 管理，所有的 IO 会转移到这个线程。TcpConnection 既可以用于 Server，也可以用于 Client。 构造函数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455namespace muduo &#123; namespace net &#123; class TcpConnection : noncopyable, public std::enable_shared_from_this&lt;TcpConnection&gt; &#123; public: TcpConnection(EventLoop *loop, const string &amp;name, int sockfd, const InetAddress &amp;localAddr, const InetAddress &amp;peerAddr): loop_(CHECK_NOTNULL(loop)), name_(nameArg), state_(kConnecting), reading_(true), socket_(new Socket(sockfd)), channel_(new Channel(loop, sockfd)), localAddr_(localAddr), peerAddr_(peerAddr), highWaterMark_(64 * 1024 * 1024) &#123; channel_-&gt;setReadCallback(std::bind(&amp;TcpConnection::handleRead, this, _1)); channel_-&gt;setWriteCallback(std::bind(&amp;TcpConnection::handleWrite, this)); channel_-&gt;setCloseCallback(std::bind(&amp;TcpConnection::handleClose, this)); channel_-&gt;setErrorCallback(std::bind(&amp;TcpConnection::handleError, this)); LOG_DEBUG &lt;&lt; \"TcpConnection::ctor[\" &lt;&lt; name_ &lt;&lt; \"] at \" &lt;&lt; this &lt;&lt; \" fd=\" &lt;&lt; sockfd; socket_-&gt;setKeepAlive(true); &#125; ... private: enum StateE &#123; kDisconnected, kConnecting, kConnected, kDisconnecting &#125;; EventLoop *loop_; const string name_; StateE state_; bool reading_; std::unique_ptr&lt;Socket&gt; socket_; std::unique_ptr&lt;Channel&gt; channel_; const InetAddress localAddr_; const InetAddress peerAddr_; size_t highWaterMark_; ... &#125; &#125; // namespace net&#125; // namespace muduo 这个构造函数比较复杂：一个 TcpConnection 和一个 EventLoop 绑定；根据 sockfd 构造一个 Socket 对象；根据 EventLoop 和 sockfd 构造一个 Channel 对象。 localAddr_、peerAddr_ 和 highWaterMark_ 暂时还不知道代表什么； 每个 channel 可以设置 4 种 CallBack，在构造函数中都设置了，接下来依次看一下. 4 种 CallBack ReadCallBack 12345678910111213141516void TcpConnection::handleRead(Timestamp receiveTime) &#123; loop_-&gt;assertInLoopThread(); int savedErrno = 0; ssize_t n = inputBuffer_.readFd(channel_-&gt;fd(), &amp;savedErrno); if (n &gt; 0) &#123; messageCallback_(shared_from_this(), &amp;inputBuffer_, receiveTime); &#125; else if (n == 0) &#123; handleClose(); &#125; else &#123; errno = savedErrno; LOG_SYSERR &lt;&lt; \"TcpConnection::handleRead\"; handleError(); &#125;&#125; ReadCallback 的目标是从内核把数据读到 inputBuffer_ 中，然后执行注册好的 messageCallback_。 WriteCallBack 123456789101112131415161718192021222324void TcpConnection::handleWrite() &#123; loop_-&gt;assertInLoopThread(); if (channel_-&gt;isWriting()) &#123; ssize_t n = sockets::write(channel_-&gt;fd(), outputBuffer_.peek(), outputBuffer_.readableBytes()); if (n &gt; 0) &#123; outputBuffer_.retrieve(n); if (outputBuffer_.readableBytes() == 0) &#123; channel_-&gt;disableWriting(); if (writeCompleteCallback_) &#123; loop_-&gt;queueInLoop(std::bind(writeCompleteCallback_, shared_from_this())); &#125; if (state_ == kDisconnecting) &#123; shutdownInLoop(); &#125; &#125; &#125; else &#123; LOG_SYSERR &lt;&lt; \"TcpConnection::handleWrite\"; &#125; &#125; else &#123; LOG_TRACE &lt;&lt; \"Connection fd = \" &lt;&lt; channel_-&gt;fd() &lt;&lt; \" is down, no more writing\"; &#125;&#125; WriteCallBack 的目标是把 outputBuffer_ 中的数据写到内核缓冲区中。如果把 outputBuffer_ 中的数据都写完了的话，那么就调用注册好的 writeCompleteCallback_ CloseCallBack 12345678910111213141516void TcpConnection::handleClose() &#123; loop_-&gt;assertInLoopThread(); LOG_TRACE &lt;&lt; \"fd = \" &lt;&lt; channel_-&gt;fd() &lt;&lt; \" state = \" &lt;&lt; stateToString(); assert(state_ == kConnected || state_ == kDisconnecting); setState(kDisconnected); channel_-&gt;disableAll(); TcpConnectionPtr guardThis(shared_from_this()); connectionCallback_(guardThis); // must be the last line closeCallback_(guardThis);&#125; 在 CloseCallBack 中不关闭 sockfd，，只是设置状态且关闭 Channel 的读写能力，并调用 connectionCallback_ ErrorCallBack 12345void TcpConnection::handleError() &#123; int err = sockets::getSocketError(channel_-&gt;fd()); LOG_ERROR &lt;&lt; \"TcpConnection::handleError [\" &lt;&lt; name_ &lt;&lt; \"] - SO_ERROR = \" &lt;&lt; err &lt;&lt; \" \" &lt;&lt; strerror_tl(err);&#125; 这个比较简单，获取一下 SocketError 就可以了 send 方法 总共有 3 个 public 方法 TcpConnection.h 123456789101112131415namespace muduo &#123; namespace net &#123; class TcpConnection : noncopyable, public std::enable_shared_from_this&lt;TcpConnection&gt; &#123; public: void send(const void *message, int len); void send(const StringPiece &amp;message); void send(Buffer *message); // this one will swap data ... &#125;; &#125; // namespace net&#125; // namespace muduo 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061void TcpConnection::send(const StringPiece &amp;message) &#123; if (state_ == kConnected) &#123; if (loop_-&gt;isInLoopThread()) &#123; sendInLoop(message); &#125; else &#123; void (TcpConnection::*fp)(const StringPiece &amp;message) = &amp;TcpConnection::sendInLoop; loop_-&gt;runInLoop(std::bind(fp,this, message.as_string())); //std::forward&lt;string&gt;(message))); &#125; &#125;&#125;void TcpConnection::sendInLoop(const StringPiece &amp;message) &#123; sendInLoop(message.data(), message.size());&#125;void TcpConnection::sendInLoop(const void *data, size_t len) &#123; loop_-&gt;assertInLoopThread(); ssize_t nwrote = 0; size_t remaining = len; bool faultError = false; if (state_ == kDisconnected) &#123; LOG_WARN &lt;&lt; \"disconnected, give up writing\"; return; &#125; // if no thing in output queue, try writing directly if (!channel_-&gt;isWriting() &amp;&amp; outputBuffer_.readableBytes() == 0) &#123; nwrote = sockets::write(channel_-&gt;fd(), data, len); if (nwrote &gt;= 0) &#123; remaining = len - nwrote; if (remaining == 0 &amp;&amp; writeCompleteCallback_) &#123; loop_-&gt;queueInLoop(std::bind(writeCompleteCallback_, shared_from_this())); &#125; &#125; else // nwrote &lt; 0 &#123; nwrote = 0; if (errno != EWOULDBLOCK) &#123; LOG_SYSERR &lt;&lt; \"TcpConnection::sendInLoop\"; if (errno == EPIPE || errno == ECONNRESET) // FIXME: any others? &#123; faultError = true; &#125; &#125; &#125; &#125; assert(remaining &lt;= len); if (!faultError &amp;&amp; remaining &gt; 0) &#123; size_t oldLen = outputBuffer_.readableBytes(); if (oldLen + remaining &gt;= highWaterMark_ &amp;&amp; oldLen &lt; highWaterMark_ &amp;&amp; highWaterMarkCallback_) &#123; loop_-&gt;queueInLoop(std::bind(highWaterMarkCallback_, shared_from_this(), oldLen + remaining)); &#125; outputBuffer_.append(static_cast&lt;const char *&gt;(data) + nwrote, remaining); if (!channel_-&gt;isWriting()) &#123; channel_-&gt;enableWriting(); &#125; &#125;&#125; send 方法的主要思想是：如果 outputBuffer_ 中没有数据了，那么尝试直接通过 sockfd 写，不经过 outBuffer_ 这一中间环节了；否则的话，就把数据先写到 outputBuffer_ 中，等待 Channel 有写事件的时候，再通过注册好的 handleWrite 方法中，从 outputBuffer 写到 sockfd 中。 shutdown() 方法 12345678910111213141516void TcpConnection::shutdown() &#123; // FIXME: use compare and swap if (state_ == kConnected) &#123; setState(kDisconnecting); // FIXME: shared_from_this()? loop_-&gt;runInLoop(std::bind(&amp;TcpConnection::shutdownInLoop, this)); &#125;&#125;void TcpConnection::shutdownInLoop() &#123; loop_-&gt;assertInLoopThread(); if (!channel_-&gt;isWriting()) &#123; // we are not writing socket_-&gt;shutdownWrite(); &#125;&#125; 比较简单，就是同步或者异步 shutdown socket forceClose 和 forceCloseWithDelay 123456789101112131415161718192021222324void TcpConnection::forceClose() &#123; // FIXME: use compare and swap if (state_ == kConnected || state_ == kDisconnecting) &#123; setState(kDisconnecting); loop_-&gt;queueInLoop(std::bind(&amp;TcpConnection::forceCloseInLoop, shared_from_this())); &#125;&#125;void TcpConnection::forceCloseWithDelay(double seconds) &#123; if (state_ == kConnected || state_ == kDisconnecting) &#123; setState(kDisconnecting); loop_-&gt;runAfter( seconds, makeWeakCallback(shared_from_this(), &amp;TcpConnection::forceClose)); // not forceCloseInLoop to avoid race condition &#125;&#125;void TcpConnection::forceCloseInLoop() &#123; loop_-&gt;assertInLoopThread(); if (state_ == kConnected || state_ == kDisconnecting) &#123; // as if we received 0 byte in handleRead(); handleClose(); &#125;&#125; 也比较简单，最终是同步或异步调用 CloseCallBack 第五层1. EventLoopThreadPool用于创建 IO 线程池，也就是说把 TcpConnection 分派到一组运行 EventLoop 的线程上。它是 TcpServer 的成员，生命期由后者控制 构造函数 12345678910111213141516171819202122232425namespace muduo &#123; namespace net &#123; class EventLoopThreadPool : noncopyable &#123; public: EventLoopThreadPool(EventLoop *baseLoop, const string &amp;nameArg) : baseLoop_(baseLoop), name_(nameArg), started_(false), numThreads_(0), next_(0) &#123; &#125; ... private: EventLoop *baseLoop_; string name_; bool started_; int numThreads_; int next_; ... &#125;; &#125; // namespace net&#125; // namespace muduo EventLoopThreadPool 有一个基本的 baseLoop_ start 12345678910111213141516171819202122232425262728293031323334namespace muduo &#123; namespace net &#123; class EventLoopThreadPool : noncopyable &#123; public: void start(const ThreadInitCallback &amp;cb = ThreadInitCallback()) &#123; assert(!started_); baseLoop_-&gt;assertInLoopThread(); started_ = true; for (int i = 0; i &lt; numThreads_; ++i) &#123; char buf[name_.size() + 32]; snprintf(buf, sizeof buf, \"%s%d\", name_.c_str(), i); EventLoopThread *t = new EventLoopThread(cb, buf); threads_.push_back(std::unique_ptr&lt;EventLoopThread&gt;(t)); loops_.push_back(t-&gt;startLoop()); &#125; if (numThreads_ == 0 &amp;&amp; cb) &#123; cb(baseLoop_); &#125; &#125; ... private: std::vector&lt;std::unique_ptr&lt;EventLoopThread&gt;&gt; threads_; std::vector&lt;EventLoop *&gt; loops_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 这个方法会开 numThreads_ 个线程，并且在 numThreads_ 线程里面让 EventLoop 开始 loop 起来，在 EventLoopThreadPool 中保存好这个 Thread 和这些 EventLoop getNextLoop 12345678910111213141516171819202122232425262728namespace muduo &#123; namespace net &#123; class EventLoopThreadPool : noncopyable &#123; public: EventLoop *getNextLoop() &#123; baseLoop_-&gt;assertInLoopThread(); assert(started_); EventLoop *loop = baseLoop_; if (!loops_.empty()) &#123; // round-robin loop = loops_[next_]; ++next_; if (implicit_cast&lt;size_t&gt;(next_) &gt;= loops_.size()) &#123; next_ = 0; &#125; &#125; return loop; &#125; ... private: int numThreads_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 这个方法是用来把 TcpConnection 分派到一组运行 EventLoop 的线程上，采取的方式是 round-robin 2. TcpServer用于编写网络服务器，接受客户的连接 构造函数 12345678910111213141516171819202122232425262728293031323334namespace muduo &#123; namespace net &#123; class TcpServer : noncopyable &#123; public: TcpServer(EventLoop *loop, const InetAddress &amp;listenAddr, const string &amp;nameArg, Option option = kNoReusePort) : loop_(CHECK_NOTNULL(loop)), ipPort_(listenAddr.toIpPort()), name_(nameArg), acceptor_(new Acceptor(loop, listenAddr,option == kReusePort)), threadPool_(new EventLoopThreadPool(loop, name_)), connectionCallback_(defaultConnectionCallback), messageCallback_(defaultMessageCallback), nextConnId_(1) &#123; acceptor_-&gt;setNewConnectionCallback(std::bind(&amp;TcpServer::newConnection, this, _1, _2)); &#125; ... private: EventLoop *loop_; // the acceptor loop const string ipPort_; const string name_; std::unique_ptr&lt;Acceptor&gt; acceptor_; // avoid revealing Acceptor std::shared_ptr&lt;EventLoopThreadPool&gt; threadPool_; ConnectionCallback connectionCallback_; MessageCallback messageCallback_; int nextConnId_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 这个构造函数里面也干了很多事情：绑定 EventLoop；构造 Acceptor、构造 EventLoopThreadPool、设置 connectionCallback_ 和 messageCallback_。 其中，acceptor_ 设置了 NewConnectionCallback ： 1234567891011121314151617181920212223242526void TcpServer::newConnection(int sockfd, const InetAddress &amp;peerAddr) &#123; loop_-&gt;assertInLoopThread(); EventLoop *ioLoop = threadPool_-&gt;getNextLoop(); char buf[64]; snprintf(buf, sizeof buf, \"-%s#%d\", ipPort_.c_str(), nextConnId_); ++nextConnId_; string connName = name_ + buf; LOG_INFO &lt;&lt; \"TcpServer::newConnection [\" &lt;&lt; name_ &lt;&lt; \"] - new connection [\" &lt;&lt; connName &lt;&lt; \"] from \" &lt;&lt; peerAddr.toIpPort(); InetAddress localAddr(sockets::getLocalAddr(sockfd)); // FIXME poll with zero timeout to double confirm the new connection // FIXME use make_shared if necessary TcpConnectionPtr conn( new TcpConnection(ioLoop, connName, sockfd, localAddr, peerAddr)); connections_[connName] = conn; conn-&gt;setConnectionCallback(connectionCallback_); conn-&gt;setMessageCallback(messageCallback_); conn-&gt;setWriteCompleteCallback(writeCompleteCallback_); conn-&gt;setCloseCallback(std::bind(&amp;TcpServer::removeConnection, this, _1)); // FIXME: unsafe ioLoop-&gt;runInLoop(std::bind(&amp;TcpConnection::connectEstablished, conn));&#125; 这个方法比较重要。 Acceptor 用于接收 TCP 连接，它监听的是 listenfd；在连接到来后，会将 connfd 传入 newConnectionCallback_ 方法。 在这个 NewConnectionCallback 方法中，首先从 ThreadPool 中 Round-Robin 式的找一个 EventLoop，然后根据这个 connfd 新建一个 TcpConnection 传入这个 EventLoop 中，然后把这个 TcpConnection 的 4 种 CallBack 设置好，最终让这个 TcpConnection 放到 EventLoop 中开始跑。 接下来看看这 4 个 CallBack，这 4 个 CallBack 里面，connectionCallback_、messageCallback_ 和 writeCompleteCallback_ 都可以在 TcpServer 中外部 set，而 CloseCallback 是固定的： TcpConnection 的 4 个 CallBack connectionCallback_ 123456void muduo::net::defaultConnectionCallback(const TcpConnectionPtr &amp;conn) &#123; LOG_TRACE &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); // do not call conn-&gt;forceClose(), because some users want to register message callback only.&#125; Default 情况下啥也不干 messageCallback_ 123void muduo::net::defaultMessageCallback(const TcpConnectionPtr &amp;, Buffer *buf, Timestamp) &#123; buf-&gt;retrieveAll();&#125; writeCompleteCallback_ 没有 Default closeCallBack_ 12345678910111213141516171819202122232425262728conn-&gt;setCloseCallback(std::bind(&amp;TcpServer::removeConnection, this, _1)); // FIXME: unsafevoid TcpServer::removeConnection(const TcpConnectionPtr &amp;conn) &#123; // FIXME: unsafe loop_-&gt;runInLoop(std::bind(&amp;TcpServer::removeConnectionInLoop, this, conn));&#125;void TcpServer::removeConnectionInLoop(const TcpConnectionPtr &amp;conn) &#123; loop_-&gt;assertInLoopThread(); LOG_INFO &lt;&lt; \"TcpServer::removeConnectionInLoop [\" &lt;&lt; name_ &lt;&lt; \"] - connection \" &lt;&lt; conn-&gt;name(); size_t n = connections_.erase(conn-&gt;name()); (void) n; assert(n == 1); EventLoop *ioLoop = conn-&gt;getLoop(); ioLoop-&gt;queueInLoop(std::bind(&amp;TcpConnection::connectDestroyed, conn));&#125;void TcpConnection::connectDestroyed() &#123; loop_-&gt;assertInLoopThread(); if (state_ == kConnected) &#123; setState(kDisconnected); channel_-&gt;disableAll(); connectionCallback_(shared_from_this()); &#125; channel_-&gt;remove();&#125; 这个 CloseCallBack 的目标是移除对应 EventLoop 中的 Channel start 方法 123456789void TcpServer::start() &#123; if (started_.getAndSet(1) == 0) &#123; threadPool_-&gt;start(threadInitCallback_); assert(!acceptor_-&gt;listenning()); loop_-&gt;runInLoop( std::bind(&amp;Acceptor::listen, get_pointer(acceptor_))); &#125;&#125; 首先一个 CAS 操作保证一个 TcpServer 只 start 一次，然后让 ThreadPool 开始启动线程，同时让 Acceptor 开始监听端口 析构函数 1234567891011TcpServer::~TcpServer() &#123; loop_-&gt;assertInLoopThread(); LOG_TRACE &lt;&lt; \"TcpServer::~TcpServer [\" &lt;&lt; name_ &lt;&lt; \"] destructing\"; for (auto &amp;item : connections_) &#123; TcpConnectionPtr conn(item.second); item.second.reset(); conn-&gt;getLoop()-&gt;runInLoop( std::bind(&amp;TcpConnection::connectDestroyed, conn)); &#125;&#125; 析构函数中，让所有的 TcpConnection 对象的 Channel 都销毁掉。 3. TcpClient用于编写网络客户端，能发起连接，并且有重试功能。 构造函数 1234567891011121314151617181920212223242526272829303132333435namespace muduo &#123; namespace net &#123; class TcpClient : noncopyable &#123; public: TcpClient(EventLoop *loop, const InetAddress &amp;serverAddr, const string &amp;nameArg): loop_(CHECK_NOTNULL(loop)), connector_(new Connector(loop, serverAddr)), name_(nameArg), connectionCallback_(defaultConnectionCallback), messageCallback_(defaultMessageCallback), retry_(false), connect_(true), nextConnId_(1) &#123; connector_-&gt;setNewConnectionCallback( std::bind(&amp;TcpClient::newConnection, this, _1)); LOG_INFO &lt;&lt; \"TcpClient::TcpClient[\" &lt;&lt; name_ &lt;&lt; \"] - connector \" &lt;&lt; get_pointer(connector_); &#125; ... private: EventLoop *loop_; ConnectorPtr connector_; // avoid revealing Connector const string name_; ConnectionCallback connectionCallback_; MessageCallback messageCallback_; bool retry_; // atomic bool connect_; // atomic // always in loop thread int nextConnId_; ... &#125;; &#125; // namespace net&#125; // namespace muduo TcpClient 绑定一个 EventLoop，内部有一个 connector 用于新建 TCP 连接；connectionCallback_ 和 messageCallback_ 初始都设为 default，default 是啥也不干；Connector 要设置 NewConnection 的 Callback 如下： 1234567891011121314151617181920212223242526void TcpClient::newConnection(int sockfd) &#123; loop_-&gt;assertInLoopThread(); InetAddress peerAddr(sockets::getPeerAddr(sockfd)); char buf[32]; snprintf(buf, sizeof buf, \":%s#%d\", peerAddr.toIpPort().c_str(), nextConnId_); ++nextConnId_; string connName = name_ + buf; InetAddress localAddr(sockets::getLocalAddr(sockfd)); // FIXME poll with zero timeout to double confirm the new connection // FIXME use make_shared if necessary TcpConnectionPtr conn( new TcpConnection(loop_, connName, sockfd, localAddr, peerAddr)); conn-&gt;setConnectionCallback(connectionCallback_); conn-&gt;setMessageCallback(messageCallback_); conn-&gt;setWriteCompleteCallback(writeCompleteCallback_); conn-&gt;setCloseCallback( std::bind(&amp;TcpClient::removeConnection, this, _1)); // FIXME: unsafe &#123; MutexLockGuard lock(mutex_); connection_ = conn; &#125; conn-&gt;connectEstablished();&#125; 这个 Callback 的目标是打开一个 TcpConnection，这个 TcpConnection 注册好各种回调，然后 channel 开始监听。 connect 让内部的 connector 开始 connect 1234567void TcpClient::connect() &#123; // FIXME: check state LOG_INFO &lt;&lt; \"TcpClient::connect[\" &lt;&lt; name_ &lt;&lt; \"] - connecting to \" &lt;&lt; connector_-&gt;serverAddress().toIpPort(); connect_ = true; connector_-&gt;start();&#125; disconnect 断开 TcpConnection，对于 TcpClient，它只有一个 TcpConnection，这点和 TcpServer 明显不同 12345678910void TcpClient::disconnect() &#123; connect_ = false; &#123; MutexLockGuard lock(mutex_); if (connection_) &#123; connection_-&gt;shutdown(); &#125; &#125;&#125; stop 让内部的 connector stop 1234void TcpClient::stop() &#123; connect_ = false; connector_-&gt;stop();&#125; 以上就把 muduo 中的 net 部分基本介绍完了，好累。。","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"Net","slug":"Net","permalink":"http://yoursite.com/tags/Net/"}]},{"title":"muduo 网络编程示例之零：前言","slug":"muduo网络编程示例之零：前言","date":"2020-03-09T02:55:49.386Z","updated":"2020-03-09T02:56:26.000Z","comments":true,"path":"开源组件/muduo/muduo网络编程示例之零：前言/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/muduo%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%A4%BA%E4%BE%8B%E4%B9%8B%E9%9B%B6%EF%BC%9A%E5%89%8D%E8%A8%80/","excerpt":"","text":"原文链接 https://blog.csdn.net/Solstice/article/details/6171831 我将会写一系列文章，介绍用 muduo 网络库完成常见的 TCP 网络编程任务。目前计划如下： UNP 中的简单协议，包括 echo、daytime、time、discard 等。 Boost.Asio 中的示例，包括 timer2~6、chat 等。 Java Netty 中的示例，包括 discard、echo、uptime 等，其中的 discard 和 echo 带流量统计功能。 Python twisted 中的示例，包括 finger01~07 用于测试两台机器的往返延迟的 roundtrip 用于测试两台机器的带宽的 pingpong 云风的串并转换连接服务器 multiplexer，包括单线程和多线程两个版本。 文件传输 一个基于 TCP 的应用层广播 hub socks4a 代理服务器，包括简单的 TCP 中继(relay)。 一个 Sudoku 服务器的演变，从单线程到多线程，从阻塞到 event-based。 一个提供短址服务的 httpd 服务器 其中前面 7 个已经放到了 muduo 代码的 examples 目录中，下载地址是： http://muduo.googlecode.com/files/muduo-0.1.5-alpha.tar.gz 这些例子都比较简单，逻辑不复杂，代码也很短，适合摘取关键部分放到博客上。其中一些有一定的代表性与针对性，比如“如何传输完整的文件”估计是网络编程的初学者经常遇到的问题。请注意，muduo 是设计来开发内网的网络程序，它没有做任何安全方面的加强措施，如果用在公网上可能会受到攻击，在后面的例子中我会谈到这一点。 本系列文章适用于 Linux 2.6.x (x &gt; 28)，主要测试发行版为 Ubuntu 10.04 LTS 和 Debian 6.0 Squeeze，64-bit x86 硬件。 1. TCP 网络编程本质论 2. Muduo 简介 1. TCP 网络编程本质论我认为，TCP 网络编程最本质的是处理三个半事件： 连接的建立，包括服务端接受 (accept) 新连接和客户端成功发起 (connect) 连接。 连接的断开，包括主动断开 (close 或 shutdown) 和被动断开 (read 返回 0)。 消息到达，文件描述符可读。这是最为重要的一个事件，对它的处理方式决定了网络编程的风格（阻塞还是非阻塞，如何处理分包，应用层的缓冲如何设计等等）。 消息发送完毕，这算半个。对于低流量的服务，可以不必关心这个事件；另外，这里“发送完毕”是指将数据写入操作系统的缓冲区，将由 TCP 协议栈负责数据的发送与重传，不代表对方已经收到数据。 这其中有很多难点，也有很多细节需要注意，比方说： 如果要主动关闭连接，如何保证对方已经收到全部数据？如果应用层有缓冲（这在非阻塞网络编程中是必须的，见下文），那么如何保证先发送完缓冲区中的数据，然后再断开连接。直接调用 close(2) 恐怕是不行的。 如果主动发起连接，但是对方主动拒绝，如何定期 (带 back-off) 重试？ 非阻塞网络编程该用边沿触发(edge trigger)还是电平触发(level trigger)？（这两个中文术语有其他译法，我选择了一个电子工程师熟悉的说法。）如果是电平触发，那么什么时候关注 EPOLLOUT 事件？会不会造成 busy-loop？如果是边沿触发，如何防止漏读造成的饥饿？epoll 一定比 poll 快吗？ 在非阻塞网络编程中，为什么要使用应用层缓冲区？假如一次读到的数据不够一个完整的数据包，那么这些已经读到的数据是不是应该先暂存在某个地方，等剩余的数据收到之后再一并处理？见 lighttpd 关于 /r/n/r/n 分包的 bug。假如数据是一个字节一个字节地到达，间隔 10ms，每个字节触发一次文件描述符可读 (readable) 事件，程序是否还能正常工作？lighttpd 在这个问题上出过安全漏洞。 在非阻塞网络编程中，如何设计并使用缓冲区？一方面我们希望减少系统调用，一次读的数据越多越划算，那么似乎应该准备一个大的缓冲区。另一方面，我们系统减少内存占用。如果有 10k 个连接，每个连接一建立就分配 64k 的读缓冲的话，将占用 640M 内存，而大多数时候这些缓冲区的使用率很低。muduo 用 readv 结合栈上空间巧妙地解决了这个问题。 如果使用发送缓冲区，万一接收方处理缓慢，数据会不会一直堆积在发送方，造成内存暴涨？如何做应用层的流量控制？ 如何设计并实现定时器？并使之与网络 IO 共用一个线程，以避免锁。 这些问题在 muduo 的代码中可以找到答案。 2. Muduo 简介我编写 Muduo 网络库的目的之一就是简化日常的 TCP 网络编程，让程序员能把精力集中在业务逻辑的实现上，而不要天天和 Sockets API 较劲。借用 Brooks 的话说，我希望 Muduo 能减少网络编程中的偶发复杂性 (accidental complexity)。 Muduo 只支持 Linux 2.6.x 下的并发非阻塞 TCP 网络编程，它的安装方法见陈硕的 blog 文章。 Muduo 的使用非常简单，不需要从指定的类派生，也不用覆写虚函数，只需要注册几个回调函数去处理前面提到的三个半事件就行了。 以经典的 echo 回显服务为例： 定义 EchoServer class，不需要派生自任何基类： 1234567891011121314151617181920#ifndef MUDUO_EXAMPLES_SIMPLE_ECHO_ECHO_H #define MUDUO_EXAMPLES_SIMPLE_ECHO_ECHO_H// RFC 862 class EchoServer &#123; public: EchoServer(muduo::net::EventLoop* loop, const muduo::net::InetAddress&amp; listenAddr); void start();private: void onConnection(const muduo::net::TcpConnectionPtr&amp; conn); void onMessage(const muduo::net::TcpConnectionPtr&amp; conn, muduo::net::Buffer* buf, muduo::Timestamp time); muduo::net::EventLoop* loop_; muduo::net::TcpServer server_; &#125;;#endif // MUDUO_EXAMPLES_SIMPLE_ECHO_ECHO_H 在构造函数里注册回调函数： 123456789101112131415EchoServer::EchoServer(EventLoop* loop, const InetAddress&amp; listenAddr) : loop_(loop), server_(loop, listenAddr, \"EchoServer\") &#123; server_.setConnectionCallback( boost::bind(&amp;EchoServer::onConnection, this, _1)); server_.setMessageCallback( boost::bind(&amp;EchoServer::onMessage, this, _1, _2, _3)); &#125;void EchoServer::start() &#123; server_.start(); &#125; 实现 EchoServer::onConnection() 和 EchoServer::onMessage()： 12345678910111213141516void EchoServer::onConnection(const TcpConnectionPtr&amp; conn) &#123; LOG_INFO &lt;&lt; \"EchoServer - \" &lt;&lt; conn-&gt;peerAddress().toHostPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;localAddress().toHostPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); &#125;void EchoServer::onMessage(const TcpConnectionPtr&amp; conn, Buffer* buf, Timestamp time) &#123; string msg(buf-&gt;retrieveAsString()); LOG_INFO &lt;&lt; conn-&gt;name() &lt;&lt; \" echo \" &lt;&lt; msg.size() &lt;&lt; \" bytes at \" &lt;&lt; time.toString(); conn-&gt;send(msg); &#125; 在 main() 里用 EventLoop 让整个程序跑起来： 123456789101112#include \"echo.h\"using namespace muduo; using namespace muduo::net;int main() &#123; LOG_INFO &lt;&lt; \"pid = \" &lt;&lt; getpid(); EventLoop loop; InetAddress listenAddr(2007); EchoServer server(&amp;loop, listenAddr); server.start(); loop.loop(); &#125; 完整的代码见 muduo/examples/simple/echo。 这个几十行的小程序实现了一个并发的 echo 服务程序，可以同时处理多个连接。 对这个程序的详细分析见下一篇博客《Muduo 网络编程示例之一：五个简单 TCP 协议》————————————————版权声明：本文为CSDN博主「陈硕」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/Solstice/article/details/6171831","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"Net","slug":"Net","permalink":"http://yoursite.com/tags/Net/"}]},{"title":"以 boost 中的 function 和 bind 取代虚函数","slug":"以boost中的function和bind取代虚函数","date":"2020-03-09T02:54:14.428Z","updated":"2020-03-09T02:54:59.000Z","comments":true,"path":"开源组件/muduo/以boost中的function和bind取代虚函数/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/%E4%BB%A5boost%E4%B8%AD%E7%9A%84function%E5%92%8Cbind%E5%8F%96%E4%BB%A3%E8%99%9A%E5%87%BD%E6%95%B0/","excerpt":"","text":"原文链接： https://blog.csdn.net/Solstice/article/details/3066268 这是一篇比较情绪化的 blog，中心思想是“继承就像一条贼船，上去就下不来了”，而借助 boost::function和boost::bind，大多数情况下，你都不用上贼船。 boost::function 和 boost::bind 已经纳入了 std::tr1，这或许是 C++0x 最值得期待的功能，它将彻底改变 C++ 库的设计方式，以及应用程序的编写方式。 Scott Meyers 的 Effective C++ 3rd ed.第35条款提到了以 boost::function 和 boost:bind 取代虚函数的做法，这里谈谈我自己使用的感受。 1. 基本用途 2. 对程序库的影响 例1：线程库 常规OO设计： 基于closure的设计： 例2：网络库 3. 对面向对象程序设计的影响 4. 对面向对象设计模式的影响 5. 依赖注入与单元测试 6. 什么时候使用继承？ 7. 基于接口的设计 8. 实现Signal/Slot 1. 基本用途boost::function 就像 C# 里的 delegate，可以指向任何函数，包括成员函数。当用 bind 把某个成员函数绑到某个对象上时，我们得到了一个closure（闭包）。例如： 12345678910111213141516171819202122232425262728293031323334353637383940414243#include &lt;boost/function.hpp&gt;#include &lt;boost/bind.hpp&gt;#include &lt;iostream&gt;class Foo &#123;public: void methodA() &#123; std::cout &lt;&lt; \"Foo::methodA\\n\"; &#125; void methodInt(int a) &#123; std::cout &lt;&lt; \"Foo::methodInt\\t\" &lt;&lt; a &lt;&lt; \"\\n\"; &#125;&#125;;class Bar &#123;public: void methodB() &#123; std::cout &lt;&lt; \"Bar::methodB\\n\"; &#125;&#125;;int main(int argc, char **args) &#123; boost::function&lt;void()&gt; f1; // 无参数，无返回值 Foo foo; f1 = boost::bind(&amp;Foo::methodA, &amp;foo); f1(); // 调用 foo.methodA(); Bar bar; f1 = boost::bind(&amp;Bar::methodB, &amp;bar); f1(); // 调用 bar.methodB(); f1 = boost::bind(&amp;Foo::methodInt, &amp;foo, 42); f1(); // 调用 foo.methodInt(42); boost::function&lt;void(int)&gt; f2; // int 参数，无返回值 f2 = boost::bind(&amp;Foo::methodInt, &amp;foo, _1); f2(53); // 调用 foo.methodInt(53); return 0;&#125; 如果没有 boost::bind，那么 boost::function 就什么都不是，而有了 bind()，“同一个类的不同对象可以 delegate 给不同的实现，从而实现不同的行为”（myan 语），简直就无敌了。 2. 对程序库的影响程序库的设计不应该给使用者带来不必要的限制（耦合），而继承是仅次于最强的一种耦合（最强耦合的是友元）。如果一个程序库限制其使用者必须从某个class派生，那么我觉得这是一个糟糕的设计。不巧的是，目前有些程序库就是这么做的。 例1：线程库常规OO设计：写一个 Thread base class，含有（纯）虚函数 Thread#run()，然后应用程序派生一个继承 class，覆写 run()。程序里的每一种线程对应一个 Thread 的派生类。例如 Java 的 Thread 可以这么用。 缺点：如果一个class的三个method需要在三个不同的线程中执行，就得写helper class(es)并玩一些OO把戏。 基于closure的设计：令 Thread 是一个具体类，其构造函数接受 Callable 对象。应用程序只需提供一个 Callable 对象，创建一份Thread 实体，调用 Thread#start() 即可。Java 的 Thread 也可以这么用，传入一个 Runnable 对象。C# 的 Thread 只支持这一种用法，构造函数的参数是 delegate ThreadStart。boost::thread 也只支持这种用法。 一个基于 closure 的 Thread class 基本结构 12345678910111213141516171819202122232425262728293031323334#include &lt;boost/function.hpp&gt;#include &lt;boost/bind.hpp&gt;class Thread &#123;public: typedef boost::function&lt;void()&gt; ThreadCallback; Thread(ThreadCallback cb) : cb_(cb) &#123;&#125; void start() &#123; /* some magic to call run() in new created thread */ &#125;private: void run() &#123; cb_(); &#125; ThreadCallback cb_; // ...&#125;;class Foo &#123;public: void runInThread();&#125;;// 使用int main() &#123; Foo foo; Thread thread(boost::bind(&amp;Foo::runInThread, &amp;foo)); thread.start();&#125; 例2：网络库以 boost::function 作为桥梁，NetServer class 对其使用者没有任何类型上的限制，只对成员函数的参数和返回类型有限制。使用者 EchoService 也完全不知道 NetServer 的存在，只要在 main() 里把两者装配到一起，程序就跑起来了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// library#include &lt;boost/noncopyable.hpp&gt;#include &lt;boost/function.hpp&gt;#include &lt;boost/bind.hpp&gt;class Connection;class NetServer : boost::noncopyable &#123;public: typedef boost::function&lt;void(Connection *)&gt; ConnectionCallback; typedef boost::function&lt;void(Connection *, const void *, int len)&gt; MessageCallback; NetServer(uint16_t port); ~NetServer(); void run(); void registerConnectionCallback(const ConnectionCallback &amp;); void registerMessageCallback(const MessageCallback &amp;); void sendMessage(Connection *, const void *buf, int len);private: // ...&#125;;// userclass EchoService &#123;public: // 符合NetServer::sendMessage的原型 typedef boost::function&lt;void(Connection *, const void *, int)&gt; SendMessageCallback; EchoService(const SendMessageCallback &amp;sendMsgCb) : sendMessageCb_(sendMsgCb) &#123;&#125; // 符合NetServer::NetServer::MessageCallback的原型 void onMessage(Connection *conn, const void *buf, int size) &#123; printf(\"Received Msg from Connection %d: %.*s/n\", conn-&gt;id(), size, (const char *) buf); sendMessageCb_(conn, buf, size); // echo back &#125; // 符合NetServer::NetServer::ConnectionCallback的原型 void onConnection(Connection *conn) &#123; printf(\"Connection from %s:%d is %s/n\", conn-&gt;ipAddr(), conn-&gt;port(), conn-&gt;connected() ? \"UP\" : \"DOWN\"); &#125;private: SendMessageCallback sendMessageCb_;&#125;;// 扮演上帝的角色，把各部件拼起来int main() &#123; NetServer server(7); EchoService echo(bind(&amp;NetServer::sendMessage, &amp;server, _1, _2, _3)); server.registerMessageCallback( bind(&amp;EchoService::onMessage, &amp;echo, _1, _2, _3)); server.registerConnectionCallback( bind(&amp;EchoService::onConnection, &amp;echo, _1)); server.run();&#125; 3. 对面向对象程序设计的影响一直以来，我对面向对象有一种厌恶感，叠床架屋，绕来绕去的，一拳拳打在棉花上，不解决实际问题。面向对象三要素是封装、继承和多态。我认为封装是根本的，继承和多态则是可有可无。用 class 来表示 concept，这是根本的；至于继承和多态，其耦合性太强，往往不划算。 继承和多态不仅规定了函数的名称、参数、返回类型，还规定了类的继承关系。在现代的 OO 编程语言里，借助反射和 attribute/annotation，已经大大放宽了限制。举例来说，JUnit 3.x 是用反射，找出派生类里的名字符合 void test*() 的函数来执行，这里就没继承什么事，只是对函数的名称有部分限制（继承是全面限制，一字不差）。至于 JUnit 4.x 和 NUnit 2.x 则更进一步，以 annoatation/attribute 来标明 test case，更没继承什么事了。 我的猜测是，当初提出面向对象的时候，closure 还没有一个通用的实现，所以它没能算作基本的抽象工具之一。现在既然 closure 已经这么方便了，或许我们应该重新审视面向对象设计，至少不要那么滥用继承。 自从找到了 boost::function+boost::bind 这对神兵利器，不用再考虑类直接的继承关系，只需要基于对象的设计(object-based)，拳拳到肉，程序写起来顿时顺手了很多。 4. 对面向对象设计模式的影响既然虚函数能用 closure 代替，那么很多 OO 设计模式，尤其是行为模式，失去了存在的必要。另外，既然没有继承体系，那么创建型模式似乎也没啥用了。 最明显的是 Strategy，不用累赘的 Strategy 基类和ConcreteStrategyA、ConcreteStrategyB 等派生类，一个 boost::function&lt;&gt; 成员就解决问题。在《设计模式》这本书提到了23个模式，我认为 iterator 有用（或许再加个 State），其他都在摆谱，拉虚架子，没啥用。或许它们解决了面向对象中的常见问题，不过要是我的程序里连面向对象（指继承和多态）都不用，那似乎也不用叨扰面向对象设计模式了。 或许 closure-based programming 将作为一种新的 programming paradiam 而流行起来。 5. 依赖注入与单元测试前面的 EchoService 可算是依赖注入的例子，EchoService 需要一个什么东西来发送消息，它对这个“东西”的要求只是函数原型满足 SendMessageCallback，而并不关系数据到底发到网络上还是发到控制台。在正常使用的时候，数据应该发给网络，而在做单元测试的时候，数据应该发给某个 DataSink。 安照面向对象的思路，先写一个 AbstractDataSink interface，包含 sendMessage() 这个虚函数，然后派生出两个 classes：NetDataSink 和 MockDataSink，前面那个干活用，后面那个单元测试用。EchoService 的构造函数应该以 AbstractDataSink* 为参数，这样就实现了所谓的接口与实现分离。 我认为这么做纯粹是脱了裤子放屁，直接传入一个 SendMessageCallback 对象就能解决问题。在单元测试的时候，可以 boost::bind() 到 MockServer 上，或某个全局函数上，完全不用继承和虚函数，也不会影响现有的设计。 6. 什么时候使用继承？如果是指 OO 中的 public 继承，即为了接口与实现分离，那么我只会在派生类的数目和功能完全确定的情况下使用。换句话说，不为将来的扩展考虑，这时候面向对象或许是一种不错的描述方法。一旦要考虑扩展，什么办法都没用，还不如把程序写简单点，将来好大改或重写。 如果是功能继承，那么我会考虑继承 boost::noncopyable 或 boost::enable_shared_from_this，下一篇 blog 会讲到 enable_shared_from_this 在实现多线程安全的 Signal/Slot 时的妙用。 例如，IO-Multiplex 在不同的操作系统下有不同的推荐实现，最通用的 select()，POSIX 的 poll()，Linux 的 epoll()，FreeBSD 的 kqueue 等等，数目固定，功能也完全确定，不用考虑扩展。那么设计一个 NetLoop base class 加若干具体 classes 就是不错的解决办法。 7. 基于接口的设计这个问题来自那个经典的讨论：不会飞的企鹅（Penguin）究竟应不应该继承自鸟（Bird），如果 Bird 定义了 virtual function fly() 的话。讨论的结果是，把具体的行为提出来，作为 interface，比如 Flyable （能飞的），Runnable（能跑的），然后让企鹅实现 Runnable，麻雀实现 Flyable 和 Runnable。（其实麻雀只能双脚跳，不能跑，这里不作深究。） 进一步的讨论表明，interface 的粒度应足够小，或许包含一个 method 就够了，那么 interface 实际上退化成了给类型打的标签(tag)。在这种情况下，完全可以使用boost::function来代替，比如： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#include &lt;boost/function.hpp&gt;#include &lt;boost/bind.hpp&gt;// 企鹅能游泳，也能跑class Penguin &#123;public: void run(); void swim();&#125;;// 麻雀能飞，也能跑class Sparrow &#123;public: void fly(); void run();&#125;;// 以 closure 作为接口typedef boost::function&lt;void()&gt; FlyCallback;typedef boost::function&lt;void()&gt; RunCallback;typedef boost::function&lt;void()&gt; SwimCallback;// 一个既用到run，也用到fly的客户classclass Foo &#123;public: Foo(FlyCallback flyCb, RunCallback runCb) : flyCb_(flyCb), runCb_(runCb) &#123;&#125;private: FlyCallback flyCb_; RunCallback runCb_;&#125;;// 一个既用到run，也用到swim的客户classclass Bar &#123;public: Bar(SwimCallback swimCb, RunCallback runCb) : swimCb_(swimCb), runCb_(runCb) &#123;&#125;private: SwimCallback swimCb_; RunCallback runCb_;&#125;;int main() &#123; Sparrow s; Penguin p; // 装配起来，Foo要麻雀，Bar要企鹅。 Foo foo(boost::bind(&amp;Sparrow::fly, &amp;s), boost::bind(&amp;Sparrow::run, &amp;s)); Bar bar(boost::bind(&amp;Penguin::swim, &amp;p), boost::bind(&amp;Penguin::run, &amp;p)); return 0;&#125; 8. 实现Signal/Slotboost::function + boost::bind 描述了一对一的回调，在项目中，我们借助 boost::shared_ptr + boost::weak_ptr 简洁地实现了多播(multi-cast)，即一对多的回调，并且考虑了对象的生命期管理与多线程安全；并且，自然地，对使用者的类型不作任何限制，篇幅略长，留作下一篇blog吧。（boost::signals 也实现了 Signal/Slot，但可惜不是线程安全的。） 最后，向伟大的C语言致敬！","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"Boost","slug":"Boost","permalink":"http://yoursite.com/tags/Boost/"}]},{"title":"muduo 中的网络库","slug":"muduo中的网络库","date":"2020-03-09T02:52:15.035Z","updated":"2020-03-09T02:52:57.000Z","comments":true,"path":"开源组件/muduo/muduo中的网络库/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/muduo%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E5%BA%93/","excerpt":"","text":"1. Buffer 1. Buffer 部分源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113namespace muduo &#123; namespace net &#123; class Buffer : public muduo::copyable &#123; public: static const size_t kCheapPrepend = 8; static const size_t kInitialSize = 1024; explicit Buffer(size_t initialSize = kInitialSize) : buffer_(kCheapPrepend + initialSize), readerIndex_(kCheapPrepend), writerIndex_(kCheapPrepend) &#123; assert(readableBytes() == 0); assert(writableBytes() == initialSize); assert(prependableBytes() == kCheapPrepend); &#125; // implicit copy-ctor, move-ctor, dtor and assignment are fine // NOTE: implicit move-ctor is added in g++ 4.6 void swap(Buffer &amp;rhs) &#123; buffer_.swap(rhs.buffer_); std::swap(readerIndex_, rhs.readerIndex_); std::swap(writerIndex_, rhs.writerIndex_); &#125; ... // retrieve returns void, to prevent // string str(retrieve(readableBytes()), readableBytes()); // the evaluation of two functions are unspecified void retrieve(size_t len) &#123; assert(len &lt;= readableBytes()); if (len &lt; readableBytes()) &#123; readerIndex_ += len; &#125; else &#123; retrieveAll(); &#125; &#125; ... void append(const char * /*restrict*/ data, size_t len) &#123; ensureWritableBytes(len); std::copy(data, data + len, beginWrite()); hasWritten(len); &#125; /// /// Peek int64_t from network endian /// /// Require: buf-&gt;readableBytes() &gt;= sizeof(int64_t) int64_t peekInt64() const &#123; assert(readableBytes() &gt;= sizeof(int64_t)); int64_t be64 = 0; ::memcpy(&amp;be64, peek(), sizeof be64); return sockets::networkToHost64(be64); &#125; ... void prepend(const void * /*restrict*/ data, size_t len) &#123; assert(len &lt;= prependableBytes()); readerIndex_ -= len; const char *d = static_cast&lt;const char *&gt;(data); std::copy(d, d + len, begin() + readerIndex_); &#125; ... void shrink(size_t reserve) &#123; // FIXME: use vector::shrink_to_fit() in C++ 11 if possible. Buffer other; other.ensureWritableBytes(readableBytes() + reserve); other.append(toStringPiece()); swap(other); &#125; /// Read data directly into buffer. /// /// It may implement with readv(2) /// @return result of read(2), @c errno is saved ssize_t readFd(int fd, int *savedErrno); private: ... void makeSpace(size_t len) &#123; if (writableBytes() + prependableBytes() &lt; len + kCheapPrepend) &#123; // FIXME: move readable data buffer_.resize(writerIndex_ + len); &#125; else &#123; // move readable data to the front, make space inside buffer assert(kCheapPrepend &lt; readerIndex_); size_t readable = readableBytes(); std::copy(begin() + readerIndex_, begin() + writerIndex_, begin() + kCheapPrepend); readerIndex_ = kCheapPrepend; writerIndex_ = readerIndex_ + readable; assert(readable == readableBytes()); &#125; &#125; private: std::vector&lt;char&gt; buffer_; size_t readerIndex_; size_t writerIndex_; static const char kCRLF[]; &#125;; &#125; // namespace net&#125; // namespace muduo 提供的 public 方法 1234void swap(Buffer &amp;rhs);void retrieve(size_t len);void append(const char * /*restrict*/ data, size_t len);... 底层是一个 vector&lt;char&gt; 类型的 buffer_； 具体原理见 “./5_Buffer类的设计.md” ssize_t readFd(int fd, int *savedErrno) 方法 123456789101112131415161718192021222324252627ssize_t Buffer::readFd(int fd, int *savedErrno) &#123; // saved an ioctl()/FIONREAD call to tell how much to read char extrabuf[65536]; struct iovec vec[2]; const size_t writable = writableBytes(); vec[0].iov_base = begin() + writerIndex_; vec[0].iov_len = writable; vec[1].iov_base = extrabuf; vec[1].iov_len = sizeof extrabuf; // when there is enough space in this buffer, don't read into extrabuf. // when extrabuf is used, we read 128k-1 bytes at most. const int iovcnt = (writable &lt; sizeof extrabuf) ? 2 : 1; const ssize_t n = sockets::readv(fd, vec, iovcnt); if (n &lt; 0) &#123; *savedErrno = errno; &#125; else if (implicit_cast&lt;size_t&gt;(n) &lt;= writable) &#123; writerIndex_ += n; &#125; else &#123; writerIndex_ = buffer_.size(); append(extrabuf, n - writable); &#125; return n;&#125; 这个方法的原理是： 走内核的系统调用较为耗时，为了提高效率，最好一次读完 最终希望达到的目标是，数据从内核缓冲区全部进入 buffer 中的缓冲区(底层是一个 vector) 第一点和第二点有所矛盾，因此这个方法在栈上先分配一个 char extrabuf[65536];：如果 buffer 中的空间大小足够，就直接从内核拷贝到 buffer 中；如果不够，那就先读到栈上的 65536 字节的缓冲区，然后再 append 到 buffer 中，而 append 的过程是 buffer 类本身要考虑的事情","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"IO","slug":"IO","permalink":"http://yoursite.com/tags/IO/"}]},{"title":"Buffer 类的设计","slug":"Buffer类的设计","date":"2020-03-09T02:49:12.510Z","updated":"2020-03-09T02:49:47.000Z","comments":true,"path":"开源组件/muduo/Buffer类的设计/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/Buffer%E7%B1%BB%E7%9A%84%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"原文链接：https://blog.csdn.net/Solstice/article/details/6329080 1.Muduo 的 IO 模型 2.为什么 non-blocking 网络编程中应用层 buffer 是必须的？ TcpConnection 必须要有 output buffer TcpConnection 必须要有 input buffer 3. Buffer 的要求 Buffer::readFd() 线程安全？ 4. Muduo Buffer 的数据结构 5. Muduo Buffer 的操作 基本的 read-write cycle 自动增长 size() 与 capacity() 内部腾挪 prepend 6. 其他设计方案 不用 vector？ Zero copy ? 7. 性能是不是问题？看跟谁比 1.Muduo 的 IO 模型UNPv1 第 6.2 节总结了 Unix/Linux 上的五种 IO 模型：阻塞(blocking)、非阻塞(non-blocking)、IO 复用(IO multiplexing)、信号驱动(signal-driven)、异步(asynchronous)。这些都是单线程下的 IO 模型。 C10k 问题的页面介绍了五种 IO 策略，把线程也纳入考量。（现在 C10k 已经不是什么问题，C100k 也不是大问题，C1000k 才算得上挑战）。 在这个多核时代，线程是不可避免的。那么服务端网络编程该如何选择线程模型呢？我赞同 libev 作者的观点：one loop per thread is usually a good model。之前我也不止一次表述过这个观点，见《多线程服务器的常用编程模型》《多线程服务器的适用场合》。 如果采用 one loop per thread 的模型，多线程服务端编程的问题就简化为如何设计一个高效且易于使用的 event loop，然后每个线程 run 一个 event loop 就行了（当然、同步和互斥是不可或缺的）。在“高效”这方面已经有了很多成熟的范例（libev、libevent、memcached、varnish、lighttpd、nginx），在“易于使用”方面我希望 muduo 能有所作为。（muduo 可算是用现代 C++ 实现了 Reactor 模式，比起原始的 Reactor 来说要好用得多。） event loop 是 non-blocking 网络编程的核心，在现实生活中，non-blocking 几乎总是和 IO-multiplexing 一起使用，原因有两点： 没有人真的会用轮询 (busy-pooling) 来检查某个 non-blocking IO 操作是否完成，这样太浪费 CPU cycles。 IO-multiplex 一般不能和 blocking IO 用在一起，因为 blocking IO 中 read()/write()/accept()/connect() 都有可能阻塞当前线程，这样线程就没办法处理其他 socket 上的 IO 事件了。见 UNPv1 第 16.6 节“nonblocking accept”的例子。 所以，当我提到 non-blocking 的时候，实际上指的是 non-blocking + IO-muleiplexing，单用其中任何一个是不现实的。另外，本文所有的“连接”均指 TCP 连接，socket 和 connection 在文中可互换使用。 当然，non-blocking 编程比 blocking 难得多，见陈硕在《Muduo 网络编程示例之零：前言》中“TCP 网络编程本质论”一节列举的难点。基于 event loop 的网络编程跟直接用 C/C++ 编写单线程 Windows 程序颇为相像： 程序不能阻塞，否则窗口就失去响应了； 在 event handler 中，程序要尽快交出控制权，返回窗口的事件循环。 2.为什么 non-blocking 网络编程中应用层 buffer 是必须的？Non-blocking IO 的核心思想是避免阻塞在 read() 或 write() 或其他 IO 系统调用上，这样可以最大限度地复用 thread-of-control，让一个线程能服务于多个 socket 连接。IO 线程只能阻塞在 IO-multiplexing 函数上，如 select()/poll()/epoll_wait()。这样一来，应用层的缓冲是必须的，每个 TCP socket 都要有 stateful 的 input buffer 和 output buffer。 TcpConnection 必须要有 output buffer考虑一个常见场景：程序想通过 TCP 连接发送 100k 字节的数据，但是在 write() 调用中，操作系统只接受了 80k 字节（受 TCP advertised window 的控制，细节见 TCPv1），你肯定不想在原地等待，因为不知道会等多久（取决于对方什么时候接受数据，然后滑动 TCP 窗口）。程序应该尽快交出控制权，返回 event loop。在这种情况下，剩余的 20k 字节数据怎么办？ 对于应用程序而言，它只管生成数据，它不应该关心到底数据是一次性发送还是分成几次发送，这些应该由网络库来操心，程序只要调用 TcpConnection::send() 就行了，网络库会负责到底。网络库应该接管这剩余的 20k 字节数据，把它保存在该 TCP connection 的 output buffer 里，然后注册 POLLOUT 事件，一旦 socket 变得可写就立刻发送数据。当然，这第二次 write() 也不一定能完全写入 20k 字节，如果还有剩余，网络库应该继续关注 POLLOUT 事件；如果写完了 20k 字节，网络库应该停止关注 POLLOUT，以免造成 busy loop。（Muduo EventLoop 采用的是 epoll level trigger，这么做的具体原因我以后再说。） 如果程序又写入了 50k 字节，而这时候 output buffer 里还有待发送的 20k 数据，那么网络库不应该直接调用 write()，而应该把这 50k 数据 append 在那 20k 数据之后，等 socket 变得可写的时候再一并写入。 如果 output buffer 里还有待发送的数据，而程序又想关闭连接（对程序而言，调用 TcpConnection::send() 之后他就认为数据迟早会发出去），那么这时候网络库不能立刻关闭连接，而要等数据发送完毕，见我在《为什么 muduo 的 shutdown() 没有直接关闭 TCP 连接？》一文中的讲解。 综上，要让程序在 write 操作上不阻塞，网络库必须要给每个 tcp connection 配置 output buffer。 TcpConnection 必须要有 input bufferTCP 是一个无边界的字节流协议，接收方必须要处理“收到的数据尚不构成一条完整的消息”和“一次收到两条消息的数据”等等情况。一个常见的场景是，发送方 send 了两条 10k 字节的消息（共 20k），接收方收到数据的情况可能是： 一次性收到 20k 数据 分两次收到，第一次 5k，第二次 15k 分两次收到，第一次 15k，第二次 5k 分两次收到，第一次 10k，第二次 10k 分三次收到，第一次 6k，第二次 8k，第三次 6k 其他任何可能 网络库在处理“socket 可读”事件的时候，必须一次性把 socket 里的数据读完（从操作系统 buffer 搬到应用层 buffer），否则会反复触发 POLLIN 事件，造成 busy-loop。（Again, Muduo EventLoop 采用的是 epoll level trigger，这么做的具体原因我以后再说。） 那么网络库必然要应对“数据不完整”的情况，收到的数据先放到 input buffer 里，等构成一条完整的消息再通知程序的业务逻辑。这通常是 codec 的职责，见陈硕《Muduo 网络编程示例之二：Boost.Asio 的聊天服务器》一文中的“TCP 分包”的论述与代码。 所以，在 tcp 网络编程中，网络库必须要给每个 tcp connection 配置 input buffer。 所有 muduo 中的 IO 都是带缓冲的 IO (buffered IO)，你不会自己去 read() 或 write() 某个 socket，只会操作 TcpConnection 的 input buffer 和 output buffer。更确切的说，是在 onMessage() 回调里读取 input buffer；调用 TcpConnection::send() 来间接操作 output buffer，一般不会直接操作 output buffer。 btw, muduo 的 onMessage() 的原型如下，它既可以是 free function，也可以是 member function，反正 muduo TcpConnection 只认 boost::function&lt;&gt;。 1void onMessage(const TcpConnectionPtr&amp; conn, Buffer* buf, Timestamp receiveTime); 对于网络程序来说，一个简单的验收测试是：输入数据每次收到一个字节（200 字节的输入数据会分 200 次收到，每次间隔 10 ms），程序的功能不受影响。对于 Muduo 程序，通常可以用 codec 来分离“消息接收”与“消息处理”，见陈硕《在 muduo 中实现 protobuf 编解码器与消息分发器》一文中对“编解码器 codec”的介绍。 如果某个网络库只提供相当于 char buf[8192] 的缓冲，或者根本不提供缓冲区，而仅仅通知程序“某 socket 可读/某 socket 可写”，要程序自己操心 IO buffering，这样的网络库用起来就很不方便了。（我有所指，你懂得。） 3. Buffer 的要求Muduo Buffer 的设计考虑了常见的网络编程需求，我试图在易用性和性能之间找一个平衡点，目前这个平衡点更偏向于易用性。 Muduo Buffer 的设计要点： 对外表现为一块连续的内存(char*, len)，以方便客户代码的编写。 其 size() 可以自动增长，以适应不同大小的消息。它不是一个 fixed size array (即 char buf[8192])。 内部以 vector of char 来保存数据，并提供相应的访问函数。 Buffer 其实像是一个 queue，从末尾写入数据，从头部读出数据。 谁会用 Buffer？谁写谁读？根据前文分析，TcpConnection 会有两个 Buffer 成员，input buffer 与 output buffer。 input buffer，TcpConnection 会从 socket 读取数据，然后写入 input buffer（其实这一步是用 Buffer::readFd() 完成的）；客户代码从 input buffer 读取数据。 output buffer，客户代码会把数据写入 output buffer（其实这一步是用 TcpConnection::send() 完成的；TcpConnection 从 output buffer 读取数据并写入 socket。 其实，input 和 output 是针对客户代码而言，客户代码从 input 读，往 output 写。TcpConnection 的读写正好相反。 这里不介绍每个成员函数的作用，留给《Muduo 网络编程示例》系列。下文会仔细介绍 readIndex 和 writeIndex 的作用。 Buffer::readFd()我在《Muduo 网络编程示例之零：前言》中写道 在非阻塞网络编程中，如何设计并使用缓冲区？ 一方面我们希望减少系统调用，一次读的数据越多越划算，那么似乎应该准备一个大的缓冲区。 另一方面，我们系统减少内存占用。如果有 10k 个连接，每个连接一建立就分配 64k 的读缓冲的话，将占用 640M 内存，而大多数时候这些缓冲区的使用率很低。 muduo 用 readv 结合栈上空间巧妙地解决了这个问题。具体做法是，在栈上准备一个 65536 字节的 stackbuf，然后利用 readv() 来读取数据，iovec 有两块，第一块指向 muduo Buffer 中的 writable 字节，另一块指向栈上的 stackbuf。这样如果读入的数据不多，那么全部都读到 Buffer 中去了；如果长度超过 Buffer 的 writable 字节数，就会读到栈上的 stackbuf 里，然后程序再把 stackbuf 里的数据 append 到 Buffer 中。 这么做利用了临时栈上空间，避免开巨大 Buffer 造成的内存浪费，也避免反复调用 read() 的系统开销（通常一次 readv() 系统调用就能读完全部数据）。 这算是一个小小的创新吧。 线程安全？muduo::net::Buffer 不是线程安全的，这么做是有意的，原因如下： 对于 input buffer，onMessage() 回调始终发生在该 TcpConnection 所属的那个 IO 线程，应用程序应该在 onMessage() 完成对 input buffer 的操作，并且不要把 input buffer 暴露给其他线程。这样所有对 input buffer 的操作都在同一个线程，Buffer class 不必是线程安全的。对于 output buffer，应用程序不会直接操作它，而是调用 TcpConnection::send() 来发送数据，后者是线程安全的。如果 TcpConnection::send() 调用发生在该 TcpConnection 所属的那个 IO 线程，那么它会转而调用 TcpConnection::sendInLoop()，sendInLoop() 会在当前线程（也就是 IO 线程）操作 output buffer；如果 TcpConnection::send() 调用发生在别的线程，它不会在当前线程调用 sendInLoop() ，而是通过 EventLoop::runInLoop() 把 sendInLoop() 函数调用转移到 IO 线程（听上去颇为神奇？），这样 sendInLoop() 还是会在 IO 线程操作 output buffer，不会有线程安全问题。当然，跨线程的函数转移调用涉及函数参数的跨线程传递，一种简单的做法是把数据拷一份，绝对安全（不明白的同学请阅读代码）。 另一种更为高效做法是用 swap()。这就是为什么 TcpConnection::send() 的某个重载以 Buffer* 为参数，而不是 const Buffer&amp;，这样可以避免拷贝，而用 Buffer::swap() 实现高效的线程间数据转移。（最后这点，仅为设想，暂未实现。目前仍然以数据拷贝方式在线程间传递，略微有些性能损失。） 4. Muduo Buffer 的数据结构Buffer 的内部是一个 vector of char，它是一块连续的内存。此外，Buffer 有两个 data members (readerIndex_ 和 writerIndex_)，指向该 vector 中的元素。这两个 indices 的类型是 int，不是 char，目的是应对迭代器失效。muduo Buffer 的设计参考了 Netty 的 ChannelBuffer 和 libevent 1.4.x 的 evbuffer。*不过，其 prependable 可算是一点“微创新”**。 Muduo Buffer 的数据结构如下： 图 1 两个 indices 把 vector 的内容分为三块：prependable、readable、writable，各块的大小是（公式一）： prependable = readIndex readable = writeIndex - readIndex writable = size() - writeIndex 注：如前文所述，对于客户代码和 TcpConnection 来说，无论这个 buffer 用于 input 还是 output，客户代码和 TcpConnection 的方向永远是相反的： input buffer output buffer 客户代码 read write TcpConnection write read （prependable 的作用留到后面讨论。） readIndex 和 writeIndex 满足以下不变式(invariant): 0 ≤ readIndex ≤ writeIndex ≤ data.size() Muduo Buffer 里有两个常数 kCheapPrepend 和 kInitialSize，定义了 prependable 的初始大小和 writable 的初始大小。（readable 的初始大小为 0。）在初始化之后，Buffer 的数据结构如下：括号里的数字是该变量或常量的值。 图 2 根据以上（公式一）可算出各块的大小，刚刚初始化的 Buffer 里没有 payload 数据，所以 readable == 0。 5. Muduo Buffer 的操作基本的 read-write cycleBuffer 初始化后的情况见图 1，如果有人向 Buffer 写入了 200 字节，那么其布局是： 图 3 图 3 中 writeIndex 向后移动了 200 字节，readIndex 保持不变，readable 和 writable 的值也有变化。 如果有人从 Buffer read() &amp; retrieve() （下称“读入”）了 50 字节，结果见图 4。与上图相比，readIndex 向后移动 50 字节，writeIndex 保持不变，readable 和 writable 的值也有变化（这句话往后从略）。 图 4 然后又写入了 200 字节，writeIndex 向后移动了 200 字节，readIndex 保持不变，见图 5。 图 5 接下来，一次性读入 350 字节，请注意，由于全部数据读完了，readIndex 和 writeIndex 返回原位以备新一轮使用，见图 6，这和图 2 是一样的。 图 6 以上过程可以看作是发送方发送了两条消息，长度分别为 50 字节和 350 字节，接收方分两次收到数据，每次 200 字节，然后进行分包，再分两次回调客户代码。 自动增长Muduo Buffer 不是固定长度的，它可以自动增长，这是使用 vector 的直接好处。 假设当前的状态如图 7 所示。（这和前面图 5 是一样的。） 图 7 客户代码一次性写入 1000 字节，而当前可写的字节数只有 624，那么 buffer 会自动增长以容纳全部数据，得到的结果是图 8。注意 readIndex 返回到了前面，以保持 prependable 等于 kCheapPrependable。由于 vector 重新分配了内存，原来指向它元素的指针会失效，这就是为什么 readIndex 和 writeIndex 是整数下标而不是指针。 代码如下： 123456789101112131415161718192021...void makeSpace(size_t len) &#123; if (writableBytes() + prependableBytes() &lt; len + kCheapPrepend) &#123; // FIXME: move readable data buffer_.resize(writerIndex_ + len); &#125; else &#123; // move readable data to the front, make space inside buffer assert(kCheapPrepend &lt; readerIndex_); size_t readable = readableBytes(); std::copy(begin() + readerIndex_, begin() + writerIndex_, begin() + kCheapPrepend); readerIndex_ = kCheapPrepend; writerIndex_ = readerIndex_ + readable; assert(readable == readableBytes()); &#125;&#125;... 图 8 然后读入 350 字节，readIndex 前移，见图 9。 图 9 最后，读完剩下的 1000 字节，readIndex 和 writeIndex 返回 kCheapPrependable，见图 10。 图 10 注意 buffer 并没有缩小大小，下次写入 1350 字节就不会重新分配内存了。换句话说，Muduo Buffer 的 size() 是自适应的，它一开始的初始值是 1k，如果程序里边经常收发 10k 的数据，那么用几次之后它的 size() 会自动增长到 10k，然后就保持不变。这样一方面避免浪费内存（有的程序可能只需要 4k 的缓冲），另一方面避免反复分配内存。当然，客户代码可以手动 shrink() buffer size()。 size() 与 capacity()使用 vector 的另一个好处是它的 capcity() 机制减少了内存分配的次数。比方说程序反复写入 1 字节，muduo Buffer 不会每次都分配内存，vector 的 capacity() 以指数方式增长，让 push_back() 的平均复杂度是常数。比方说经过第一次增长，size() 刚好满足写入的需求，如图 11。但这个时候 vector 的 capacity() 已经大于 size()，在接下来写入 capacity()-size() 字节的数据时，都不会重新分配内存，见图 12。 图 11 图 12 细心的读者可能会发现用 capacity() 也不是完美的，它有优化的余地。具体来说，vector::resize() 会初始化(memset/bzero)内存，而我们不需要它初始化，因为反正立刻就要填入数据。比如，在图 12 的基础上写入 200 字节，由于 capacity() 足够大，不会重新分配内存，这是好事；但是 vector::resize() 会先把那 200 字节设为 0 （图 13），然后 muduo buffer 再填入数据（图 14）。这么做稍微有点浪费，不过我不打算优化它，除非它确实造成了性能瓶颈。（精通 STL 的读者可能会说用 vector::append() 以避免浪费，但是 writeIndex 和 size() 不一定是对齐的，会有别的麻烦。） 图 13 图 14 google protobuf 中有一个 STLStringResizeUninitialized 函数，干的就是这个事情。 内部腾挪有时候，经过若干次读写，readIndex 移到了比较靠后的位置，留下了巨大的 prependable 空间，见图 14。 图 14 这时候，如果我们想写入 300 字节，而 writable 只有 200 字节，怎么办？muduo Buffer 在这种情况下不会重新分配内存，而是先把已有的数据移到前面去，腾出 writable 空间，见图 15。 图 15 然后，就可以写入 300 字节了，见图 16。 图 16 这么做的原因是，如果重新分配内存，反正也是要把数据拷到新分配的内存区域，代价只会更大。 prepend前面说 muduo Buffer 有个小小的创新（或许不是创新，我记得在哪儿看到过类似的做法，忘了出处），即提供 prependable 空间，让程序能以很低的代价在数据前面添加几个字节。 比方说，程序以固定的4个字节表示消息的长度（即《Muduo 网络编程示例之二：Boost.Asio 的聊天服务器》中的 LengthHeaderCodec），我要序列化一个消息，但是不知道它有多长，那么我可以一直 append() 直到序列化完成（图 17，写入了 200 字节），然后再在序列化数据的前面添加消息的长度（图 18，把 200 这个数 prepend 到首部）。 图 17 图 18 通过预留 kCheapPrependable 空间，可以简化客户代码，一个简单的空间换时间思路。 6. 其他设计方案这里简单谈谈其他可能的应用层 buffer 设计方案。 不用 vector？如果有 STL 洁癖，那么可以自己管理内存，以 4 个指针为 buffer 的成员，数据结构见图 19。 图 19 说实话我不觉得这种方案比 vector 好。代码变复杂，性能也未见得有 noticeable 的改观。 如果放弃“连续性”要求，可以用 circular buffer，这样可以减少一点内存拷贝（没有“内部腾挪”）。 Zero copy ?如果对性能有极高的要求，受不了 copy() 与 resize()，那么可以考虑实现分段连续的 zero copy buffer 再配合 gather scatter IO，数据结构如图 20，这是 libevent 2.0.x 的设计方案。TCPv2介绍的 BSD TCP/IP 实现中的 mbuf 也是类似的方案，Linux 的 sk_buff 估计也差不多。细节有出入，但基本思路都是不要求数据在内存中连续，而是用链表把数据块链接到一起。 图 20 当然，高性能的代价是代码变得晦涩难读，buffer 不再是连续的，parse 消息会稍微麻烦。如果你的程序只处理 protobuf Message，这不是问题，因为 protobuf 有 ZeroCopyInputStream 接口，只要实现这个接口，parsing 的事情就交给 protobuf Message 去操心了。 7. 性能是不是问题？看跟谁比看到这里，有的读者可能会嘀咕，muduo Buffer 有那么多可以优化的地方，其性能会不会太低？对此，我的回应是“可以优化，不一定值得优化。” Muduo 的设计目标是用于开发公司内部的分布式程序。换句话说，它是用来写专用的 Sudoku server 或者游戏服务器，不是用来写通用的 httpd 或 ftpd 或 www proxy。前者通常有业务逻辑，后者更强调高并发与高吞吐。 以 Sudoku 为例，假设求解一个 Sudoku 问题需要 0.2ms，服务器有 8 个核，那么理想情况下每秒最多能求解 40,000 个问题。每次 Sudoku 请求的数据大小低于 100 字节（一个 9x9 的数独只要 81 字节，加上 header 也可以控制在 100 bytes 以下），就是说 100 x 40000 = 4 MB per second 的吞吐量就足以让服务器的 CPU 饱和。在这种情况下，去优化 Buffer 的内存拷贝次数似乎没有意义。 再举一个例子，目前最常用的千兆以太网的裸吞吐量是 125MB/s，扣除以太网 header、IP header、TCP header之后，应用层的吞吐率大约在 115 MB/s 上下。而现在服务器上最常用的 DDR2/DDR3 内存的带宽至少是 4GB/s，比千兆以太网高 40 倍以上。就是说，对于几 k 或几十 k 大小的数据，在内存里边拷几次根本不是问题，因为受以太网延迟和带宽的限制，跟这个程序通信的其他机器上的程序不会觉察到性能差异。 最后举一个例子，如果你实现的服务程序要跟数据库打交道，那么瓶颈常常在 DB 上，优化服务程序本身不见得能提高性能（从 DB 读一次数据往往就抵消了你做的全部 low-level 优化），这时不如把精力投入在 DB 调优上。 专用服务程序与通用服务程序的另外一点区别是 benchmark 的对象不同。如果你打算写一个 httpd，自然有人会拿来和目前最好的 nginx 对比，立马就能比出性能高低。然而，如果你写一个实现公司内部业务的服务程序（比如分布式存储或者搜索或者微博或者短网址），由于市面上没有同等功能的开源实现，你不需要在优化上投入全部精力，只要一版做得比一版好就行。先正确实现所需的功能，投入生产应用，然后再根据真实的负载情况来做优化，这恐怕比在编码阶段就盲目调优要更 effective 一些。 Muduo 的设计目标之一是吞吐量能让千兆以太网饱和，也就是每秒收发 120 兆字节的数据。这个很容易就达到，不用任何特别的努力。 如果确实在内存带宽方面遇到问题，说明你做的应用实在太 critical，或许应该考虑放到 Linux kernel 里边去，而不是在用户态尝试各种优化。毕竟只有把程序做到 kernel 里才能真正实现 zero copy，否则，核心态和用户态之间始终是有一次内存拷贝的。如果放到 kernel 里还不能满足需求，那么要么自己写新的 kernel，或者直接用 FPGA 或 ASIC 操作 network adapter 来实现你的高性能服务器。","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"}]},{"title":"muduo 中的 base 组件","slug":"muduo中的base组件","date":"2020-03-09T02:48:00.867Z","updated":"2020-03-09T02:48:37.000Z","comments":true,"path":"开源组件/muduo/muduo中的base组件/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/muduo%E4%B8%AD%E7%9A%84base%E7%BB%84%E4%BB%B6/","excerpt":"","text":"在 muduo/muduo/base 中，主要包括： 123456789101. Atomic2. BlockingQueue3. BoundedBlockingQueue4. Condition5. CountDownLatch6. CurrentThread7. Mutex8. Thread9. ThreadLocal10. Singleton 等。 还有几个工具类，包括： 123456781. Date2. Exception3. FileUtil4. Logging5. AsyncLogging6. GzipFile7. TimeZone8. TimeStamp 接下来依次看一下。 1.Atomic 2.Mutex 3.Singleton 4.Condition 5.CountDownLatch 6.Thread 7.ThreadLocal 8.ThreadPool 9.BlockingQueue 1.Atomic 部分源码 123456789101112131415161718192021222324252627282930313233343536namespace muduo &#123; namespace detail &#123; template&lt;typename T&gt; class AtomicIntegerT : noncopyable &#123; public: AtomicIntegerT() : value_(0) &#123; &#125; T get() &#123; // in gcc &gt;= 4.7: __atomic_load_n(&amp;value_, __ATOMIC_SEQ_CST) return __sync_val_compare_and_swap(&amp;value_, 0, 0); &#125; T getAndAdd(T x) &#123; // in gcc &gt;= 4.7: __atomic_fetch_add(&amp;value_, x, __ATOMIC_SEQ_CST) return __sync_fetch_and_add(&amp;value_, x); &#125; T getAndSet(T newValue) &#123; // in gcc &gt;= 4.7: __atomic_exchange_n(&amp;value, newValue, __ATOMIC_SEQ_CST) return __sync_lock_test_and_set(&amp;value_, newValue); &#125; ... private: volatile T value_; &#125; &#125; // namespace detail typedef detail::AtomicIntegerT&lt;int32_t&gt; AtomicInt32; typedef detail::AtomicIntegerT&lt;int64_t&gt; AtomicInt64;&#125; // namespace muduo 底层用一个 volatile T value_ 变量，C++ 中的 volatile 语义是确保读取和写入这个被修饰的变量时，都强制在内存中进行，防止出现编译器优化或从寄存器中取值，保证多线程的可见性。 Atomic 类的底层函数是 1231. T get()2. T getAndAdd(T x)3. T getAndSet(T newValue) 它们用到的是 gcc 提供的 CAS 操作系列函数 Atomic 类提供的全部方法： 123456789101. T get()2. T getAndAdd(T x)3. T getAndSet(T newValue)4. T addAndGet(T x)5. T incrementAndGet()6. T decrementAndGet()7. void add(T x)8. void increment()9. void decrement() &#123; 其中 4-9 使用了 1-3 作为底层方法 一个不错的习惯是提供了常见模板类的具体实例 AtomicInt32 和 AtomicInt64 2.Mutex 部分源码 123456789101112131415161718192021222324252627282930313233343536373839namespace muduo &#123; class CAPABILITY(\"mutex\") MutexLock : noncopyable &#123; public: MutexLock(): holder_(0) &#123; pthread_mutex_init(&amp;mutex_, NULL); &#125; ~MutexLock() &#123; assert(holder_ == 0); pthread_mutex_destroy(&amp;mutex_); &#125; // must be called when locked, i.e. for assertion bool isLockedByThisThread() const &#123; return holder_ == CurrentThread::tid(); &#125; void lock() ACQUIRE() &#123; pthread_mutex_lock(&amp;mutex_); assignHolder(); &#125; void unlock() RELEASE() &#123; unassignHolder(); pthread_mutex_unlock(&amp;mutex_); &#125; private: void unassignHolder() &#123; holder_ = 0; &#125; void assignHolder() &#123; holder_ = CurrentThread::tid(); &#125; pthread_mutex_t mutex_; pid_t holder_; &#125;; 底层是一个 pthread_mutex_t 的互斥锁，和一个 pid_t 类型的变量，用于标识持有锁的线程 底层用到的方法是四个 12341. pthread_mutex_init(&amp;mutex_, NULL);2. pthread_mutex_lock(&amp;mutex_);3. pthread_mutex_unlock(&amp;mutex_);4. pthread_mutex_destroy(&amp;mutex_); 要注意的是 lock 和 unlock 中的顺序 真正使用时，为了防止忘记 unlock 的情况发生，使用一个包装类 MutexLockGuard 进行控制：这个类的在构造函数中加锁，在析构函数中解锁。 12345678910111213class MutexLockGuard : noncopyable &#123;public: explicit MutexLockGuard(MutexLock &amp;mutex) ACQUIRE(mutex) : mutex_(mutex) &#123; mutex_.lock(); &#125; ~MutexLockGuard() RELEASE() &#123; mutex_.unlock(); &#125;private: MutexLock &amp;mutex_;&#125;; 3.Singleton 部分源码 12345678910111213141516171819202122232425262728293031323334353637383940414243namespace muduo &#123; template&lt;typename T&gt; class Singleton : noncopyable &#123; public: Singleton() = delete; ~Singleton() = delete; static T &amp;instance() &#123; pthread_once(&amp;ponce_, &amp;Singleton::init); assert(value_ != NULL); return *value_; &#125; private: static void init() &#123; value_ = new T(); if (!detail::has_no_destroy&lt;T&gt;::value) &#123; ::atexit(destroy); &#125; &#125; static void destroy() &#123; typedef char T_must_be_complete_type[sizeof(T) == 0 ? -1 : 1]; T_must_be_complete_type dummy; (void) dummy; delete value_; value_ = NULL; &#125; private: static pthread_once_t ponce_; static T *value_; &#125;; template&lt;typename T&gt; pthread_once_t Singleton&lt;T&gt;::ponce_ = PTHREAD_ONCE_INIT; template&lt;typename T&gt; T *Singleton&lt;T&gt;::value_ = NULL;&#125; 底层使用了 pthread_once_t 作为控制多线程下单例类安全的措施 将单例类的构造函数和析构函数全部禁用掉 123public: Singleton() = delete; ~Singleton() = delete; 4.Condition 部分源码 123456789101112131415161718192021222324252627282930313233namespace muduo &#123; class Condition : noncopyable &#123; public: explicit Condition(MutexLock &amp;mutex): mutex_(mutex) &#123; pthread_cond_init(&amp;pcond_, NULL); &#125; ~Condition() &#123; pthread_cond_destroy(&amp;pcond_); &#125; void wait() &#123; MutexLock::UnassignGuard ug(mutex_); pthread_cond_wait(&amp;pcond_, mutex_.getPthreadMutex()); &#125; // returns true if time out, false otherwise. bool waitForSeconds(double seconds); void notify() &#123; pthread_cond_signal(&amp;pcond_); &#125; void notifyAll() &#123; pthread_cond_broadcast(&amp;pcond_); &#125; private: MutexLock &amp;mutex_; pthread_cond_t pcond_; &#125;;&#125; // namespace muduo 这个类的底层对 pthread_cond_t 对象进行了封装，底层方法是 123451. pthread_cond_init(&amp;pcond_, NULL);2. pthread_cond_wait(&amp;pcond_, mutex_.getPthreadMutex());3. pthread_cond_signal(&amp;pcond_);4. pthread_cond_broadcast(&amp;pcond_);5. pthread_cond_destroy(&amp;pcond_); 条件变量要和互斥锁关联，所以在构造函数中强制绑定一个 Mutex；在 wait 方法前要先持有锁，结束后要释放锁，因此使用 Mutex 的包装类 MutexLockGuard 自动控制锁的持有和释放 5.CountDownLatch 部分源码 CountDownLatch.h 1234567891011121314151617181920namespace muduo &#123; class CountDownLatch : noncopyable &#123; public: explicit CountDownLatch(int count); void wait(); void countDown(); int getCount() const; private: mutable MutexLock mutex_; Condition condition_; int count_; &#125;;&#125; // namespace muduo CountDownLatch.cc 12345678910111213141516171819202122232425CountDownLatch::CountDownLatch(int count) : mutex_(), condition_(mutex_), count_(count) &#123;&#125;void CountDownLatch::wait() &#123; MutexLockGuard lock(mutex_); while (count_ &gt; 0) &#123; condition_.wait(); &#125;&#125;void CountDownLatch::countDown() &#123; MutexLockGuard lock(mutex_); --count_; if (count_ == 0) &#123; condition_.notifyAll(); &#125;&#125;int CountDownLatch::getCount() const &#123; MutexLockGuard lock(mutex_); return count_;&#125; 原理挺简单的，使用 mutex 和 condition 配合实现逻辑 6.Thread 部分源码 Thread.h 1234567891011121314151617181920212223242526272829303132333435363738394041namespace muduo &#123; class Thread : noncopyable &#123; public: typedef std::function&lt;void()&gt; ThreadFunc; explicit Thread(ThreadFunc, const string &amp;name = string()); // FIXME: make it movable in C++11 ~Thread(); void start(); int join(); // return pthread_join() bool started() const &#123; return started_; &#125; pthread_t pthreadId() const &#123; return pthreadId_; &#125; pid_t tid() const &#123; return tid_; &#125; const string&amp; name() const &#123; return name_; &#125; static int numCreated() &#123; return numCreated_.get(); &#125; private: void setDefaultName(); bool started_; bool joined_; pthread_t pthreadId_; pid_t tid_; ThreadFunc func_; string name_; CountDownLatch latch_; static AtomicInt32 numCreated_; &#125;;&#125; // namespace muduo Thread.cc 12345678910111213141516171819202122232425262728293031323334353637383940414243444546namespace muduo &#123; Thread::Thread(ThreadFunc func, const string &amp;n) : started_(false), joined_(false), pthreadId_(0), tid_(0), func_(std::move(func)), name_(n), latch_(1) &#123; setDefaultName(); &#125; Thread::~Thread() &#123; if (started_ &amp;&amp; !joined_) &#123; pthread_detach(pthreadId_); &#125; &#125; void Thread::start() &#123; assert(!started_); started_ = true; detail::ThreadData* data = new detail::ThreadData(func_, name_, &amp;tid_, &amp;latch_); if (pthread_create(&amp;pthreadId_, NULL, &amp;detail::startThread, data)) &#123; started_ = false; delete data; // or no delete? LOG_SYSFATAL &lt;&lt; \"Failed in pthread_create\"; &#125; else &#123; latch_.wait(); assert(tid_ &gt; 0); &#125; &#125; int Thread::join() &#123; assert(started_); assert(!joined_); joined_ = true; return pthread_join(pthreadId_, NULL); &#125; ...&#125; 析构函数12345Thread::~Thread() &#123; if (started_ &amp;&amp; !joined_) &#123; pthread_detach(pthreadId_); &#125;&#125; 如果当前这个线程 start 但是未 join，那么就 pthread_detach，线程结束后自动释放存储器资源（例如栈） start 函数 在 muduo::detail 命名空间中定义了 ThreadData 结构体 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849namespace muduo &#123; namespace detail &#123; struct ThreadData &#123; typedef muduo::Thread::ThreadFunc ThreadFunc; ThreadFunc func_; string name_; pid_t* tid_; CountDownLatch *latch_; ThreadData(ThreadFunc func, const string &amp;name, pid_t *tid, CountDownLatch *latch) : func_(std::move(func)), name_(name), tid_(tid), latch_(latch) &#123;&#125; void runInThread() &#123; *tid_ = muduo::CurrentThread::tid(); tid_ = NULL; latch_-&gt;countDown(); latch_ = NULL; muduo::CurrentThread::t_threadName = name_.empty() ? \"muduoThread\" : name_.c_str(); ::prctl(PR_SET_NAME, muduo::CurrentThread::t_threadName); try &#123; func_(); muduo::CurrentThread::t_threadName = \"finished\"; &#125; catch (const Exception &amp;ex) &#123; muduo::CurrentThread::t_threadName = \"crashed\"; fprintf(stderr, \"exception caught in Thread %s\\n\", name_.c_str()); fprintf(stderr, \"reason: %s\\n\", ex.what()); fprintf(stderr, \"stack trace: %s\\n\", ex.stackTrace()); abort(); &#125; catch (const std::exception &amp;ex) &#123; muduo::CurrentThread::t_threadName = \"crashed\"; fprintf(stderr, \"exception caught in Thread %s\\n\", name_.c_str()); fprintf(stderr, \"reason: %s\\n\", ex.what()); abort(); &#125; catch (...) &#123; muduo::CurrentThread::t_threadName = \"crashed\"; fprintf(stderr, \"unknown exception caught in Thread %s\\n\", name_.c_str()); throw; // rethrow &#125; &#125; &#125;; &#125;&#125; 整体思路是将参数先传进 ThreadData 结构体，然后用 pthread_create 开新线程，新线程的入口函数是 detail::startThread： 12345678910namespace muduo &#123; namespace detail &#123; void *startThread(void *obj) &#123; ThreadData *data = static_cast&lt;ThreadData *&gt;(obj); data-&gt;runInThread(); delete data; return NULL; &#125; &#125;&#125; 在这个入口函数中，会执行 data-&gt;runInThread(); 在 runInThread() 方法中，会将 countDownLatch count down，从而结束 start Thread 的线程。 join 函数 对 pthread_join 进行封装 使用示例 Thread_test.cc 123456789int main() &#123; printf(\"pid=%d, tid=%d\\n\", ::getpid(), muduo::CurrentThread::tid()); muduo::Thread t1(threadFunc); t1.start(); printf(\"t1.tid=%d\\n\", t1.tid()); t1.join(); } 12345678910111213141516171819202122232425262728293031323334353637383940414243### 7.ThreadLocal0. 部分源码&#96;&#96;&#96;cppnamespace muduo &#123; template&lt;typename T&gt; class ThreadLocal : noncopyable &#123; public: ThreadLocal() &#123; pthread_key_create(&amp;pkey_, &amp;ThreadLocal::destructor); &#125; ~ThreadLocal() &#123; pthread_key_delete(pkey_); &#125; T &amp;value() &#123; T *perThreadValue &#x3D; static_cast&lt;T *&gt;(pthread_getspecific(pkey_)); if (!perThreadValue) &#123; T *newObj &#x3D; new T(); pthread_setspecific(pkey_, newObj); perThreadValue &#x3D; newObj; &#125; return *perThreadValue; &#125; private: static void destructor(void *x) &#123; T *obj &#x3D; static_cast&lt;T *&gt;(x); typedef char T_must_be_complete_type[sizeof(T) &#x3D;&#x3D; 0 ? -1 : 1]; T_must_be_complete_type dummy; (void) dummy; delete obj; &#125; private: pthread_key_t pkey_; &#125;;&#125; &#x2F;&#x2F; namespace muduo 利用 pthread_key_create 创建线程私有数据，在析构时调用 pthread_key_delete 销毁；使用 pthread_getspecific 获取线程私有数据 8.ThreadPool 部分源码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950namespace muduo &#123; class ThreadPool : noncopyable &#123; public: typedef std::function&lt;void()&gt; Task; explicit ThreadPool(const string &amp;nameArg = string(\"ThreadPool\")); ~ThreadPool(); // Must be called before start(). void setMaxQueueSize(int maxSize) &#123; maxQueueSize_ = maxSize; &#125; void setThreadInitCallback(const Task &amp;cb) &#123; threadInitCallback_ = cb; &#125; void start(int numThreads); void stop(); const string &amp;name() const &#123; return name_; &#125; size_t queueSize() const; // Could block if maxQueueSize &gt; 0 // There is no move-only version of std::function in C++ as of C++14. // So we don't need to overload a const&amp; and an &amp;&amp; versions // as we do in (Bounded)BlockingQueue. // https://stackoverflow.com/a/25408989 void run(Task f); private: bool isFull() const; void runInThread(); Task take(); mutable MutexLock mutex_; Condition notEmpty_; Condition notFull_; string name_; Task threadInitCallback_; std::vector&lt;std::unique_ptr&lt;muduo::Thread&gt;&gt; threads_; std::deque &lt;Task&gt; queue_; size_t maxQueueSize_; bool running_; &#125;;&#125; // namespace muduo 构造函数 12345678ThreadPool::ThreadPool(const string &amp;nameArg) : mutex_(), notEmpty_(mutex_), notFull_(mutex_), name_(nameArg), maxQueueSize_(0), running_(false) &#123;&#125; 析构函数 1234567891011121314151617ThreadPool::~ThreadPool() &#123; if (running_) &#123; stop(); &#125;&#125;void ThreadPool::stop() &#123; &#123; MutexLockGuard lock(mutex_); running_ = false; notEmpty_.notifyAll(); &#125; for (auto &amp;thr : threads_) &#123; thr-&gt;join(); &#125;&#125; 思路是在析构 ThreadPool 的时候，将每个未执行完的线程 join 一并执行掉，直到全部结束后再退出析构函数 start 方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152void ThreadPool::start(int numThreads) &#123; assert(threads_.empty()); running_ = true; threads_.reserve(numThreads); for (int i = 0; i &lt; numThreads; ++i) &#123; char id[32]; snprintf(id, sizeof id, \"%d\", i + 1); threads_.emplace_back(new muduo::Thread( std::bind(&amp;ThreadPool::runInThread, this), name_ + id)); threads_[i]-&gt;start(); &#125; if (numThreads == 0 &amp;&amp; threadInitCallback_) &#123; threadInitCallback_(); &#125;&#125;void ThreadPool::runInThread() &#123; try &#123; if (threadInitCallback_) &#123; threadInitCallback_(); &#125; while (running_) &#123; Task task(take()); if (task) &#123; task(); &#125; &#125; &#125; catch (const Exception &amp;ex) &#123; ... &#125; ...&#125;ThreadPool::Task ThreadPool::take() &#123; MutexLockGuard lock(mutex_); // always use a while-loop, due to spurious wakeup while (queue_.empty() &amp;&amp; running_) &#123; notEmpty_.wait(); &#125; Task task; if (!queue_.empty()) &#123; task = queue_.front(); queue_.pop_front(); if (maxQueueSize_ &gt; 0) &#123; notFull_.notify(); &#125; &#125; return task;&#125; start 以后，会开 numThreads 个线程开始跑，每个线程都会从队列中取任务 take，然后执行之。 run 方法 1234567891011121314void ThreadPool::run(Task task) &#123; if (threads_.empty()) &#123; task(); &#125; else &#123; MutexLockGuard lock(mutex_); while (isFull()) &#123; notFull_.wait(); &#125; assert(!isFull()); queue_.push_back(std::move(task)); notEmpty_.notify(); &#125;&#125; 这个方法的目标是向线程池中添加任务，本质上是向队列 queue_ 中添加任务 9.BlockingQueue 部分源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748namespace muduo &#123; template&lt;typename T&gt; class BlockingQueue : noncopyable &#123; public: BlockingQueue() : mutex_(), notEmpty_(mutex_), queue_() &#123; &#125; void put(const T &amp;x) &#123; MutexLockGuard lock(mutex_); queue_.push_back(x); notEmpty_.notify(); // wait morphing saves us // http://www.domaigne.com/blog/computing/condvars-signal-with-mutex-locked-or-not/ &#125; void put(T &amp;&amp;x) &#123; MutexLockGuard lock(mutex_); queue_.push_back(std::move(x)); notEmpty_.notify(); &#125; T take() &#123; MutexLockGuard lock(mutex_); // always use a while-loop, due to spurious wakeup while (queue_.empty()) &#123; notEmpty_.wait(); &#125; assert(!queue_.empty()); T front(std::move(queue_.front())); queue_.pop_front(); return front; &#125; size_t size() const &#123; MutexLockGuard lock(mutex_); return queue_.size(); &#125; private: mutable MutexLock mutex_; Condition notEmpty_; std::deque &lt;T&gt; queue_; &#125;;&#125; // namespace muduo 底层用的是 deque，原理挺简单的","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"}]},{"title":"muduo 项目中的 CMakeLists","slug":"muduo项目中的CMakeLists","date":"2020-03-08T08:30:22.299Z","updated":"2020-03-08T08:30:23.000Z","comments":true,"path":"开源组件/muduo/muduo项目中的CMakeLists/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/muduo%E9%A1%B9%E7%9B%AE%E4%B8%AD%E7%9A%84CMakeLists/","excerpt":"","text":"目录 base net test1 muduo base位置：muduo/muduo/base/CMakeLists.txt 这个目录下有一堆.cc文件和.h文件 1234567AsyncLogging.hAsyncLogging.ccAtomic.hBlockingQueue.hCountDownLatch.hCountDownLatch.cc... CMakeLists.txt 1234567891011121314151617181920212223242526272829303132333435# 所有的源文件 set 进 base_SRCS 这个变量set(base_SRCS AsyncLogging.cc Condition.cc CountDownLatch.cc CurrentThread.cc Date.cc Exception.cc FileUtil.cc LogFile.cc Logging.cc LogStream.cc ProcessInfo.cc Timestamp.cc Thread.cc ThreadPool.cc TimeZone.cc )# 将指定的源文件生成链接文件add_library(muduo_base $&#123;base_SRCS&#125;)# 将目标文件与库文件进行链接target_link_libraries(muduo_base pthread rt)COMPILE_FLAGS &quot;-std&#x3D;c++0x&quot;)install(TARGETS muduo_base DESTINATION lib)file(GLOB HEADERS &quot;*.h&quot;)install(FILES $&#123;HEADERS&#125; DESTINATION include&#x2F;muduo&#x2F;base)if(MUDUO_BUILD_EXAMPLES) add_subdirectory(tests)endif() net位置：muduo/muduo/net/CMakeLists.txt 当前目录下是一堆头文件和源文件 12345Acceptor.hAcceptor.ccBuffer.hBuffer.cc... CMakeLists.txt 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465include(CheckFunctionExists)check_function_exists(accept4 HAVE_ACCEPT4)if(NOT HAVE_ACCEPT4) set_source_files_properties(SocketsOps.cc PROPERTIES COMPILE_FLAGS &quot;-DNO_ACCEPT4&quot;)endif()set(net_SRCS Acceptor.cc Buffer.cc Channel.cc Connector.cc EventLoop.cc EventLoopThread.cc EventLoopThreadPool.cc InetAddress.cc Poller.cc poller&#x2F;DefaultPoller.cc poller&#x2F;EPollPoller.cc poller&#x2F;PollPoller.cc Socket.cc SocketsOps.cc TcpClient.cc TcpConnection.cc TcpServer.cc Timer.cc TimerQueue.cc )add_library(muduo_net $&#123;net_SRCS&#125;)target_link_libraries(muduo_net muduo_base)install(TARGETS muduo_net DESTINATION lib)set(HEADERS Buffer.h Callbacks.h Channel.h Endian.h EventLoop.h EventLoopThread.h EventLoopThreadPool.h InetAddress.h TcpClient.h TcpConnection.h TcpServer.h TimerId.h )install(FILES $&#123;HEADERS&#125; DESTINATION include&#x2F;muduo&#x2F;net)# 添加子目录 http 到 build 中. http 子目录中同样有头文件、源文件和 CMakeLists.txtadd_subdirectory(http)add_subdirectory(inspect)if(MUDUO_BUILD_EXAMPLES) add_subdirectory(tests)endif()if(PROTOBUF_FOUND) add_subdirectory(protobuf) add_subdirectory(protorpc)else() add_subdirectory(protobuf EXCLUDE_FROM_ALL) add_subdirectory(protorpc EXCLUDE_FROM_ALL)endif() test1位置：muduo/muduo/base/tests/CMakeLists.txt 这个目录下是各种 test 文件，包含了 123AsyncLogging_test.ccAtomic_unittest.cc... 这些 test 文件中包含了 main 方法，用多个 add_executable 编译成多个可执行文件 CMakeLists.txt 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586add_executable(asynclogging_test AsyncLogging_test.cc)target_link_libraries(asynclogging_test muduo_base)add_executable(atomic_unittest Atomic_unittest.cc)add_test(NAME atomic_unittest COMMAND atomic_unittest)# Add a test to the project to be run by __ctest__ commandadd_executable(blockingqueue_test BlockingQueue_test.cc)target_link_libraries(blockingqueue_test muduo_base)add_executable(blockingqueue_bench BlockingQueue_bench.cc)target_link_libraries(blockingqueue_bench muduo_base)add_executable(boundedblockingqueue_test BoundedBlockingQueue_test.cc)target_link_libraries(boundedblockingqueue_test muduo_base)add_executable(date_unittest Date_unittest.cc)target_link_libraries(date_unittest muduo_base)add_test(NAME date_unittest COMMAND date_unittest)add_executable(exception_test Exception_test.cc)target_link_libraries(exception_test muduo_base)add_test(NAME exception_test COMMAND exception_test)add_executable(fileutil_test FileUtil_test.cc)target_link_libraries(fileutil_test muduo_base)add_test(NAME fileutil_test COMMAND fileutil_test)add_executable(fork_test Fork_test.cc)target_link_libraries(fork_test muduo_base)if(ZLIB_FOUND) add_executable(gzipfile_test GzipFile_test.cc) target_link_libraries(gzipfile_test muduo_base z) add_test(NAME gzipfile_test COMMAND gzipfile_test)endif()add_executable(logfile_test LogFile_test.cc)target_link_libraries(logfile_test muduo_base)add_executable(logging_test Logging_test.cc)target_link_libraries(logging_test muduo_base)add_executable(logstream_bench LogStream_bench.cc)target_link_libraries(logstream_bench muduo_base)if(BOOSTTEST_LIBRARY)add_executable(logstream_test LogStream_test.cc)target_link_libraries(logstream_test muduo_base boost_unit_test_framework)add_test(NAME logstream_test COMMAND logstream_test)endif()add_executable(mutex_test Mutex_test.cc)target_link_libraries(mutex_test muduo_base)add_executable(processinfo_test ProcessInfo_test.cc)target_link_libraries(processinfo_test muduo_base)add_executable(singleton_test Singleton_test.cc)target_link_libraries(singleton_test muduo_base)add_executable(singleton_threadlocal_test SingletonThreadLocal_test.cc)target_link_libraries(singleton_threadlocal_test muduo_base)add_executable(thread_bench Thread_bench.cc)target_link_libraries(thread_bench muduo_base)add_executable(thread_test Thread_test.cc)target_link_libraries(thread_test muduo_base)add_executable(threadlocal_test ThreadLocal_test.cc)target_link_libraries(threadlocal_test muduo_base)add_executable(threadlocalsingleton_test ThreadLocalSingleton_test.cc)target_link_libraries(threadlocalsingleton_test muduo_base)add_executable(threadpool_test ThreadPool_test.cc)target_link_libraries(threadpool_test muduo_base)add_executable(timestamp_unittest Timestamp_unittest.cc)target_link_libraries(timestamp_unittest muduo_base)add_test(NAME timestamp_unittest COMMAND timestamp_unittest)add_executable(timezone_unittest TimeZone_unittest.cc)target_link_libraries(timezone_unittest muduo_base)add_test(NAME timezone_unittest COMMAND timezone_unittest) muduo位置：muduo/CMakeLists.txt 这个目录下有： 1234add_subdirectory(muduo&#x2F;base)add_subdirectory(muduo&#x2F;net)add_subdirectory(contrib)add_subdirectory(examples) CMakeLists.txt 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116cmake_minimum_required(VERSION 2.6)project(muduo C CXX)enable_testing()if(NOT CMAKE_BUILD_TYPE) set(CMAKE_BUILD_TYPE &quot;Release&quot;)endif()# only build examples if this is the main projectif(CMAKE_PROJECT_NAME STREQUAL &quot;muduo&quot;) option(MUDUO_BUILD_EXAMPLES &quot;Build Muduo examples&quot; ON)endif()set(CXX_FLAGS -g # -DVALGRIND -DCHECK_PTHREAD_RETURN_VALUE -D_FILE_OFFSET_BITS&#x3D;64 -Wall -Wextra -Werror -Wconversion -Wno-unused-parameter -Wold-style-cast -Woverloaded-virtual -Wpointer-arith -Wshadow -Wwrite-strings -march&#x3D;native # -MMD -std&#x3D;c++11 -rdynamic )if(CMAKE_BUILD_BITS EQUAL 32) list(APPEND CXX_FLAGS &quot;-m32&quot;)endif()if(CMAKE_CXX_COMPILER_ID STREQUAL &quot;Clang&quot;) list(APPEND CXX_FLAGS &quot;-Wno-null-dereference&quot;) list(APPEND CXX_FLAGS &quot;-Wno-sign-conversion&quot;) list(APPEND CXX_FLAGS &quot;-Wno-unused-local-typedef&quot;) list(APPEND CXX_FLAGS &quot;-Wthread-safety&quot;) list(REMOVE_ITEM CXX_FLAGS &quot;-rdynamic&quot;)endif()string(REPLACE &quot;;&quot; &quot; &quot; CMAKE_CXX_FLAGS &quot;$&#123;CXX_FLAGS&#125;&quot;)set(CMAKE_CXX_FLAGS_DEBUG &quot;-O0&quot;)set(CMAKE_CXX_FLAGS_RELEASE &quot;-O2 -DNDEBUG&quot;)set(EXECUTABLE_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;&#x2F;bin)set(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;&#x2F;lib)find_package(Boost REQUIRED)find_package(Protobuf)find_package(CURL)find_package(ZLIB)find_path(CARES_INCLUDE_DIR ares.h)find_library(CARES_LIBRARY NAMES cares)find_path(MHD_INCLUDE_DIR microhttpd.h)find_library(MHD_LIBRARY NAMES microhttpd)find_library(BOOSTTEST_LIBRARY NAMES boost_unit_test_framework)find_library(BOOSTPO_LIBRARY NAMES boost_program_options)find_library(BOOSTSYSTEM_LIBRARY NAMES boost_system)find_path(TCMALLOC_INCLUDE_DIR gperftools&#x2F;heap-profiler.h)find_library(TCMALLOC_LIBRARY NAMES tcmalloc_and_profiler)find_path(HIREDIS_INCLUDE_DIR hiredis&#x2F;hiredis.h)find_library(HIREDIS_LIBRARY NAMES hiredis)find_path(GD_INCLUDE_DIR gd.h)find_library(GD_LIBRARY NAMES gd)find_program(THRIFT_COMPILER thrift)find_path(THRIFT_INCLUDE_DIR thrift)find_library(THRIFT_LIBRARY NAMES thrift)if(CARES_INCLUDE_DIR AND CARES_LIBRARY) message(STATUS &quot;found cares&quot;)endif()if(CURL_FOUND) message(STATUS &quot;found curl&quot;)endif()if(PROTOBUF_FOUND) message(STATUS &quot;found protobuf&quot;)endif()if(TCMALLOC_INCLUDE_DIR AND TCMALLOC_LIBRARY) message(STATUS &quot;found tcmalloc&quot;)endif()if(ZLIB_FOUND) message(STATUS &quot;found zlib&quot;)endif()if(HIREDIS_INCLUDE_DIR AND HIREDIS_LIBRARY) message(STATUS &quot;found hiredis&quot;)endif()if(GD_INCLUDE_DIR AND GD_LIBRARY) message(STATUS &quot;found gd&quot;)endif()if(THRIFT_COMPILER AND THRIFT_INCLUDE_DIR AND THRIFT_LIBRARY) message(STATUS &quot;found thrift&quot;)endif()include_directories($&#123;Boost_INCLUDE_DIRS&#125;)include_directories($&#123;PROJECT_SOURCE_DIR&#125;)string(TOUPPER $&#123;CMAKE_BUILD_TYPE&#125; BUILD_TYPE)message(STATUS &quot;CXX_FLAGS &#x3D; &quot; $&#123;CMAKE_CXX_FLAGS&#125; &quot; &quot; $&#123;CMAKE_CXX_FLAGS_$&#123;BUILD_TYPE&#125;&#125;)add_subdirectory(muduo&#x2F;base)add_subdirectory(muduo&#x2F;net)if(MUDUO_BUILD_EXAMPLES) add_subdirectory(contrib) add_subdirectory(examples)else() if(CARES_INCLUDE_DIR AND CARES_LIBRARY) add_subdirectory(examples&#x2F;cdns) endif()endif()","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"CMake","slug":"CMake","permalink":"http://yoursite.com/tags/CMake/"}]},{"title":"发布一个基于 Reactor 模式的 C++ 网络库","slug":"发布一个基于 Reactor 模式的 C++ 网络库","date":"2020-03-08T08:21:03.674Z","updated":"2020-03-08T08:22:59.000Z","comments":true,"path":"开源组件/muduo/发布一个基于 Reactor 模式的 C++ 网络库/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/%E5%8F%91%E5%B8%83%E4%B8%80%E4%B8%AA%E5%9F%BA%E4%BA%8E%20Reactor%20%E6%A8%A1%E5%BC%8F%E7%9A%84%20C++%20%E7%BD%91%E7%BB%9C%E5%BA%93/","excerpt":"","text":"1来源 https:&#x2F;&#x2F;blog.csdn.net&#x2F;Solstice&#x2F;article&#x2F;details&#x2F;5848547 123陈硕 (giantchen_AT_gmail)Blog.csdn.net&#x2F;Solstice2010 Aug 30 本文主要介绍 muduo 网络库的使用。其设计与实现将有另文讲解。 目录 由来 下载与编译 例子 基本结构 公开接口 内部实现 线程模型 结语 由来半年前我写了一篇《学之者生，用之者死——ACE历史与简评》，其中提到“我心目中理想的网络库”的样子： 线程安全，支持多核多线程 不考虑可移植性，不跨平台，只支持 Linux，不支持 Windows。 在不增加复杂度的前提下可以支持 FreeBSD/Darwin，方便将来用 Mac 作为开发用机，但不为它做性能优化。也就是说 IO multiplexing 使用 poll 和 epoll。 主要支持 x86-64，兼顾 IA32 不支持 UDP，只支持 TCP 不支持 IPv6，只支持 IPv4 不考虑广域网应用，只考虑局域网 只支持一种使用模式：non-blocking IO + one event loop per thread，不考虑阻塞 IO API 简单易用，只暴露具体类和标准库里的类，不使用 - non-trivial templates，也不使用虚函数 只满足常用需求的 90%，不面面俱到，必要的时候以 app 来适应 lib 只做 library，不做成 framework 争取全部代码在 5000 行以内（不含测试） 以上条件都满足时，可以考虑搭配 Google Protocol Buffers RPC 在想清楚这些目标之后，我开始第三次尝试编写自己的 C++ 网络库。与前两次不同，这次我一开始就想好了库的名字，叫 muduo （木铎），并在 Google code 上创建了项目： http://code.google.com/p/muduo/ 。muduo 的主体内容在 5 月底已经基本完成，现在我把它开源。 本文主要介绍 muduo 网络库的使用，其设计与实现将有另文讲解。 下载与编译下载地址： http://muduo.googlecode.com/files/muduo-0.1.0-alpha.tar.gz SHA1 Checksum: 5d3642e311177ded89ed0d15c10921738f8c984c Muduo 使用了 Linux 较新的系统调用，要求 Linux 的内核版本大于 2.6.28 （我自己用的是 2.6.32 ）。在 Debian Squeeze / Ubuntu 10.04 LTS 上编译测试通过，32 位和 64 位系统都能使用。 Muduo 采用 CMake 为 build system，安装方法： $ sudo apt-get install cmake Muduo 依赖 Boost，很容易安装： $ sudo apt-get install libboost1.40-dev # 或 libboost1.42-dev 编译方法很简单： $ tar zxf muduo-0.1.0-alpha.tar.gz $ cd muduo/ $ ./build.sh 编译生成的可执行文件和静态库文件分别位于 ../build/debug/{bin,lib} 如果要编译 release 版，可执行 $ BUILD_TYPE=release ./build.sh 编译生成的可执行文件和静态库文件分别位于 ../build/release/{bin,lib} 编译完成之后请试运行其中的例子。比如 bin/inspector_test ，然后通过浏览器访问 http://10.0.0.10:12345/ 或 http://10.0.0.10:12345/proc/status，其中 10.0.0.10 替换为你的 Linux box 的 IP。 例子Muduo 附带了几十个小例子，位于 examples 目录。其中包括从 Boost.Asio、JBoss Netty、Python Twisted 等处移植过来的例子。 examples 12345678910111213141516171819202122232425262728293031323334353637|-- simple # 简单网络协议的实现| |-- allinone # 在一个程序里同时实现下面 5 个协议| |-- chargen # RFC 864，可测试带宽| |-- daytime # RFC 867| |-- discard # RFC 863| |-- echo # RFC 862| |-- time # RFC 868| &#96;-- timeclient # time 协议的客户端|-- hub # 一个简单的 pub&#x2F;sub&#x2F;hub 服务，演示应用级的广播|-- roundtrip # 测试两台机器的网络延时与时间差|-- asio # 从 Boost.Asio 移植的例子| |-- chat # 聊天服务| &#96;-- tutorial # 一系列 timers|-- netty # 从 JBoss Netty 移植的例子| |-- discard # 可用于测试带宽，服务器可多线程运行| |-- echo # 可用于测试带宽，服务器可多线程运行| &#96;-- uptime # TCP 长连接&#96;-- twisted # 从 Python Twisted 移植的例子 &#96;-- finger # finger01 ~ 07 基本结构Muduo 的目录结构如下。 1234567891011muduo|-- base # 与网络无关的基础代码，已提前发布&#96;-- net # 网络库 |-- http # 一个简单的可嵌入的 web 服务器 |-- inspect # 基于以上 web 服务器的“窥探器”，用于报告进程的状态 &#96;-- poller # poll(2) 和 epoll(4) 两种 IO multiplexing 后端 Muduo 是基于 Reactor 模式的网络库，其核心是个事件循环 EventLoop，用于响应计时器和 IO 事件。Muduo 采用基于对象（object based）而非面向对象（object oriented）的设计风格，其接口多以 boost::function + boost::bind 表达。 Muduo 的头文件明确分为客户可见和客户不可见两类。客户可见的为白底，客户不可见的为灰底。 这里简单介绍各个头文件及 class 的作用，详细的介绍留给以后的博客。 公开接口 Buffer 仿 Netty ChannelBuffer 的 buffer class，数据的读写通过 buffer 进行 InetAddress 封装 IPv4 地址 (end point)，注意，muduo 目前不能解析域名，只认 IP EventLoop 反应器 Reactor，用户可以注册计时器回调 EventLoopThread 启动一个线程，在其中运行 EventLoop::loop() TcpConnection 整个网络库的核心，封装一次 TCP 连接 TcpClient 用于编写网络客户端，能发起连接，并且有重试功能 TcpServer 用于编写网络服务器，接受客户的连接 在这些类中，TcpConnection 的生命期依靠 shared_ptr 控制（即用户和库共同控制）。Buffer 的生命期由 TcpConnection 控制。其余类的生命期由用户控制。 HttpServer 和 Inspector，暴露出一个 http 界面，用于监控进程的状态，类似于 Java JMX。这么做的原因是，《程序员修炼之道》第 6 章第 34 条提到“对于更大、更复杂的服务器代码，提供其操作的内部试图的一种漂亮技术是使用内建的 Web 服务器”，Jeff Dean 也说“（每个 Google 的服务器进程）Export HTML-based status pages for easy diagnosis”。 内部实现 Channel 是 selectable IO channel，负责注册与响应 IO 事件，它不拥有 file descriptor。它是 Acceptor、Connector、EventLoop、TimerQueue、TcpConnection 的成员，生命期由后者控制。 Socket 封装一个 file descriptor，并在析构时关闭 fd。它是 Acceptor、TcpConnection 的成员，生命期由后者控制。EventLoop、TimerQueue 也拥有 fd，但是不封装为 Socket。SocketsOps 封装各种 sockets 系统调用。 EventLoop 封装事件循环，也是事件分派的中心。它用 eventfd(2) 来异步唤醒，这有别于传统的用一对 pipe(2) 的办法。它用 TimerQueue 作为计时器管理，用 Poller 作为 IO Multiplexing。 Poller 是 PollPoller 和 EPollPoller 的基类，采用“电平触发”的语意。它是 EventLoop 的成员，生命期由后者控制。 PollPoller 和 EPollPoller 封装 poll(2) 和 epoll(4) 两种 IO Multiplexing 后端。Poll 的存在价值是便于调试，因为 poll(2) 调用是上下文无关的，用 strace 很容易知道库的行为是否正确。 Connector 用于发起 TCP 连接，它是 TcpClient 的成员，生命期由后者控制。 Acceptor 用于接受 TCP 连接，它是 TcpServer 的成员，生命期由后者控制。 TimerQueue 用 timerfd 实现定时，这有别于传统的设置 poll/epoll_wait 的等待时长的办法。为了简单起见，目前用链表来管理 Timer，如果有必要可改为优先队列，这样复杂度可从 O(n) 降为O(ln n) （某些操作甚至是 O(1)）。它是 EventLoop 的成员，生命期由后者控制。 EventLoopThreadPool 用于创建 IO 线程池，也就是说把 TcpConnection 分派到一组运行 EventLoop 的线程上。它是 TcpServer 的成员，生命期由后者控制。 类图 线程模型Muduo 的线程模型符合我主张的 one loop per thread + thread pool 模型。每个线程最多有一个 EventLoop。每个 TcpConnection 必须归某个 EventLoop 管理，所有的 IO 会转移到这个线程，换句话说一个 file descriptor 只能由一个线程读写。TcpConnection 所在的线程由其所属的 EventLoop 决定，这样我们可以很方便地把不同的 TCP 连接放到不同的线程去，也可以把一些 TCP 连接放到一个线程里。TcpConnection 和 EventLoop 是线程安全的，可以跨线程调用。TcpServer 直接支持多线程，它有两种模式： 单线程，accept 与 TcpConnection 用同一个线程做 IO。 多线程，accept 与 EventLoop 在同一个线程，另外创建一个 EventLoopThreadPool，新到的连接会按 round-robin 方式分配到线程池中。 结语Muduo 是我对常见网络编程任务的总结，用它我能很容易地编写多线程的 TCP 服务器和客户端。Muduo 是我业余时间的作品，代码估计还有很多 bug，功能也不完善（例如不支持 signal 处理），待日后慢慢改进吧。","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"IO","slug":"IO","permalink":"http://yoursite.com/tags/IO/"},{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"},{"name":"Reactor","slug":"Reactor","permalink":"http://yoursite.com/tags/Reactor/"}]}]}