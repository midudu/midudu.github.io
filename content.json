{"meta":{"title":"Midudu's Home","subtitle":"","description":"Tech Blog","author":"Midudu","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"muduo 多线程模型：一个 Sudoku 服务器演变","slug":"muduo多线程模型：一个Sudoku服务器演变","date":"2020-03-09T03:01:57.280Z","updated":"2020-03-09T03:02:43.000Z","comments":true,"path":"开源组件/muduo/muduo多线程模型：一个Sudoku服务器演变/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/muduo%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B%EF%BC%9A%E4%B8%80%E4%B8%AASudoku%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%BC%94%E5%8F%98/","excerpt":"","text":"原文链接 https://blog.csdn.net/solstice/article/details/6548228 本文以一个 Sudoku Solver 为例，回顾了并发网络服务程序的多种设计方案，并介绍了使用 muduo 网络库编写多线程服务器的两种最常用手法。以往的例子展现了 Muduo 在编写单线程并发网络服务程序方面的能力与便捷性，今天我们看一看它在多线程方面的表现。 本文代码见：http://code.google.com/p/muduo/source/browse/trunk/examples/sudoku/ 下载：http://muduo.googlecode.com/files/muduo-0.2.5-alpha.tar.gz 关于数独的求解算法见 https://blog.csdn.net/Solstice/article/details/2096209 一、Sudoku Solver 协议 基本实现 二、常见的并发网络服务程序设计方案 三、结语 四、代码 方案 5：单线程 Reactor 方案 8：Reactor + Thread Pool 方案 9：Multiple Reactors 一、Sudoku Solver假设有这么一个网络编程任务：写一个求解数独的程序 (Sudoku Solver)，并把它做成一个网络服务。 Sudoku Solver 是我喜爱的网络编程例子，它曾经出现在《分布式系统部署、监控与进程管理的几重境界》、《Muduo 设计与实现之一：Buffer 类的设计》、《〈多线程服务器的适用场合〉例释与答疑》等文中，它也可以看成是 echo 服务的一个变种（《谈一谈网络编程学习经验》把 echo 列为三大 TCP 网络编程案例之一）。 写这么一个程序在网络编程方面的难度不高，跟写 echo 服务差不多（从网络连接读入一个 Sudoku 题目，算出答案，再发回给客户），挑战在于怎样做才能发挥现在多核硬件的能力？在谈这个问题之前，让我们先写一个基本的单线程版。 协议一个简单的以 /r/n 分隔的文本行协议，使用 TCP 长连接，客户端在不需要服务时主动断开连接。 请求：[id:]〈81digits〉/r/n 响应：[id:]〈81digits〉/r/n 或者 [id:]NoSolution/r/n 其中 [id:] 表示可选的 id，用于区分先后的请求，以支持 Parallel Pipelining，响应中会回显请求中的 id。Parallel Pipelining 的意义见赖勇浩的《以小见大——那些基于 protobuf 的五花八门的 RPC（2） 》，或者见我写的《分布式系统的工程化开发方法》第 54 页关于 out-of-order RPC 的介绍。 〈81digits〉是 Sudoku 的棋盘，9x9 个数字，未知数字以 0 表示。 如果 Sudoku 有解，那么响应是填满数字的棋盘；如果无解，则返回 NoSolution。 例子1： 123请求：000000010400000000020000000000050407008000300001090000300400200050100000000806000&#x2F;r&#x2F;n响应：693784512487512936125963874932651487568247391741398625319475268856129743274836159&#x2F;r&#x2F;n 例子2： 123请求：a:000000010400000000020000000000050407008000300001090000300400200050100000000806000&#x2F;r&#x2F;n响应：a:693784512487512936125963874932651487568247391741398625319475268856129743274836159&#x2F;r&#x2F;n 例子3： 123请求：b:000000010400000000020000000000050407008000300001090000300400200050100000000806005&#x2F;r&#x2F;n响应：b:NoSolution&#x2F;r&#x2F;n 基于这个文本协议，我们可以用 telnet 模拟客户端来测试 sudoku solver，不需要单独编写 sudoku client。SudokuSolver 的默认端口号是 9981，因为它有 9x9=81 个格子。 基本实现Sudoku 的求解算法见《谈谈数独(Sudoku)》一文，这不是本文的重点。假设我们已经有一个函数能求解 Sudoku，它的原型如下 string solveSudoku(const string&amp; puzzle); 函数的输入是上文的”〈81digits〉”，输出是”〈81digits〉”或”NoSolution”。这个函数是个 pure function，同时也是线程安全的。 有了这个函数，我们以《Muduo 网络编程示例之零：前言》中的 EchoServer 为蓝本，稍作修改就能得到 SudokuServer。这里只列出最关键的 onMessage() 函数，完整的代码见 http://code.google.com/p/muduo/source/browse/trunk/examples/sudoku/server_basic.cc 。onMessage() 的主要功能是处理协议格式，并调用 solveSudoku() 求解问题。 server_basic.cc 1234567891011121314151617181920212223242526void onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp) &#123; LOG_DEBUG &lt;&lt; conn-&gt;name(); size_t len = buf-&gt;readableBytes(); while (len &gt;= kCells + 2) &#123; const char *crlf = buf-&gt;findCRLF(); if (crlf) &#123; string request(buf-&gt;peek(), crlf); buf-&gt;retrieveUntil(crlf + 2); len = buf-&gt;readableBytes(); if (!processRequest(conn, request)) &#123; conn-&gt;send(\"Bad Request!\\r\\n\"); conn-&gt;shutdown(); break; &#125; &#125; else if (len &gt; 100) &#123; conn-&gt;send(\"Id too long!\\r\\n\"); conn-&gt;shutdown(); break; &#125; else &#123; break; &#125; &#125;&#125; server_basic.cc 是一个并发服务器，可以同时服务多个客户连接。但是它是单线程的，无法发挥多核硬件的能力。 Sudoku 是一个计算密集型的任务（见《Muduo 设计与实现之一：Buffer 类的设计》中关于其性能的分析），其瓶颈在 CPU。为了让这个单线程 server_basic 程序充分利用 CPU 资源，一个简单的办法是在同一台机器上部署多个 server_basic 进程，让每个进程占用不同的端口，比如在一台 8 核机器上部署 8 个 server_basic 进程，分别占用 9981、9982、……、9988 端口。这样做其实是把难题推给了客户端，因为客户端(s)要自己做负载均衡。再想得远一点，在 8 个 server_basic 前面部署一个 load balancer？似乎小题大做了。 能不能在一个端口上提供服务，并且又能发挥多核处理器的计算能力呢？当然可以，办法不止一种。 二、常见的并发网络服务程序设计方案W. Richard Stevens 的 UNP2e 第 27 章 Client-Server Design Alternatives 介绍了十来种当时（90 年代末）流行的编写并发网络程序的方案。UNP3e 第 30 章，内容未变，还是这几种。以下简称 UNP CSDA 方案。UNP 这本书主要讲解阻塞式网络编程，在非阻塞方面着墨不多，仅有一章。正确使用 non-blocking IO 需要考虑的问题很多，不适宜直接调用 Sockets API，而需要一个功能完善的网络库支撑。 随着 2000 年前后第一次互联网浪潮的兴起，业界对高并发 http 服务器的强烈需求大大推动了这一领域的研究，目前高性能 httpd 普遍采用的是单线程 reactor 方式。另外一个说法是 IBM Lotus 使用 TCP 长连接协议，而把 Lotus 服务端移植到 Linux 的过程中 IBM 的工程师们大大提高了 Linux 内核在处理并发连接方面的可伸缩性，因为一个公司可能有上万人同时上线，连接到同一台跑着 Lotus server 的 Linux 服务器。 可伸缩网络编程这个领域其实近十年来没什么新东西，POSA2 已经作了相当全面的总结，另外以下几篇文章也值得参考。 http://bulk.fefe.de/scalable-networking.pdf http://www.kegel.com/c10k.html http://gee.cs.oswego.edu/dl/cpjslides/nio.pdf 下表是陈硕总结的 10 种常见方案。其中“多连接互通”指的是如果开发 chat 服务，多个客户连接之间是否能方便地交换数据（chat 也是《谈一谈网络编程学习经验》中举的三大 TCP 网络编程案例之一）。对于 echo/http/sudoku 这类“连接相互独立”的服务程序，这个功能无足轻重，但是对于 chat 类服务至关重要。“顺序性”指的是在 http/sudoku 这类请求-响应服务中，如果客户连接顺序发送多个请求，那么计算得到的多个响应是否按相同的顺序发还给客户（这里指的是在自然条件下，不含刻意同步）。 方案 model UNP 对应 阻塞/非阻塞 多进程？ 多线程？ IO 复用？ 长连接？ 并发性 多核？ 开销 多连接互通？ 顺序性 线程数确定？ 特点 0 accept + read/write 0 阻塞 no no no no 无 no 低 no yes yes 一次服务一个客户 1 accept + fork 1 阻塞 yes no no yes 低 yes 高 no yes no process-per-connection 2 accept + thread 6 阻塞 no yes no yes 中 yes 中 yes yes no thread-per-connection 3 prefork 2/3/4/5 阻塞 yes no no yes 低 yes 高 no yes no 见 UNP 4 pre threaded 7/8 阻塞 no yes no yes 中 yes 中 yes yes no 见 UNP 5 poll(reactor) sec 6.8 非阻塞 no no yes yes 高 no 低 yes yes yes 单线程 reactor 6 reactor + thread-per-task 无 非阻塞 no yes yes yes 中 yes 中 yes no no thread-per-request 7 reactor + worker thread 无 非阻塞 no yes yes yes 中 yes 中 yes yes no worker-thread-per-connection 8 reactor + thread pool 无 非阻塞 no yes yes yes 高 yes 低 yes no yes 主线程 io + 工作线程计算 9 multiple reactors 无 非阻塞 no yes yes yes 高 yes 低 yes yes yes one-loop-per-thread UNP CSDA 方案归入 0~5。5 也是目前用得很多的单线程 reactor 方案，muduo 对此提供了很好的支持。6 和 7 其实不是实用的方案，只是作为过渡品。8 和 9 是本文重点介绍的方案，其实这两个方案已经在《多线程服务器的常用编程模型》一文中提到过，只不过当时我还没有写 muduo，无法用具体的代码示例来说明。 在对比各方案之前，我们先看看基本的 micro benchmark 数据（前三项由 lmbench 测得）： fork()+exit(): 160us pthread_create()+pthread_join(): 12us context switch : 1.5us sudoku resolve: 100us (根据题目难度不同，浮动范围 20~200us) 接下来看一下几种方案： 方案 0：这其实不是并发服务器，而是 iterative 服务器，因为它一次只能服务一个客户。代码见 UNP figure 1.9，UNP 以此为对比其他方案的基准点。这个方案不适合长连接，到是很适合 daytime 这种 write-only 服务。 方案 1：这是传统的 Unix 并发网络编程方案，UNP 称之为 child-per-client 或 fork()-per-client，另外也俗称 process-per-connection。这种方案适合并发连接数不大的情况。至今仍有一些网络服务程序用这种方式实现，比如 PostgreSQL 和 Perforce 的服务端。这种方案适合“计算响应的工作量远大于 fork() 的开销”这种情况，比如数据库服务器。这种方案适合长连接，但不太适合短连接，因为 fork() 开销大于求解 sudoku 的用时。 方案 2：这是传统的 Java 网络编程方案 thread-per-connection，在 Java 1.4 引入 NIO 之前，Java 网络服务程序多采用这种方案。它的初始化开销比方案 1 要小很多。这种方案的伸缩性受到线程数的限制，一两百个还行，几千个的话对操作系统的 scheduler 恐怕是个不小的负担。 方案 3：这是针对方案 1 的优化，UNP 详细分析了几种变化，包括对 accept 惊群问题的考虑。 方案 4：这是对方案 2 的优化，UNP 详细分析了它的几种变化。 以上几种方案都是阻塞式网络编程，程序（thread-of-control）通常阻塞在 read() 上，等待数据到达。但是 TCP 是个全双工协议，同时支持 read() 和 write() 操作，当一个线程/进程阻塞在 read() 上，但程序又想给这个 TCP 连接发数据，那该怎么办？比如说 echo client，既要从 stdin 读，又要从网络读，当程序正在阻塞地读网络的时候，如何处理键盘输入？又比如 proxy，既要把连接 a 收到的数据发给连接 b，又要把从连接 b 收到的数据发给连接 a，那么到底读哪个？（proxy 是《谈一谈网络编程学习经验》中举的三大 TCP 网络编程案例之一。） 一种方法是用两个线程/进程，一个负责读，一个负责写。UNP 也在实现 echo client 时介绍了这种方案。另外见 Python Pinhole 的代码：http://code.activestate.com/recipes/114642/ ((另一种方法))是使用 IO multiplexing，也就是 select/poll/epoll/kqueue 这一系列的“多路选择器”，让一个 thread-of-control 能处理多个连接。“IO 复用”其实复用的不是 IO 连接，而是复用线程。使用 select/poll 几乎肯定要配合 non-blocking IO，而使用 non-blocking IO 肯定要使用应用层 buffer，原因见《Muduo 设计与实现之一：Buffer 类的设计》。这就不是一件轻松的事儿了，如果每个程序都去搞一套自己的 IO multiplexing 机制（本质是 event-driven 事件驱动），这是一种很大的浪费。感谢 Doug Schmidt 为我们总结出了 Reactor 模式，让 event-driven 网络编程有章可循。继而出现了一些通用的 reactor 框架/库，比如 libevent、muduo、Netty、twisted、POE 等等，有了这些库，我想基本不用去编写阻塞式的网络程序了（特殊情况除外，比如 proxy 流量限制）。 单线程 reactor 的程序结构是（图片取自 Doug Lea 的演讲）： 方案 5：基本的单线程 reactor 方案，即前面的 server_basic.cc 程序。本文以它作为对比其他方案的基准点。这种方案的优点是由网络库搞定数据收发，程序只关心业务逻辑；缺点在前面已经谈了：适合 IO 密集的应用，不太适合 CPU 密集的应用，因为较难发挥多核的威力。 方案 6：这是一个过渡方案，收到 Sudoku 请求之后，不在 reactor 线程计算，而是创建一个新线程去计算，以充分利用多核 CPU。这是非常初级的多线程应用，因为它为每个请求（而不是每个连接）创建了一个新线程。这个开销可以用线程池来避免，即方案 8。这个方案还有一个特点是 out-of-order，即同时创建多个线程去计算同一个连接上收到的多个请求，那么算出结果的次序是不确定的，可能第 2 个 Sudoku 比较简单，比第 1 个先算出结果。这也是为什么我们在一开始设计协议的时候使用了 id，以便客户端区分 response 对应的是哪个 request。 方案 7：为了让返回结果的顺序确定，我们可以为每个连接创建一个计算线程，每个连接上的请求固定发给同一个线程去算，先到先得。这也是一个过渡方案，因为并发连接数受限于线程数目，这个方案或许还不如直接使用阻塞 IO 的 thread-per-connection 方案2。方案 7 与方案 6 的另外一个区别是一个 client 的最大 CPU 占用率，在方案 6 中，一个 connection 上发来的一长串突发请求(burst requests) 可以占满全部 8 个 core；而在方案 7 中，由于每个连接上的请求固定由同一个线程处理，那么它最多占用 12.5% 的 CPU 资源。这两种方案各有优劣，取决于应用场景的需要，到底是公平性重要还是突发性能重要。这个区别在方案 8 和方案 9 中同样存在，需要根据应用来取舍。 方案 8：为了弥补方案 6 中为每个请求创建线程的缺陷，我们使用固定大小线程池，程序结构如下图。全部的 IO 工作都在一个 reactor 线程完成，而计算任务交给 thread pool。如果计算任务彼此独立，而且 IO 的压力不大，那么这种方案是非常适用的。Sudoku Solver 正好符合。代码见：http://code.google.com/p/muduo/source/browse/trunk/examples/sudoku/server_threadpool.cc 后文给出了它与方案 9 的区别。 如果 IO 的压力比较大，一个 reactor 忙不过来，可以试试 multiple reactors 的方案 9。 方案 9：这是 muduo 内置的多线程方案，也是 Netty 内置的多线程方案。这种方案的特点是 one loop per thread，有一个 main reactor 负责 accept 连接，然后把连接挂在某个 sub reactor 中（muduo 采用 round-robin 的方式来选择 sub reactor），这样该连接的所有操作都在那个 sub reactor 所处的线程中完成。多个连接可能被分派到多个线程中，以充分利用 CPU。Muduo 采用的是固定大小的 reactor pool，池子的大小通常根据 CPU 核数确定，也就是说线程数是固定的，这样程序的总体处理能力不会随连接数增加而下降。另外，由于一个连接完全由一个线程管理，那么请求的顺序性有保证，突发请求也不会占满全部 8 个核（如果需要优化突发请求，可以考虑方案 10）。这种方案把 IO 分派给多个线程，防止出现一个 reactor 的处理能力饱和。与方案 8 的线程池相比，方案 9 减少了进出 thread pool 的两次上下文切换。我认为这是一个适应性很强的多线程 IO 模型，因此把它作为 muduo 的默认线程模型。 方案 10：把方案 8 和方案 90 混合，既使用多个 reactors 来处理 IO，又使用线程池来处理计算。这种方案适合既有突发 IO （利用多线程处理多个连接上的 IO），又有突发计算的应用（利用线程池把一个连接上的计算任务分配给多个线程去做）。 这种其实方案看起来复杂，其实写起来很简单，只要把方案 8 的代码加一行 server_.setThreadNum(numThreads); 就行，这里就不举例了。 三、结语我在《多线程服务器的常用编程模型》一文中说 总结起来，我推荐的多线程服务端编程模式为：event loop per thread + thread pool。 event loop 用作 non-blocking IO 和定时器。thread pool 用来做计算，具体可以是任务队列或消费者-生产者队列。 当时（2010年2月）我还说“以这种方式写服务器程序，需要一个优质的基于 Reactor 模式的网络库来支撑，我只用过in-house的产品，无从比较并推荐市面上常见的 C++ 网络库，抱歉。” 现在有了 muduo 网络库，我终于能够用具体的代码示例把思想完整地表达出来。 四、代码方案 5：单线程 Reactorserver_basic.cc 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475class SudokuServer &#123;public: SudokuServer(EventLoop *loop, const InetAddress &amp;listenAddr) : server_(loop, listenAddr, \"SudokuServer\"), startTime_(Timestamp::now()) &#123; server_.setConnectionCallback( std::bind(&amp;SudokuServer::onConnection, this, _1)); server_.setMessageCallback( std::bind(&amp;SudokuServer::onMessage, this, _1, _2, _3)); &#125; void start() &#123; server_.start(); &#125;private: void onConnection(const TcpConnectionPtr &amp;conn) &#123; ... &#125; void onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp) &#123; LOG_DEBUG &lt;&lt; conn-&gt;name(); size_t len = buf-&gt;readableBytes(); while (len &gt;= kCells + 2) &#123; const char *crlf = buf-&gt;findCRLF(); if (crlf) &#123; string request(buf-&gt;peek(), crlf); buf-&gt;retrieveUntil(crlf + 2); len = buf-&gt;readableBytes(); if (!processRequest(conn, request)) &#123; conn-&gt;send(\"Bad Request!\\r\\n\"); conn-&gt;shutdown(); break; &#125; &#125; else if (len &gt; 100) // id + \":\" + kCells + \"\\r\\n\" &#123; conn-&gt;send(\"Id too long!\\r\\n\"); conn-&gt;shutdown(); break; &#125; else &#123; break; &#125; &#125; &#125; bool processRequest(const TcpConnectionPtr &amp;conn, const string &amp;request) &#123; string id; string puzzle; bool goodRequest = true; string::const_iterator colon = find(request.begin(), request.end(), ':'); if (colon != request.end()) &#123; id.assign(request.begin(), colon); puzzle.assign(colon + 1, request.end()); &#125; else &#123; puzzle = request; &#125; if (puzzle.size() == implicit_cast&lt;size_t&gt;(kCells)) &#123; LOG_DEBUG &lt;&lt; conn-&gt;name(); string result = solveSudoku(puzzle); if (id.empty()) &#123; conn-&gt;send(result + \"\\r\\n\"); &#125; else &#123; conn-&gt;send(id + \":\" + result + \"\\r\\n\"); &#125; &#125; else &#123; goodRequest = false; &#125; return goodRequest; &#125; TcpServer server_; Timestamp startTime_;&#125;; 方案 8：Reactor + Thread Poolserver_threadpool.cc 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586class SudokuServer &#123;public: SudokuServer(EventLoop *loop, const InetAddress &amp;listenAddr, int numThreads) : server_(loop, listenAddr, \"SudokuServer\"), numThreads_(numThreads), startTime_(Timestamp::now()) &#123; server_.setConnectionCallback( std::bind(&amp;SudokuServer::onConnection, this, _1)); server_.setMessageCallback( std::bind(&amp;SudokuServer::onMessage, this, _1, _2, _3)); &#125; void start() &#123; LOG_INFO &lt;&lt; \"starting \" &lt;&lt; numThreads_ &lt;&lt; \" threads.\"; threadPool_.start(numThreads_); server_.start(); &#125;private: void onConnection(const TcpConnectionPtr &amp;conn) &#123; ... &#125; void onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp) &#123; LOG_DEBUG &lt;&lt; conn-&gt;name(); size_t len = buf-&gt;readableBytes(); while (len &gt;= kCells + 2) &#123; const char *crlf = buf-&gt;findCRLF(); if (crlf) &#123; string request(buf-&gt;peek(), crlf); buf-&gt;retrieveUntil(crlf + 2); len = buf-&gt;readableBytes(); if (!processRequest(conn, request)) &#123; conn-&gt;send(\"Bad Request!\\r\\n\"); conn-&gt;shutdown(); break; &#125; &#125; else if (len &gt; 100) &#123; conn-&gt;send(\"Id too long!\\r\\n\"); conn-&gt;shutdown(); break; &#125; else &#123; break; &#125; &#125; &#125; bool processRequest(const TcpConnectionPtr &amp;conn, const string &amp;request) &#123; string id; string puzzle; bool goodRequest = true; string::const_iterator colon = find(request.begin(), request.end(), ':'); if (colon != request.end()) &#123; id.assign(request.begin(), colon); puzzle.assign(colon + 1, request.end()); &#125; else &#123; puzzle = request; &#125; if (puzzle.size() == implicit_cast&lt;size_t&gt;(kCells)) &#123; threadPool_.run(std::bind(&amp;solve, conn, puzzle, id)); &#125; else &#123; goodRequest = false; &#125; return goodRequest; &#125; static void solve(const TcpConnectionPtr &amp;conn, const string &amp;puzzle, const string &amp;id) &#123; LOG_DEBUG &lt;&lt; conn-&gt;name(); string result = solveSudoku(puzzle); if (id.empty()) &#123; conn-&gt;send(result + \"\\r\\n\"); &#125; else &#123; conn-&gt;send(id + \":\" + result + \"\\r\\n\"); &#125; &#125; TcpServer server_; ThreadPool threadPool_; int numThreads_; Timestamp startTime_;&#125;; 方案 9：Multiple Reactorsserver_multiloop.cc 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879class SudokuServer &#123;public: SudokuServer(EventLoop *loop, const InetAddress &amp;listenAddr, int numThreads) : server_(loop, listenAddr, \"SudokuServer\"), numThreads_(numThreads), startTime_(Timestamp::now()) &#123; server_.setConnectionCallback( std::bind(&amp;SudokuServer::onConnection, this, _1)); server_.setMessageCallback( std::bind(&amp;SudokuServer::onMessage, this, _1, _2, _3)); server_.setThreadNum(numThreads); &#125; void start() &#123; LOG_INFO &lt;&lt; \"starting \" &lt;&lt; numThreads_ &lt;&lt; \" threads.\"; server_.start(); &#125;private: void onConnection(const TcpConnectionPtr &amp;conn) &#123; ... &#125; void onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp) &#123; LOG_DEBUG &lt;&lt; conn-&gt;name(); size_t len = buf-&gt;readableBytes(); while (len &gt;= kCells + 2) &#123; const char *crlf = buf-&gt;findCRLF(); if (crlf) &#123; string request(buf-&gt;peek(), crlf); buf-&gt;retrieveUntil(crlf + 2); len = buf-&gt;readableBytes(); if (!processRequest(conn, request)) &#123; conn-&gt;send(\"Bad Request!\\r\\n\"); conn-&gt;shutdown(); break; &#125; &#125; else if (len &gt; 100) &#123; conn-&gt;send(\"Id too long!\\r\\n\"); conn-&gt;shutdown(); break; &#125; else &#123; break; &#125; &#125; &#125; bool processRequest(const TcpConnectionPtr &amp;conn, const string &amp;request) &#123; string id; string puzzle; bool goodRequest = true; string::const_iterator colon = find(request.begin(), request.end(), ':'); if (colon != request.end()) &#123; id.assign(request.begin(), colon); puzzle.assign(colon + 1, request.end()); &#125; else &#123; puzzle = request; &#125; if (puzzle.size() == implicit_cast&lt;size_t&gt;(kCells)) &#123; LOG_DEBUG &lt;&lt; conn-&gt;name(); string result = solveSudoku(puzzle); if (id.empty()) &#123; conn-&gt;send(result + \"\\r\\n\"); &#125; else &#123; conn-&gt;send(id + \":\" + result + \"\\r\\n\"); &#125; &#125; else &#123; goodRequest = false; &#125; return goodRequest; &#125; TcpServer server_; int numThreads_; Timestamp startTime_;&#125;;","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"Net","slug":"Net","permalink":"http://yoursite.com/tags/Net/"}]},{"title":"muduo 网络编程示例之二： Boost.Asio 的聊天服务器","slug":"muduo网络编程示例之二：Boost.Asio的聊天服务器","date":"2020-03-09T03:00:45.855Z","updated":"2020-03-09T03:01:30.000Z","comments":true,"path":"开源组件/muduo/muduo网络编程示例之二：Boost.Asio的聊天服务器/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/muduo%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%A4%BA%E4%BE%8B%E4%B9%8B%E4%BA%8C%EF%BC%9ABoost.Asio%E7%9A%84%E8%81%8A%E5%A4%A9%E6%9C%8D%E5%8A%A1%E5%99%A8/","excerpt":"","text":"原文链接 https://blog.csdn.net/Solstice/article/details/6172391 本文将介绍一个与 Boost.Asio 的示例代码中的聊天服务器功能类似的网络服务程序，包括客户端与服务端的 muduo 实现。这个例子的主要目的是介绍如何处理分包，并初步涉及 Muduo 的多线程功能。Muduo 的下载地址： http://muduo.googlecode.com/files/muduo-0.1.7-alpha.tar.gz ，SHA1 873567e43b3c2cae592101ea809b30ba730f2ee6，本文的完整代码可在线阅读http://code.google.com/p/muduo/source/browse/trunk/examples/asio/chat/ 。 一、TCP 分包 二、聊天服务 三、消息格式 四、打包的代码 五、分包的代码 六、编解码器 LengthHeaderCodec 七、服务端的实现 八、客户端的实现 九、简单测试 一、TCP 分包前面一篇《五个简单 TCP 协议》中处理的协议没有涉及分包，在 TCP 这种字节流协议上做应用层分包是网络编程的基本需求。分包指的是在发送一个消息(message)或一帧(frame)数据时，通过一定的处理，让接收方能从字节流中识别并截取（还原）出一个个消息。“粘包问题”是个伪问题。 对于短连接的 TCP 服务，分包不是一个问题，只要发送方主动关闭连接，就表示一条消息发送完毕，接收方 read() 返回 0，从而知道消息的结尾。例如前一篇文章里的 daytime 和 time 协议。 注：一方主动关闭 TCP 连接时，另一方 read() 返回 0 ，则代表对方已经关闭连接。 对于长连接的 TCP 服务，分包有四种方法： 消息长度固定，比如 muduo 的 roundtrip 示例就采用了固定的 16 字节消息； 使用特殊的字符或字符串作为消息的边界，例如 HTTP 协议的 headers 以 “/r/n” 为字段的分隔符； 在每条消息的头部加一个长度字段，这恐怕是最常见的做法，本文的聊天协议也采用这一办法； 利用消息本身的格式来分包，例如 XML 格式的消息中 ... 的配对，或者 JSON 格式中的 { ... } 的配对。解析这种消息格式通常会用到状态机。 在后文的代码讲解中还会仔细讨论用长度字段分包的常见陷阱。 二、聊天服务本文实现的聊天服务非常简单，由服务端程序和客户端程序组成，协议如下： 服务端程序中某个端口侦听 (listen) 新的连接； 客户端向服务端发起连接； 连接建立之后，客户端随时准备接收服务端的消息并在屏幕上显示出来； 客户端接受键盘输入，以回车为界，把消息发送给服务端； 服务端接收到消息之后，依次发送给每个连接到它的客户端；原来发送消息的客户端进程也会收到这条消息； 一个服务端进程可以同时服务多个客户端进程，当有消息到达服务端后，每个客户端进程都会收到同一条消息，服务端广播发送消息的顺序是任意的，不一定哪个客户端会先收到这条消息。 （可选）如果消息 A 先于消息 B 到达服务端，那么每个客户端都会先收到 A 再收到 B。 这实际上是一个简单的基于 TCP 的应用层广播协议，由服务端负责把消息发送给每个连接到它的客户端。参与“聊天”的既可以是人，也可以是程序。在以后的文章中，我将介绍一个稍微复杂的一点的例子 hub，它有“聊天室”的功能，客户端可以注册特定的 topic(s)，并往某个 topic 发送消息，这样代码更有意思。 三、消息格式本聊天服务的消息格式非常简单，“消息”本身是一个字符串，每条消息的有一个 4 字节的头部，以网络序存放字符串的长度。消息之间没有间隙，字符串也不一定以 ‘/0’ 结尾。比方说有两条消息 “hello” 和 “chenshuo”，那么打包后的字节流是： 10x00, 0x00, 0x00, 0x05, &#39;h&#39;, &#39;e&#39;, &#39;l&#39;, &#39;l&#39;, &#39;o&#39;, 0x00, 0x00, 0x00, 0x08, &#39;c&#39;, &#39;h&#39;, &#39;e&#39;, &#39;n&#39;, &#39;s&#39;, &#39;h&#39;, &#39;u&#39;, &#39;o&#39; 共 21 字节。 四、打包的代码这段代码把 const string&amp; message 打包为 muduo::net::Buffer，并通过 conn 发送。 muduo/examples/asio/chat/codec.h 12345678910void send(muduo::net::TcpConnection* conn, const string&amp; message) &#123; muduo::net::Buffer buf; buf.append(message.data(), message.size()); int32_t len = muduo::net::sockets::hostToNetwork32(static_cast(message.size())); buf.prepend(&amp;len, sizeof len); conn-&gt;send(&amp;buf);&#125; muduo::Buffer 有一个很好的功能，它在头部预留了 8 个字节的空间，这样第 6 行的 prepend() 操作就不需要移动已有的数据，效率较高。 五、分包的代码解析数据往往比生成数据复杂，分包打包也不例外。 muduo/examples/asio/chat/codec.h 12345678910111213141516171819void onMessage(const muduo::net::TcpConnectionPtr&amp; conn, muduo::net::Buffer* buf, muduo::Timestamp receiveTime) &#123; while (buf-&gt;readableBytes() &gt;= kHeaderLen) &#123; const void* data = buf-&gt;peek(); int32_t tmp = *static_cast&lt;const int32_t*&gt;(data); int32_t len = muduo::net::sockets::networkToHost32(tmp); if (len &gt; 65536 || len &lt; 0) &#123; LOG_ERROR &lt;&lt; \"Invalid length \" &lt;&lt; len; conn-&gt;shutdown(); &#125; else if (buf-&gt;readableBytes() &gt;= len + kHeaderLen) &#123; buf-&gt;retrieve(kHeaderLen); muduo::string message(buf-&gt;peek(), len); buf-&gt;retrieve(len); messageCallback_(conn, message, receiveTime); // 收到完整的消息，通知用户 &#125; else &#123; break; &#125; &#125;&#125; 上面这段代码第 7 行用了 while 循环来反复读取数据，直到 Buffer 中的数据不够一条完整的消息。请读者思考，如果换成 if (buf-&gt;readableBytes() &gt;= kHeaderLen) 会有什么后果。 以前面提到的两条消息的字节流为例： 0x00, 0x00, 0x00, 0x05, ‘h’, ‘e’, ‘l’, ‘l’, ‘o’, 0x00, 0x00, 0x00, 0x08, ‘c’, ‘h’, ‘e’, ‘n’, ‘s’, ‘h’, ‘u’, ‘o’ 假设数据最终都全部到达，onMessage() 至少要能正确处理以下各种数据到达的次序，每种情况下 messageCallback_ 都应该被调用两次： 每次收到一个字节的数据，onMessage() 被调用 21 次； 数据分两次到达，第一次收到 2 个字节，不足消息的长度字段； 数据分两次到达，第一次收到 4 个字节，刚好够长度字段，但是没有 body； 数据分两次到达，第一次收到 8 个字节，长度完整，但 body 不完整； 数据分两次到达，第一次收到 9 个字节，长度完整，body 也完整； 数据分两次到达，第一次收到 10 个字节，第一条消息的长度完整、body 也完整，第二条消息长度不完整； 请自行移动分割点，验证各种情况； 数据一次就全部到达，这时必须用 while 循环来读出两条消息，否则消息会堆积。 请读者验证 onMessage() 是否做到了以上几点。这个例子充分说明了 non-blocking read 必须和 input buffer 一起使用。这也解释了为什么该用 while 而不是 if 。 六、编解码器 LengthHeaderCodec有人评论 Muduo 的接收缓冲区不能设置回调函数的触发条件，确实如此。每当 socket 可读，Muduo 的 TcpConnection 会读取数据并存入 Input Buffer，然后回调用户的函数。不过，一个简单的间接层就能解决问题，让用户代码只关心“消息到达”而不是“数据到达”，如本例中的 LengthHeaderCodec 所展示的那一样。 12345678910111213141516171819202122#ifndef MUDUO_EXAMPLES_ASIO_CHAT_CODEC_H#define MUDUO_EXAMPLES_ASIO_CHAT_CODEC_H using muduo::Logger;class LengthHeaderCodec : boost::noncopyable &#123;public: typedef boost::function&lt;void (const muduo::net::TcpConnectionPtr&amp;, const muduo::string&amp; message, muduo::Timestamp)&gt; StringMessageCallback; explicit LengthHeaderCodec(const StringMessageCallback&amp; cb) : messageCallback_(cb) &#123; &#125; void onMessage(const muduo::net::TcpConnectionPtr&amp; conn, muduo::net::Buffer* buf, muduo::Timestamp receiveTime) &#123; 同上 &#125; void send(muduo::net::TcpConnection* conn, const muduo::string&amp; message) &#123; 同上 &#125;private: StringMessageCallback messageCallback_; const static size_t kHeaderLen = sizeof(int32_t);&#125;;#endif // MUDUO_EXAMPLES_ASIO_CHAT_CODEC_H 这段代码把以 Buffer* 为参数的 MessageCallback 转换成了以 const string&amp; 为参数的 StringMessageCallback，让用户代码不必关心分包操作。客户端和服务端都能从中受益。 七、服务端的实现聊天服务器的服务端代码小于 100 行，不到 asio 的一半。 请先阅读第 68 行起的数据成员的定义。除了经常见到的 EventLoop 和 TcpServer，ChatServer 还定义了 codec_ 和 std::set connections_ 作为成员，connections_ 是目前已建立的客户连接，在收到消息之后，服务器会遍历整个容器，把消息广播给其中每一个 TCP 连接。 首先，在构造函数里注册回调： 12345678910111213141516171819#include \"codec.h\"using namespace muduo;using namespace muduo::net;class ChatServer : boost::noncopyable &#123;public: ChatServer(EventLoop* loop, const InetAddress&amp; listenAddr): loop_(loop), server_(loop, listenAddr, \"ChatServer\"), codec_(boost::bind(&amp;ChatServer::onStringMessage, this, _1, _2, _3)) &#123; server_.setConnectionCallback(boost::bind(&amp;ChatServer::onConnection, this, _1)); server_.setMessageCallback(boost::bind(&amp;LengthHeaderCodec::onMessage, &amp;codec_, _1, _2, _3)); &#125; void start() &#123; server_.start(); &#125; 这里有几点值得注意，在以往的代码里是直接把本 class 的 onMessage() 注册给 server_；这里我们把 LengthHeaderCodec::onMessage() 注册给 server_，然后向 codec_ 注册了 ChatServer::onStringMessage()，等于说让 codec_ 负责解析消息，然后把完整的消息回调给 ChatServer。这正是我前面提到的“一个简单的间接层”，在不增加 Muduo 库的复杂度的前提下，提供了足够的灵活性让我们在用户代码里完成需要的工作。 另外，server_.start() 绝对不能在构造函数里调用，这么做将来会有线程安全的问题，见我在《当析构函数遇到多线程 ── C++ 中线程安全的对象回调》一文中的论述。 以下是处理连接的建立和断开的代码，注意它把新建的连接加入到 connections_ 容器中，把已断开的连接从容器中删除。这么做是为了避免内存和资源泄漏，TcpConnectionPtr 是 boost::shared_ptr，是 muduo 里唯一一个默认采用 shared_ptr 来管理生命期的对象。以后我们会谈到这么做的原因。 1234567891011private: void onConnection(const TcpConnectionPtr&amp; conn) &#123; LOG_INFO &lt;&lt; conn-&gt;localAddress().toHostPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;peerAddress().toHostPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); MutexLockGuard lock(mutex_); if (conn-&gt;connected()) &#123; connections_.insert(conn); &#125; else &#123; connections_.erase(conn); &#125; &#125; 以下是服务端处理消息的代码，它遍历整个 connections_ 容器，把消息打包发送给各个客户连接。 1234567void onStringMessage(const TcpConnectionPtr&amp;, const string&amp; message, Timestamp) &#123; MutexLockGuard lock(mutex_); for (ConnectionList::iterator it = connections_.begin(); it != connections_.end(); ++it) &#123; codec_.send(get_pointer(*it), message); &#125;&#125; 数据成员： 1234567typedef std::set ConnectionList;EventLoop* loop_;TcpServer server_;LengthHeaderCodec codec_;MutexLock mutex_;ConnectionList connections_;&#125;; main() 函数里边是例行公事的代码： 12345678910111213int main(int argc, char* argv[]) &#123; LOG_INFO &lt;&lt; \"pid = \" &lt;&lt; getpid(); if (argc &gt; 1) &#123; EventLoop loop; uint16_t port = static_cast(atoi(argv[1])); InetAddress serverAddr(port); ChatServer server(&amp;loop, serverAddr); server.start(); loop.loop(); &#125; else &#123; printf(\"Usage: %s port/n\", argv[0]); &#125;&#125; 如果你读过 asio 的对应代码，会不会觉得 Reactor 往往比 Proactor 容易使用？ 八、客户端的实现我有时觉得服务端的程序常常比客户端的更容易写，聊天服务器再次验证了我的看法。客户端的复杂性来自于它要读取键盘输入，而 EventLoop 是独占线程的，所以我用了两个线程，main() 函数所在的线程负责读键盘，另外用一个 EventLoopThread 来处理网络 IO。我暂时没有把标准输入输出融入 Reactor 的想法，因为服务器程序的 stdin 和 stdout 往往是重定向了的。 来看代码，首先，在构造函数里注册回调，并使用了跟前面一样的 LengthHeaderCodec 作为中间层，负责打包分包。 12345678910111213141516171819#include \"codec.h\"using namespace muduo;using namespace muduo::net;class ChatClient : boost::noncopyable &#123;public: ChatClient(EventLoop* loop, const InetAddress&amp; listenAddr) : loop_(loop), client_(loop, listenAddr, \"ChatClient\"), codec_(boost::bind(&amp;ChatClient::onStringMessage, this, _1, _2, _3)) &#123; client_.setConnectionCallback(boost::bind(&amp;ChatClient::onConnection, this, _1)); client_.setMessageCallback(boost::bind(&amp;LengthHeaderCodec::onMessage, &amp;codec_, _1, _2, _3)); client_.enableRetry(); &#125; void connect() &#123; client_.connect(); &#125; disconnect() 目前为空，客户端的连接由操作系统在进程终止时关闭。 123void disconnect() &#123; // client_.disconnect();&#125; write() 会由 main 线程调用，所以要加锁，这个锁不是为了保护 TcpConnection，而是保护 shared_ptr。 123456void write(const string&amp; message) &#123; MutexLockGuard lock(mutex_); if (connection_) &#123; codec_.send(get_pointer(connection_), message); &#125;&#125; onConnection() 会由 EventLoop 线程调用，所以要加锁以保护 shared_ptr。 123456789101112private: void onConnection(const TcpConnectionPtr&amp; conn) &#123; LOG_INFO &lt;&lt; conn-&gt;localAddress().toHostPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;peerAddress().toHostPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); MutexLockGuard lock(mutex_); if (conn-&gt;connected()) &#123; connection_ = conn; &#125; else &#123; connection_.reset(); &#125; &#125; 把收到的消息打印到屏幕，这个函数由 EventLoop 线程调用，但是不用加锁，因为 printf() 是线程安全的。注意这里不能用 cout，它不是线程安全的。 123void onStringMessage(const TcpConnectionPtr&amp;, const string&amp; message, Timestamp) &#123; printf(\"&lt;&lt;&lt; %s/n\", message.c_str());&#125; 数据成员： 123456EventLoop* loop_;TcpClient client_;LengthHeaderCodec codec_;MutexLock mutex_;TcpConnectionPtr connection_;&#125;; main() 函数里除了例行公事，还要启动 EventLoop 线程和读取键盘输入。 1234567891011121314151617181920int main(int argc, char* argv[]) &#123; LOG_INFO &lt;&lt; \"pid = \" &lt;&lt; getpid(); if (argc &gt; 2) &#123; EventLoopThread loopThread; uint16_t port = static_cast(atoi(argv[2])); InetAddress serverAddr(argv[1], port); ChatClient client(loopThread.startLoop(), serverAddr); // 注册到 EventLoopThread 的 EventLoop 上。 client.connect(); std::string line; while (std::getline(std::cin, line)) &#123; string message(line.c_str()); // 这里似乎多此一举，可直接发送 line。这里是 client.write(message); &#125; client.disconnect(); &#125; else &#123; printf(\"Usage: %s host_ip port/n\", argv[0]); &#125;&#125; 九、简单测试开三个命令行窗口，在第一个运行 $ ./asio_chat_server 3000 第二个运行 $ ./asio_chat_client 127.0.0.1 3000 第三个运行同样的命令 $ ./asio_chat_client 127.0.0.1 3000 这样就有两个客户端进程参与聊天。在第二个窗口里输入一些字符并回车，字符会出现在本窗口和第三个窗口中。","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"Net","slug":"Net","permalink":"http://yoursite.com/tags/Net/"},{"name":"Boost","slug":"Boost","permalink":"http://yoursite.com/tags/Boost/"}]},{"title":"为什么 muduo 的 shutdown() 没有直接关闭 TCP 连接？","slug":"为什么muduo的shutdown()没有直接关闭TCP连接？","date":"2020-03-09T02:59:18.134Z","updated":"2020-03-09T03:00:08.000Z","comments":true,"path":"开源组件/muduo/为什么muduo的shutdown()没有直接关闭TCP连接？/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/%E4%B8%BA%E4%BB%80%E4%B9%88muduo%E7%9A%84shutdown()%E6%B2%A1%E6%9C%89%E7%9B%B4%E6%8E%A5%E5%85%B3%E9%97%ADTCP%E8%BF%9E%E6%8E%A5%EF%BC%9F/","excerpt":"","text":"原文链接 https://blog.csdn.net/Solstice/article/details/6208634 问题： 相关代码 回答 问题：1在 simple 中的 daytime 示例中，服务端主动关闭时调用的是如下函数序列，这不是只是关闭了连接上的写操作吗，怎么是关闭了整个连接？ 相关代码daytime.cc 123456789void DaytimeServer::onConnection(const TcpConnectionPtr &amp;conn) &#123; LOG_INFO &lt;&lt; \"DaytimeServer - \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); if (conn-&gt;connected()) &#123; conn-&gt;send(Timestamp::now().toFormattedString() + \"\\n\"); conn-&gt;shutdown(); &#125;&#125; TcpConnection.cc 12345678910111213141516void TcpConnection::shutdown() &#123; // FIXME: use compare and swap if (state_ == kConnected) &#123; setState(kDisconnecting); // FIXME: shared_from_this()? loop_-&gt;runInLoop(std::bind(&amp;TcpConnection::shutdownInLoop, this)); &#125;&#125;void TcpConnection::shutdownInLoop() &#123; loop_-&gt;assertInLoopThread(); if (!channel_-&gt;isWriting()) &#123; // we are not writing socket_-&gt;shutdownWrite(); &#125;&#125; Socket.cc 123456789void Socket::shutdownWrite() &#123; sockets::shutdownWrite(sockfd_);&#125;void sockets::shutdownWrite(int sockfd) &#123; if (::shutdown(sockfd, SHUT_WR) &lt; 0) &#123; LOG_SYSERR &lt;&lt; \"sockets::shutdownWrite\"; &#125;&#125; 回答Muduo TcpConnection 没有提供 close，而只提供 shutdown ，这么做是为了收发数据的完整性。 TCP 是一个全双工协议，同一个文件描述符既可读又可写， shutdownWrite() 关闭了“写”方向的连接，保留了“读”方向，这称为 TCP half-close。如果直接 close(socket_fd)，那么 socket_fd 就不能读或写了。 用 shutdown 而不用 close 的效果是，如果对方已经发送了数据，这些数据还“在路上”，那么 muduo 不会漏收这些数据。换句话说，muduo 在 TCP 这一层面解决了“当你打算关闭网络连接的时候，如何得知对方有没有发了一些数据而你还没有收到？”这一问题。当然，这个问题也可以在上面的协议层解决，双方商量好不再互发数据，就可以直接断开连接。 等于说 muduo 把“主动关闭连接”这件事情分成两步来做，如果要主动关闭连接，它会先关本地“写”端，等对方关闭之后，再关本地“读”端。 1练习：阅读代码，回答“如果被动关闭连接，muduo 的行为如何？ 提示：muduo 在 read() 返回 0 的时候会回调 connection callback，这样客户代码就知道对方断开连接了。 Muduo 这种关闭连接的方式对对方也有要求，那就是对方 read() 到 0 字节之后会主动关闭连接（无论 shutdownWrite() 还是 close()），一般的网络程序都会这样，不是什么问题。当然，这么做有一个潜在的安全漏洞，万一对方故意不不关，那么 muduo 的连接就一直半开着，消耗系统资源。 完整的流程是：我们发完了数据，于是 shutdownWrite，发送 TCP FIN 分节，对方会读到 0 字节，然后对方通常会关闭连接，这样 muduo 会读到 0 字节，然后 muduo 关闭连接。（思考题，在 shutdown() 之后，muduo 回调 connection callback 的时间间隔大约是一个 round-trip time，为什么？） 另外，如果有必要，对方可以在 read() 返回 0 之后继续发送数据，这是直接利用了 half-close TCP 连接。muduo 会收到这些数据，通过 message callback 通知客户代码。 那么 muduo 什么时候真正 close socket 呢？在 TcpConnection 对象析构的时候。TcpConnection 持有一个 Socket 对象，Socket 是一个 RAII handler，它的析构函数会 close(sockfd_)。这样，如果发生 TcpConnection 对象泄漏，那么我们从 /proc/pid/fd/ 就能找到没有关闭的文件描述符，便于查错。 muduo 在 read() 返回 0 的时候会回调 connection callback，然后把 TcpConnection 的引用计数减一，如果 TcpConnection 的引用计数降到零，它就会析构了。 参考： 《TCP/IP 详解》第一卷第 18.5 节，TCP Half-Close。 《UNIX 网络编程》第一卷第三版第 6.6 节， shutdown() 函数。","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"TCP","slug":"TCP","permalink":"http://yoursite.com/tags/TCP/"}]},{"title":"muduo 网络编程示例之一：五个简单 TCP 协议","slug":"muduo网络编程示例之一：五个简单TCP协议","date":"2020-03-09T02:58:09.704Z","updated":"2020-03-09T02:58:54.000Z","comments":true,"path":"开源组件/muduo/muduo网络编程示例之一：五个简单TCP协议/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/muduo%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%A4%BA%E4%BE%8B%E4%B9%8B%E4%B8%80%EF%BC%9A%E4%BA%94%E4%B8%AA%E7%AE%80%E5%8D%95TCP%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"原文链接 https://blog.csdn.net/Solstice/article/details/6171905 本文将介绍第一个示例：五个简单 TCP 网络服务协议，包括 echo (RFC 862)、discard (RFC 863)、chargen (RFC 864)、daytime (RFC 867)、time (RFC 868)，以及 time 协议的客户端。各协议的功能简介如下： discard - 丢弃所有收到的数据； daytime - 服务端 accept 连接之后，以字符串形式发送当前时间，然后主动断开连接； time - 服务端 accept 连接之后，以二进制形式发送当前时间（从 Epoch 到现在的秒数），然后主动断开连接；我们需要一个客户程序来把收到的时间转换为字符串。 echo - 回显服务，把收到的数据发回客户端； chargen - 服务端 accept 连接之后，不停地发送测试数据。 以上五个协议使用不同的端口，可以放到同一个进程中实现，且不必使用多线程。完整的代码见 muduo/examples/simple，下载地址 http://muduo.googlecode.com/files/muduo-0.1.6-alpha.tar.gz 。 一、discard 二、daytime 三、time server client 四、echo 五、chargen server client 六、Five in one 一、discardDiscard 恐怕算是最简单的长连接 TCP 应用层协议，它只需要关注“三个半事件”中的“消息/数据到达”事件 main.cpp 12345678int main() &#123; LOG_INFO &lt;&lt; \"pid = \" &lt;&lt; getpid(); EventLoop loop; InetAddress listenAddr(2009); DiscardServer server(&amp;loop, listenAddr); server.start(); loop.loop();&#125; discard.h 12345678910111213141516class DiscardServer &#123;public: DiscardServer(muduo::net::EventLoop *loop, const muduo::net::InetAddress &amp;listenAddr); void start();private: void onConnection(const muduo::net::TcpConnectionPtr &amp;conn); void onMessage(const muduo::net::TcpConnectionPtr &amp;conn, muduo::net::Buffer *buf, muduo::Timestamp time); muduo::net::TcpServer server_;&#125;; discard.cc 1234567891011121314151617181920212223242526DiscardServer::DiscardServer(EventLoop *loop, const InetAddress &amp;listenAddr) : server_(loop, listenAddr, \"DiscardServer\") &#123; server_.setConnectionCallback( std::bind(&amp;DiscardServer::onConnection, this, _1)); server_.setMessageCallback( std::bind(&amp;DiscardServer::onMessage, this, _1, _2, _3));&#125;void DiscardServer::start() &#123; server_.start();&#125;void DiscardServer::onConnection(const TcpConnectionPtr &amp;conn) &#123; LOG_INFO &lt;&lt; \"DiscardServer - \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\");&#125;void DiscardServer::onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp time) &#123; string msg(buf-&gt;retrieveAllAsString()); LOG_INFO &lt;&lt; conn-&gt;name() &lt;&lt; \" discards \" &lt;&lt; msg.size() &lt;&lt; \" bytes received at \" &lt;&lt; time.toString();&#125; 二、daytimeDaytime 是短连接协议，在发送完当前时间后，由服务端主动断开连接。它只需要关注“三个半事件”中的“连接已建立”事件 仅关注两个回调函数，其他部分和 discard 大同小异 daytime.cc 1234567891011121314151617void DaytimeServer::onConnection(const TcpConnectionPtr &amp;conn) &#123; LOG_INFO &lt;&lt; \"DaytimeServer - \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); if (conn-&gt;connected()) &#123; conn-&gt;send(Timestamp::now().toFormattedString() + \"\\n\"); conn-&gt;shutdown(); &#125;&#125;void DaytimeServer::onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp time) &#123; string msg(buf-&gt;retrieveAllAsString()); LOG_INFO &lt;&lt; conn-&gt;name() &lt;&lt; \" discards \" &lt;&lt; msg.size() &lt;&lt; \" bytes received at \" &lt;&lt; time.toString();&#125; 比较值得注意的是 onConnection 方法，发送完数据以后，由 server 端 shutdown 这个 socket 三、timeTime 协议与 daytime 极为类似，只不过它返回的不是日期时间字符串，而是一个 32-bit 整数，表示从 1970-01-01 00:00:00Z 到现在的秒数。当然，这个协议有“2038 年问题”。服务端只需要关注“三个半事件”中的“连接已建立”事件。 serverserver 端和 daytime 差不多，也是 send 以后主动 shutdown time.cc 1234567891011121314151617181920void TimeServer::onConnection(const muduo::net::TcpConnectionPtr &amp;conn) &#123; LOG_INFO &lt;&lt; \"TimeServer - \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); if (conn-&gt;connected()) &#123; time_t now = ::time(NULL); int32_t be32 = sockets::hostToNetwork32(static_cast&lt;int32_t&gt;(now)); conn-&gt;send(&amp;be32, sizeof be32); conn-&gt;shutdown(); &#125;&#125;void TimeServer::onMessage(const muduo::net::TcpConnectionPtr &amp;conn, muduo::net::Buffer *buf, muduo::Timestamp time) &#123; string msg(buf-&gt;retrieveAllAsString()); LOG_INFO &lt;&lt; conn-&gt;name() &lt;&lt; \" discards \" &lt;&lt; msg.size() &lt;&lt; \" bytes received at \" &lt;&lt; time.toString();&#125; client因为 time 服务端发送的是二进制数据，不便直接阅读，我们编写一个客户端来解析并打印收到的 4 个字节数据。这个程序只需要关注“三个半事件”中的“消息/数据到达”事件。 main.cpp 12345678910111213int main(int argc, char *argv[]) &#123; LOG_INFO &lt;&lt; \"pid = \" &lt;&lt; getpid(); if (argc &gt; 1) &#123; EventLoop loop; InetAddress serverAddr(argv[1], 2037); TimeClient timeClient(&amp;loop, serverAddr); timeClient.connect(); loop.loop(); &#125; else &#123; printf(\"Usage: %s host_ip\\n\", argv[0]); &#125;&#125; timeclient.h 123456789101112131415161718192021222324252627282930313233343536373839404142434445class TimeClient : noncopyable &#123;public: TimeClient(EventLoop *loop, const InetAddress &amp;serverAddr) : loop_(loop), client_(loop, serverAddr, \"TimeClient\") &#123; client_.setConnectionCallback( std::bind(&amp;TimeClient::onConnection, this, _1)); client_.setMessageCallback( std::bind(&amp;TimeClient::onMessage, this, _1, _2, _3)); // client_.enableRetry(); &#125; void connect() &#123; client_.connect(); &#125;private: EventLoop *loop_; TcpClient client_; void onConnection(const TcpConnectionPtr &amp;conn) &#123; LOG_INFO &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); if (!conn-&gt;connected()) &#123; loop_-&gt;quit(); // // 如果连接断开，则终止主循环，退出程序 &#125; &#125; void onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp receiveTime) &#123; if (buf-&gt;readableBytes() &gt;= sizeof(int32_t)) &#123; const void *data = buf-&gt;peek(); int32_t be32 = *static_cast&lt;const int32_t *&gt;(data); buf-&gt;retrieve(sizeof(int32_t)); time_t time = sockets::networkToHost32(be32); Timestamp ts(implicit_cast&lt;uint64_t&gt;(time) * Timestamp::kMicroSecondsPerSecond); LOG_INFO &lt;&lt; \"Server time = \" &lt;&lt; time &lt;&lt; \", \" &lt;&lt; ts.toFormattedString(); &#125; else &#123; LOG_INFO &lt;&lt; conn-&gt;name() &lt;&lt; \" no enough data \" &lt;&lt; buf-&gt;readableBytes() &lt;&lt; \" at \" &lt;&lt; receiveTime.toFormattedString(); &#125; &#125;&#125;; 注意其中考虑到了如果数据没有一次性收全，已经收到的数据会暂存在 Buffer 里，以等待下一次机会，程序也不会阻塞。这样即便服务器一个字节一个字节地发送数据，代码还是能正常工作，这也是非阻塞网络编程必须在用户态使用接受缓冲的主要原因。 四、echoEcho 是我们遇到的第一个带交互的协议：服务端把客户端发过来的数据原封不动地传回去。它只需要关注“三个半事件”中的“消息/数据到达”事件。 echo.cc 12345678910111213141516void EchoServer::onConnection(const muduo::net::TcpConnectionPtr &amp;conn) &#123; LOG_INFO &lt;&lt; \"EchoServer - \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\");&#125;void EchoServer::onMessage(const muduo::net::TcpConnectionPtr &amp;conn, muduo::net::Buffer *buf, muduo::Timestamp time) &#123; muduo::string msg(buf-&gt;retrieveAllAsString()); LOG_INFO &lt;&lt; conn-&gt;name() &lt;&lt; \" echo \" &lt;&lt; msg.size() &lt;&lt; \" bytes, \" &lt;&lt; \"data received at \" &lt;&lt; time.toString(); conn-&gt;send(msg);&#125; 这段代码实现的不是行回显(line echo)服务，而是有一点数据就发送一点数据。这样可以避免客户端恶意地不发送换行字符，而服务端又必须缓存已经收到的数据，导致服务器内存暴涨。但这个程序还是有一个安全漏洞，即如果客户端故意不断发生数据，但从不接收，那么服务端的发送缓冲区会一直堆积，导致内存暴涨。解决办法可以参考下面的 chargen 协议。 五、chargenChargen 协议很特殊，它只发送数据，不接收数据。而且，它发送数据的速度不能快过客户端接收的速度，因此需要关注“三个半事件”中的半个“消息/数据发送完毕”事件(onWriteComplete)。 serverchargen.h 12345678910111213141516171819202122232425class ChargenServer &#123;public: ChargenServer(muduo::net::EventLoop *loop, const muduo::net::InetAddress &amp;listenAddr, bool print = false); void start();private: void onConnection(const muduo::net::TcpConnectionPtr &amp;conn); void onMessage(const muduo::net::TcpConnectionPtr &amp;conn, muduo::net::Buffer *buf, muduo::Timestamp time); void onWriteComplete(const muduo::net::TcpConnectionPtr &amp;conn); void printThroughput(); muduo::net::TcpServer server_; muduo::string message_; int64_t transferred_; muduo::Timestamp startTime_;&#125;; chargen.cc 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162ChargenServer::ChargenServer(EventLoop *loop, const InetAddress &amp;listenAddr, bool print) : server_(loop, listenAddr, \"ChargenServer\"), transferred_(0), startTime_(Timestamp::now()) &#123; server_.setConnectionCallback( std::bind(&amp;ChargenServer::onConnection, this, _1)); server_.setMessageCallback( std::bind(&amp;ChargenServer::onMessage, this, _1, _2, _3)); server_.setWriteCompleteCallback( std::bind(&amp;ChargenServer::onWriteComplete, this, _1)); if (print) &#123; loop-&gt;runEvery(3.0, std::bind(&amp;ChargenServer::printThroughput, this)); &#125; string line; for (int i = 33; i &lt; 127; ++i) &#123; line.push_back(char(i)); &#125; line += line; for (size_t i = 0; i &lt; 127 - 33; ++i) &#123; message_ += line.substr(i, 72) + '\\n'; &#125;&#125;void ChargenServer::start() &#123; server_.start();&#125;void ChargenServer::onConnection(const TcpConnectionPtr &amp;conn) &#123; LOG_INFO &lt;&lt; \"ChargenServer - \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); if (conn-&gt;connected()) &#123; // 在连接建立时发生第一次数据 conn-&gt;setTcpNoDelay(true); conn-&gt;send(message_); &#125;&#125;void ChargenServer::onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp time) &#123; string msg(buf-&gt;retrieveAllAsString()); LOG_INFO &lt;&lt; conn-&gt;name() &lt;&lt; \" discards \" &lt;&lt; msg.size() &lt;&lt; \" bytes received at \" &lt;&lt; time.toString();&#125;void ChargenServer::onWriteComplete(const TcpConnectionPtr &amp;conn) &#123; // 继续发送数据 transferred_ += message_.size(); conn-&gt;send(message_);&#125;void ChargenServer::printThroughput() &#123; Timestamp endTime = Timestamp::now(); double time = timeDifference(endTime, startTime_); printf(\"%4.3f MiB/s\\n\", static_cast&lt;double&gt;(transferred_) / time / 1024 / 1024); transferred_ = 0; startTime_ = endTime;&#125; clientchargenclient.cc 123456789101112131415161718192021222324252627282930313233class ChargenClient : noncopyable &#123;public: ChargenClient(EventLoop *loop, const InetAddress &amp;listenAddr) : loop_(loop), client_(loop, listenAddr, \"ChargenClient\") &#123; client_.setConnectionCallback( std::bind(&amp;ChargenClient::onConnection, this, _1)); client_.setMessageCallback( std::bind(&amp;ChargenClient::onMessage, this, _1, _2, _3)); // client_.enableRetry(); &#125; void connect() &#123; client_.connect(); &#125;private: void onConnection(const TcpConnectionPtr &amp;conn) &#123; LOG_INFO &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); if (!conn-&gt;connected()) loop_-&gt;quit(); &#125; void onMessage(const TcpConnectionPtr &amp;conn, Buffer *buf, Timestamp receiveTime) &#123; buf-&gt;retrieveAll(); &#125; EventLoop *loop_; TcpClient client_;&#125;; 六、Five in one前面五个程序都用到了 EventLoop，这其实是个 Reactor，用于注册和分发 IO 事件。Muduo 遵循 one loop per thread 模型，多个服务端(TcpServer)和客户端(TcpClient)可以共享同一个 EventLoop，也可以分配到多个 EventLoop 上以发挥多核多线程的好处。这里我们把五个服务端用同一个 EventLoop 跑起来，程序还是单线程的，功能却强大了很多： 123456789101112131415161718192021int main() &#123; LOG_INFO &lt;&lt; \"pid = \" &lt;&lt; getpid(); EventLoop loop; // one loop shared by multiple servers ChargenServer chargenServer(&amp;loop, InetAddress(2019)); chargenServer.start(); DaytimeServer daytimeServer(&amp;loop, InetAddress(2013)); daytimeServer.start(); DiscardServer discardServer(&amp;loop, InetAddress(2009)); discardServer.start(); EchoServer echoServer(&amp;loop, InetAddress(2007)); echoServer.start(); TimeServer timeServer(&amp;loop, InetAddress(2037)); timeServer.start(); loop.loop();&#125; 以上几个协议的消息格式都非常简单，没有涉及 TCP 网络编程中常见的分包处理，在下一篇文章讲 Boost.Asio 的聊天服务器时我们再来讨论这个问题。","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"TCP","slug":"TCP","permalink":"http://yoursite.com/tags/TCP/"}]},{"title":"muduo 中的 net 组件","slug":"muduo中的net组件","date":"2020-03-09T02:57:01.062Z","updated":"2020-03-09T02:57:36.000Z","comments":true,"path":"开源组件/muduo/muduo中的net组件/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/muduo%E4%B8%AD%E7%9A%84net%E7%BB%84%E4%BB%B6/","excerpt":"","text":"类之间的耦合关系 最底层 Buffer：√ Socket：√ Timer：√ Channel ：和 EventLoop 相互耦合 √ 第二层 TimerId ：依赖 Timer √ Poller ：依赖 Channel，和 EventLoop 相互耦合 √ Connector ：依赖 Channel，和 EventLoop 相互耦合 √ Acceptor ：依赖 Channel 和 Socket，和 EventLoop 相互耦合 √ TimerQueue ：依赖 Timer 和 Channel，和 EventLoop 相互耦合 √ 第三层 EventLoop ：和 Channel、Poller 和 TimerQueue 相互耦合 √ 第四层 EventLoopThread ：依赖 EventLoop √ TcpConnection ：依赖 EventLoop、Socket、Channel 和 Buffer √ 第五层 EventLoopThreadPool ：依赖 EventLoopThread √ TcpServer ：依赖 EventLoop、Acceptor、EventLoopThread 和 TcpConnection √ TcpClient ：依赖 EventLoop 、 Connector 和 TcpConnection √ 最底层 1. Buffer 2. Socket 3. Timer 4. Channel 第二层 1. TimerId 2. Poller 3. Acceptor 4. TimerQueue 5. Connector 第三层 1. EventLoop 第四层 1. EventLoopThread 2. TcpConnection 第五层 1. EventLoopThreadPool 2. TcpServer 3. TcpClient 最底层1. Buffer见 “./5_Buffer类的设计.md” 2. Socketsocket_fd 的 wrapper，目标是提供了几种 Socket 的封装，例如 listen、bind 等。 12345678910111213141516171819202122namespace muduo &#123; namespace net &#123; class InetAddress; class Socket : noncopyable &#123; public: explicit Socket(int sockfd) : sockfd_(sockfd) &#123;&#125; void bindAddress(const InetAddress &amp;localaddr); void listen(); int accept(InetAddress *peeraddr); ... private: const int sockfd_; &#125;; &#125; // namespace net&#125; // namespace muduo 3. Timer用于时间事件的内部类 1234567891011121314151617181920212223242526272829303132333435363738394041namespace muduo &#123; namespace net &#123; class Timer : noncopyable &#123; public: Timer(TimerCallback cb, Timestamp when, double interval) : callback_(std::move(cb)), expiration_(when), interval_(interval), repeat_(interval &gt; 0.0), sequence_(s_numCreated_.incrementAndGet()) &#123;&#125; void run() const &#123; callback_(); &#125; void restart(Timestamp now) &#123; if (repeat_) &#123; expiration_ = addTime(now, interval_); &#125; else &#123; expiration_ = Timestamp::invalid(); &#125; &#125; Timestamp expiration() const; bool repeat() const; int64_t sequence() const; static int64_t numCreated(); private: const TimerCallback callback_; Timestamp expiration_; const double interval_; const bool repeat_; const int64_t sequence_; static AtomicInt64 s_numCreated_; &#125;; &#125; // namespace net&#125; // namespace muduo 4. Channel用于注册与响应 IO 事件 构造函数中和一个 EventLoop 绑定、和一个文件描述符 fd 绑定 123456789101112namespace muduo &#123; namespace net &#123; class EventLoop; class Channel : noncopyable &#123; public: Channel(EventLoop *loop, int fd); ... &#125;; &#125; // namespace net&#125; // namespace muduo IO 事件类型分为两种：ReadEventCallback 和 EventCallback；有四种回调函数类型：readCallback_、writeCallback_、closeCallback_ 和 errorCallback_，可以通过 setXXX 函数来注册（注意 std::move() 将左值转化为了右值引用以减少拷贝次数） 12345678910111213141516171819202122232425262728namespace muduo &#123; namespace net &#123; class Channel : noncopyable &#123; public: typedef std::function&lt;void()&gt; EventCallback; typedef std::function&lt;void(Timestamp)&gt; ReadEventCallback; void setReadCallback(ReadEventCallback cb) &#123; readCallback_ = std::move(cb); &#125; void setWriteCallback(EventCallback cb) &#123; writeCallback_ = std::move(cb); &#125; void setCloseCallback(EventCallback cb) &#123; closeCallback_ = std::move(cb); &#125; void setErrorCallback(EventCallback cb) &#123; errorCallback_ = std::move(cb); &#125; ... private: ReadEventCallback readCallback_; EventCallback writeCallback_; EventCallback closeCallback_; EventCallback errorCallback_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 设置事件读写能力，通过对 events_ 变量设置掩码完成，并且通知绑定的 EventLoop update 1234567891011121314151617181920212223242526272829303132333435363738394041424344namespace muduo &#123; namespace net &#123; class Channel : noncopyable &#123; public: void enableReading() &#123; events_ |= kReadEvent; update(); &#125; void disableReading() &#123; events_ &amp;= ~kReadEvent; update(); &#125; void enableWriting() &#123; events_ |= kWriteEvent; update(); &#125; void disableWriting() &#123; events_ &amp;= ~kWriteEvent; update(); &#125; void disableAll() &#123; events_ = kNoneEvent; update(); &#125; ... private: static const int kNoneEvent = 0; static const int kReadEvent = POLLIN | POLLPRI; static const int kWriteEvent = POLLOUT; int events_; update() &#123; addedToLoop_ = true; loop_-&gt;updateChannel(this); &#125; ... &#125;; &#125; // namespace net&#125; // namespace muduo 处理事件 revents_ 变量由 Poller 设置，表征事件的类型；处理事件时根据事件类型，调用注册好的几种回调函数 123456789101112131415161718192021222324252627282930313233void Channel::handleEventWithGuard(Timestamp receiveTime) &#123; eventHandling_ = true; LOG_TRACE &lt;&lt; reventsToString(); if ((revents_ &amp; POLLHUP) &amp;&amp; !(revents_ &amp; POLLIN)) &#123; if (logHup_) &#123; LOG_WARN &lt;&lt; \"fd = \" &lt;&lt; fd_ &lt;&lt; \" Channel::handle_event() POLLHUP\"; &#125; if (closeCallback_) &#123; closeCallback_(); &#125; &#125; if (revents_ &amp; POLLNVAL) &#123; LOG_WARN &lt;&lt; \"fd = \" &lt;&lt; fd_ &lt;&lt; \" Channel::handle_event() POLLNVAL\"; &#125; if (revents_ &amp; (POLLERR | POLLNVAL)) &#123; if (errorCallback_) &#123; errorCallback_(); &#125; &#125; if (revents_ &amp; (POLLIN | POLLPRI | POLLRDHUP)) &#123; if (readCallback_) &#123; readCallback_(receiveTime); &#125; &#125; if (revents_ &amp; POLLOUT) &#123; if (writeCallback_) &#123; writeCallback_(); &#125; &#125; eventHandling_ = false;&#125; 第二层1. TimerId用于取消 Timer 123456789101112131415161718192021222324namespace muduo &#123; namespace net &#123; class Timer; class TimerId : public muduo::copyable &#123; public: TimerId() : timer_(NULL), sequence_(0) &#123;&#125; TimerId(Timer *timer, int64_t seq) : timer_(timer), sequence_(seq) &#123; &#125; // default copy-ctor, dtor and assignment are okay friend class TimerQueue; private: Timer *timer_; int64_t sequence_; &#125;; &#125; // namespace net&#125; // namespace muduo 2. Poller IO 多路复用的虚基类，和一个 EventLoop 对象绑定，poll 方法必须在 loop 线程中调用。 一个 Poller 对象中可以包含多个 Channel，代表多个 IO 事件 有两个实现类：PollPoller 底层使用 poll；EPollPoller 底层使用 epoll 123456789101112131415161718192021222324252627282930313233343536373839404142namespace muduo &#123; namespace net &#123; class Channel; class Poller : noncopyable &#123; public: typedef std::vector&lt;Channel *&gt; ChannelList; Poller(EventLoop *loop); virtual ~Poller(); /// Polls the I/O events. /// Must be called in the loop thread. virtual Timestamp poll(int timeoutMs, ChannelList *activeChannels) = 0; /// Changes the interested I/O events. /// Must be called in the loop thread. virtual void updateChannel(Channel *channel) = 0; /// Remove the channel, when it destructs. /// Must be called in the loop thread. virtual void removeChannel(Channel *channel) = 0; virtual bool hasChannel(Channel *channel) const; static Poller *newDefaultPoller(EventLoop *loop); void assertInLoopThread() const &#123; ownerLoop_-&gt;assertInLoopThread(); &#125; protected: typedef std::map&lt;int, Channel *&gt; ChannelMap; ChannelMap channels_; private: EventLoop *ownerLoop_; &#125;; &#125; // namespace net&#125; // namespace muduo 3. Acceptor用于接受 TCP 连接 构造函数 12345678910111213141516171819202122232425262728293031namespace muduo &#123; namespace net &#123; class Acceptor : noncopyable &#123; public: Acceptor(EventLoop *loop, const InetAddress &amp;listenAddr, bool reuseport) : loop_(loop), acceptSocket_(sockets::createNonblockingOrDie(listenAddr.family())), acceptChannel_(loop, acceptSocket_.fd()), listenning_(false), idleFd_(::open(\"/dev/null\", O_RDONLY | O_CLOEXEC)) &#123; assert(idleFd_ &gt;= 0); acceptSocket_.setReuseAddr(true); acceptSocket_.setReusePort(reuseport); acceptSocket_.bindAddress(listenAddr); acceptChannel_.setReadCallback( std::bind(&amp;Acceptor::handleRead, this)); &#125; private: EventLoop *loop_; Socket acceptSocket_; Channel acceptChannel_; NewConnectionCallback newConnectionCallback_; bool listenning_; int idleFd_; &#125;; &#125; // namespace net&#125; // namespace muduo 每个 Acceptor 和一个 EventLoop 绑定，并且传入一个 InetAddress 对象用于构造 Socket 监听的端口。 在构造函数中，acceptSocket_ 对象初始化好，acceptChannel_ 也初始化好，并且注册 Read 事件的 CallBack 为 Acceptor::handleRead。 Acceptor::handleRead 的代码是： 123456789101112131415161718192021222324252627void Acceptor::handleRead() &#123; loop_-&gt;assertInLoopThread(); InetAddress peerAddr; //FIXME loop until no more int connfd = acceptSocket_.accept(&amp;peerAddr); if (connfd &gt;= 0) &#123; // string hostport = peerAddr.toIpPort(); // LOG_TRACE &lt;&lt; \"Accepts of \" &lt;&lt; hostport; if (newConnectionCallback_) &#123; newConnectionCallback_(connfd, peerAddr); &#125; else &#123; sockets::close(connfd); &#125; &#125; else &#123; LOG_SYSERR &lt;&lt; \"in Acceptor::handleRead\"; // Read the section named \"The special problem of // accept()ing when you can't\" in libev's doc. // By Marc Lehmann, author of libev. if (errno == EMFILE) &#123; ::close(idleFd_); idleFd_ = ::accept(acceptSocket_.fd(), NULL, NULL); ::close(idleFd_); idleFd_ = ::open(\"/dev/null\", O_RDONLY | O_CLOEXEC); &#125; &#125;&#125; 也就是说，一旦 acceptChannel_ 产生了 read 事件，它就会调用 Acceptor 注册好的 newConnectionCallback_ 回调函数。 listen 让 acceptSocket_ 开始 listen 某个端口（这个不是阻塞的），然后打开 acceptChannel_ 的读事件能力。 123456void Acceptor::listen() &#123; loop_-&gt;assertInLoopThread(); listenning_ = true; acceptSocket_.listen(); acceptChannel_.enableReading();&#125; 4. TimerQueue一个 Timer 队列，用于管理定时事件 构造函数 1234567891011121314151617181920212223242526272829303132333435namespace muduo &#123; namespace net &#123; class TimerQueue : noncopyable &#123; public: explicit TimerQueue(EventLoop *loop): loop_(loop), timerfd_(createTimerfd()), timerfdChannel_(loop, timerfd_), timers_(), callingExpiredTimers_(false) &#123; timerfdChannel_.setReadCallback(std::bind(&amp;TimerQueue::handleRead, this)); timerfdChannel_.enableReading(); &#125; ... private: typedef std::pair&lt;Timestamp, Timer *&gt; Entry; typedef std::set&lt;Entry&gt; TimerList; EventLoop *loop_; const int timerfd_; Channel timerfdChannel_; // Timer list sorted by expiration TimerList timers_; bool callingExpiredTimers_; /* atomic */ ... &#125;; &#125; // namespace net&#125; // namespace muduo 在构造函数中，首先将一个 TimerQueue 和一个 EventLoop 绑定，然后创建一个 timer 的 fd，然后用这个 fd 构造一个 timerfdChannel_。这个 timerfdChannel_ 会设置一个 Read 事件的 CallBack 回调函数，看一下这个回调函数 1234567891011121314151617void TimerQueue::handleRead() &#123; loop_-&gt;assertInLoopThread(); Timestamp now(Timestamp::now()); readTimerfd(timerfd_, now); std::vector&lt;Entry&gt; expired = getExpired(now); callingExpiredTimers_ = true; cancelingTimers_.clear(); // safe to callback outside critical section for (const Entry &amp;it : expired) &#123; it.second-&gt;run(); &#125; callingExpiredTimers_ = false; reset(expired, now);&#125; 这个回调函数的大致原理是：如果 timerfdChannel_ 变得可读，说明有时间事件到达了；此时找一下过期的 timer 们，执行一下 timer 的 callback 函数 addTimer 添加一个 Timer。 12345678910111213141516namespace muduo &#123; namespace net &#123; class TimerQueue : noncopyable &#123; public: TimerId addTimer(TimerCallback cb, Timestamp when, double interval) &#123; Timer *timer = new Timer(std::move(cb), when, interval); loop_-&gt;runInLoop(std::bind(&amp;TimerQueue::addTimerInLoop, this, timer)); return TimerId(timer, timer-&gt;sequence()); &#125; ... &#125;; 具体方式是先 new 一个 Timer 对象，然后向绑定的 EventLoop 对象添加。 1234567void EventLoop::runInLoop(Functor cb) &#123; if (isInLoopThread()) &#123; cb(); &#125; else &#123; queueInLoop(std::move(cb)); &#125;&#125; EventLoop 要么直接执行，要么稍后执行（对于添加 Timer 这个行为，很可能发生在其他线程），但是无论如何最终都会执行 addTimerInLoop 方法： 12345678void TimerQueue::addTimerInLoop(Timer *timer) &#123; loop_-&gt;assertInLoopThread(); bool earliestChanged = insert(timer); if (earliestChanged) &#123; resetTimerfd(timerfd_, timer-&gt;expiration()); &#125;&#125; 细节没有关注，整体思路就是更新 timer 列表 cancel 12345678910111213namespace muduo &#123; namespace net &#123; class TimerQueue : noncopyable &#123; public: void cancel(TimerId timerId) &#123; loop_-&gt;runInLoop(std::bind(&amp;TimerQueue::cancelInLoop, this, timerId)); &#125; ... &#125; &#125; // namespace net&#125; // namespace muduo 和 addTimer 一样，最终也是到 EventLoop 中执行，接下来看 cancelInLoop 12345678910111213141516void TimerQueue::cancelInLoop(TimerId timerId) &#123; loop_-&gt;assertInLoopThread(); assert(timers_.size() == activeTimers_.size()); ActiveTimer timer(timerId.timer_, timerId.sequence_); ActiveTimerSet::iterator it = activeTimers_.find(timer); if (it != activeTimers_.end()) &#123; size_t n = timers_.erase(Entry(it-&gt;first-&gt;expiration(), it-&gt;first)); assert(n == 1); (void) n; delete it-&gt;first; // FIXME: no delete please activeTimers_.erase(it); &#125; else if (callingExpiredTimers_) &#123; cancelingTimers_.insert(timer); &#125; assert(timers_.size() == activeTimers_.size());&#125; 基本思路也是更新 TimerQueue 中的 Timer 列表 5. Connector属于 TcpClient 的底层组件，用于发起 TCP 连接，它是 TcpClient 的成员，生命期由后者控制。 构造函数 12345678910111213141516171819202122232425namespace muduo &#123; namespace net &#123; class Connector : noncopyable, public std::enable_shared_from_this&lt;Connector&gt; &#123; public: Connector(EventLoop *loop, const InetAddress &amp;serverAddr) : loop_(loop), serverAddr_(serverAddr), connect_(false), state_(kDisconnected), retryDelayMs_(kInitRetryDelayMs) &#123; &#125; ... private: static const int kInitRetryDelayMs = 500; EventLoop *loop_; InetAddress serverAddr_; bool connect_; // atomic States state_; // FIXME: use atomic variable int retryDelayMs_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 绑定一个 EventLoop，指定一下 server 的地址和端口 start 可以在任何线程中调用 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253void Connector::start() &#123; connect_ = true; loop_-&gt;runInLoop(std::bind(&amp;Connector::startInLoop, this)); &#125;void Connector::startInLoop() &#123; loop_-&gt;assertInLoopThread(); assert(state_ == kDisconnected); if (connect_) &#123; connect(); &#125; else &#123; LOG_DEBUG &lt;&lt; \"do not connect\"; &#125;&#125;void Connector::connect() &#123; int sockfd = sockets::createNonblockingOrDie(serverAddr_.family()); int ret = sockets::connect(sockfd, serverAddr_.getSockAddr()); int savedErrno = (ret == 0) ? 0 : errno; switch (savedErrno) &#123; case 0: case EINPROGRESS: case EINTR: case EISCONN: connecting(sockfd); break; case EAGAIN: case EADDRINUSE: case EADDRNOTAVAIL: case ECONNREFUSED: case ENETUNREACH: retry(sockfd); break; case EACCES: case EPERM: case EAFNOSUPPORT: case EALREADY: case EBADF: case EFAULT: case ENOTSOCK: LOG_SYSERR &lt;&lt; \"connect error in Connector::startInLoop \" &lt;&lt; savedErrno; sockets::close(sockfd); break; default: LOG_SYSERR &lt;&lt; \"Unexpected error in Connector::startInLoop \" &lt;&lt; savedErrno; sockets::close(sockfd); // connectErrorCallback_(); break; &#125;&#125; start 方法从客户端开启一个 TCP 连接。基本做法是把 connect 放在 EventLoop 中执行，最终会开启 socket::connect 在 connect 方法中有很多种情况，最终归结为 3 大类：connecting、retry 和 close，close 就是 socket::close 就行了，重点看一下前两个。 先看 retry，retry 的操作是先把原来的 sockfd close 掉，然后设一个定时任务 runAfter，而这个延时时延每次翻倍，直到到达最大阈值。 12345678910111213void Connector::retry(int sockfd) &#123; sockets::close(sockfd); setState(kDisconnected); if (connect_) &#123; LOG_INFO &lt;&lt; \"Connector::retry - Retry connecting to \" &lt;&lt; serverAddr_.toIpPort() &lt;&lt; \" in \" &lt;&lt; retryDelayMs_ &lt;&lt; \" milliseconds. \"; loop_-&gt;runAfter(retryDelayMs_ / 1000.0, std::bind(&amp;Connector::startInLoop, shared_from_this())); retryDelayMs_ = std::min(retryDelayMs_ * 2, kMaxRetryDelayMs); &#125; else &#123; LOG_DEBUG &lt;&lt; \"do not connect\"; &#125;&#125; 另一种是 connecting，能进入 connecting 说明 connect 阻塞过程已经结束，接下来正式进入通讯过程，因此设置 channel 的两个回调。 1234567891011void Connector::connecting(int sockfd) &#123; setState(kConnecting); assert(!channel_); channel_.reset(new Channel(loop_, sockfd)); channel_-&gt;setWriteCallback( std::bind(&amp;Connector::handleWrite, this)); channel_-&gt;setErrorCallback( std::bind(&amp;Connector::handleError, this)); channel_-&gt;enableWriting();&#125; 首先看一下 WriteCallback，基本就是执行一下设置好的 newConnectionCallback_ 1234567891011121314151617181920212223242526void Connector::handleWrite() &#123; LOG_TRACE &lt;&lt; \"Connector::handleWrite \" &lt;&lt; state_; if (state_ == kConnecting) &#123; int sockfd = removeAndResetChannel(); int err = sockets::getSocketError(sockfd); if (err) &#123; LOG_WARN &lt;&lt; \"Connector::handleWrite - SO_ERROR = \" &lt;&lt; err &lt;&lt; \" \" &lt;&lt; strerror_tl(err); retry(sockfd); &#125; else if (sockets::isSelfConnect(sockfd)) &#123; LOG_WARN &lt;&lt; \"Connector::handleWrite - Self connect\"; retry(sockfd); &#125; else &#123; setState(kConnected); if (connect_) &#123; newConnectionCallback_(sockfd); &#125; else &#123; sockets::close(sockfd); &#125; &#125; &#125; else &#123; // what happened? assert(state_ == kDisconnected); &#125;&#125; 再看一下 ErrorCallback，基本操作就是 log 然后 retry。 123456789void Connector::handleError() &#123; LOG_ERROR &lt;&lt; \"Connector::handleError state=\" &lt;&lt; state_; if (state_ == kConnecting) &#123; int sockfd = removeAndResetChannel(); int err = sockets::getSocketError(sockfd); LOG_TRACE &lt;&lt; \"SO_ERROR = \" &lt;&lt; err &lt;&lt; \" \" &lt;&lt; strerror_tl(err); retry(sockfd); &#125;&#125; restart restart 只能在 loop thread 中被调用，目标是重新设置各项 connector 参数，然后重启这个 connector 1234567void Connector::restart() &#123; loop_-&gt;assertInLoopThread(); setState(kDisconnected); retryDelayMs_ = kInitRetryDelayMs; connect_ = true; startInLoop();&#125; 第三层1. EventLoop封装事件循环，也是事件分派的中心。它用 TimerQueue 作为计时器管理，用 Poller 作为 IO Multiplexing。 构造函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950namespace muduo &#123; namespace net &#123; class EventLoop : noncopyable &#123; public: EventLoop() : looping_(false), quit_(false), eventHandling_(false), callingPendingFunctors_(false), iteration_(0), threadId_(CurrentThread::tid()), poller_(Poller::newDefaultPoller(this)), timerQueue_(new TimerQueue(this)), wakeupFd_(createEventfd()), wakeupChannel_(new Channel(this, wakeupFd_)), currentActiveChannel_(NULL) &#123; LOG_DEBUG &lt;&lt; \"EventLoop created \" &lt;&lt; this &lt;&lt; \" in thread \" &lt;&lt; threadId_; if (t_loopInThisThread) &#123; LOG_FATAL &lt;&lt; \"Another EventLoop \" &lt;&lt; t_loopInThisThread &lt;&lt; \" exists in this thread \" &lt;&lt; threadId_; &#125; else &#123; t_loopInThisThread = this; &#125; wakeupChannel_-&gt;setReadCallback(std::bind(&amp;EventLoop::handleRead, this)); wakeupChannel_-&gt;enableReading(); &#125; ... private: bool looping_; /* atomic */ std::atomic&lt;bool&gt; quit_; bool eventHandling_; /* atomic */ bool callingPendingFunctors_; /* atomic */ int64_t iteration_; const pid_t threadId_; std::unique_ptr&lt;Poller&gt; poller_; std::unique_ptr&lt;TimerQueue&gt; timerQueue_; int wakeupFd_; std::unique_ptr&lt;Channel&gt; wakeupChannel_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 在构造函数中，构造了 Poller 和 TimerQueue。 另外就是构造了 wakeupChannel_，这个 channel 的 fd 是 eventfd，注册的 read 回调函数是下面这样，目前不知道这个 channel 是干啥的 1234567void EventLoop::handleRead() &#123; uint64_t one = 1; ssize_t n = sockets::read(wakeupFd_, &amp;one, sizeof one); if (n != sizeof one) &#123; LOG_ERROR &lt;&lt; \"EventLoop::handleRead() reads \" &lt;&lt; n &lt;&lt; \" bytes instead of 8\"; &#125;&#125; Channel 相关 一共有 3 个相关的函数 12345678910111213141516171819namespace muduo &#123; namespace net &#123; class EventLoop : noncopyable &#123; public: void updateChannel(Channel *channel); void removeChannel(Channel *channel); bool hasChannel(Channel *channel); ... private: std::unique_ptr&lt;Poller&gt; poller_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 这几个函数的本质是向内部的 poller_ 对象注册或注销 123456789101112131415161718192021void EventLoop::updateChannel(Channel *channel) &#123; assert(channel-&gt;ownerLoop() == this); assertInLoopThread(); poller_-&gt;updateChannel(channel);&#125;void EventLoop::removeChannel(Channel *channel) &#123; assert(channel-&gt;ownerLoop() == this); assertInLoopThread(); if (eventHandling_) &#123; assert(currentActiveChannel_ == channel || std::find(activeChannels_.begin(), activeChannels_.end(), channel) == activeChannels_.end()); &#125; poller_-&gt;removeChannel(channel);&#125;bool EventLoop::hasChannel(Channel *channel) &#123; assert(channel-&gt;ownerLoop() == this); assertInLoopThread(); return poller_-&gt;hasChannel(channel);&#125; Timer 相关 一共有 4 个相关方法： 123456789101112131415161718192021namespace muduo &#123; namespace net &#123; class EventLoop : noncopyable &#123; public: TimerId runAt(Timestamp time, TimerCallback cb); TimerId runAfter(double delay, TimerCallback cb); TimerId runEvery(double interval, TimerCallback cb); void cancel(TimerId timerId); ... private: std::unique_ptr&lt;TimerQueue&gt; timerQueue_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 这几个方法的目标是注册定时回调函数或取消，实现靠的是内部的 TimerQueue 对象： 1234567891011121314151617TimerId EventLoop::runAt(Timestamp time, TimerCallback cb) &#123; return timerQueue_-&gt;addTimer(std::move(cb), time, 0.0);&#125;TimerId EventLoop::runAfter(double delay, TimerCallback cb) &#123; Timestamp time(addTime(Timestamp::now(), delay)); return runAt(time, std::move(cb));&#125;TimerId EventLoop::runEvery(double interval, TimerCallback cb) &#123; Timestamp time(addTime(Timestamp::now(), interval)); return timerQueue_-&gt;addTimer(std::move(cb), time, interval);&#125;void EventLoop::cancel(TimerId timerId) &#123; return timerQueue_-&gt;cancel(timerId);&#125; loop EventLoop.h 123456789101112131415161718192021222324namespace muduo &#123; namespace net &#123; class EventLoop : noncopyable &#123; public: typedef std::function&lt;void()&gt; Functor; void loop(); ... private: bool looping_; /* atomic */ std::atomic&lt;bool&gt; quit_; int64_t iteration_; std::unique_ptr&lt;Poller&gt; poller_; // scratch variables ChannelList activeChannels_; Channel *currentActiveChannel_; ... &#125;; &#125; // namespace net&#125; // namespace muduo EventLoop.cc 12345678910111213141516171819202122232425262728293031323334const int kPollTimeMs = 10000;void EventLoop::loop() &#123; assert(!looping_); assertInLoopThread(); looping_ = true; quit_ = false; LOG_TRACE &lt;&lt; \"EventLoop \" &lt;&lt; this &lt;&lt; \" start looping\"; while (!quit_) &#123; activeChannels_.clear(); pollReturnTime_ = poller_-&gt;poll(kPollTimeMs, &amp;activeChannels_); ++iteration_; if (Logger::logLevel() &lt;= Logger::TRACE) &#123; printActiveChannels(); &#125; // TODO sort channel by priority eventHandling_ = true; for (Channel *channel : activeChannels_) &#123; currentActiveChannel_ = channel; currentActiveChannel_-&gt;handleEvent(pollReturnTime_); &#125; currentActiveChannel_ = NULL; eventHandling_ = false; doPendingFunctors(); &#125; LOG_TRACE &lt;&lt; \"EventLoop \" &lt;&lt; this &lt;&lt; \" stop looping\"; looping_ = false;&#125; loop 方法是一个死循环，它的主体是让 poller_ 监听一段时间的 IO 事件，然后调用已经注册好的这些 Channel 的各种 CallBack 函数进行处理；处理完这些事件以后，每一次循环会 doPendingFunctors 一次，看一下这个方法： 1234567891011121314void EventLoop::doPendingFunctors() &#123; std::vector&lt;Functor&gt; functors; callingPendingFunctors_ = true; &#123; MutexLockGuard lock(mutex_); functors.swap(pendingFunctors_); &#125; for (const Functor &amp;functor : functors) &#123; functor(); &#125; callingPendingFunctors_ = false;&#125; 这个函数的作用是有一些未执行的方法 pendingFunctors，在这个里面依次执行一下，pendingFunctors_ 是在以下的函数中被更新的： 123456789101112131415161718void EventLoop::runInLoop(Functor cb) &#123; if (isInLoopThread()) &#123; cb(); &#125; else &#123; queueInLoop(std::move(cb)); &#125;&#125;void EventLoop::queueInLoop(Functor cb) &#123; &#123; MutexLockGuard lock(mutex_); pendingFunctors_.push_back(std::move(cb)); &#125; if (!isInLoopThread() || callingPendingFunctors_) &#123; wakeup(); &#125;&#125; 外部调用的接口是 runInLoop，如果 cb 没有被立刻执行，那么它就会加入到 pendingFunctors_ 中，等待在每次循环中执行掉。 事件包括了两部分：刚才说的是 IO 事件，还有一部分是 Timer 定时的事件，Timer 事件其实也跟 doPendingFunctors 有关： 123456789101112TimerId EventLoop::runAt(Timestamp time, TimerCallback cb) &#123; return timerQueue_-&gt;addTimer(std::move(cb), time, 0.0);&#125;TimerId TimerQueue::addTimer(TimerCallback cb, Timestamp when, double interval) &#123; Timer *timer = new Timer(std::move(cb), when, interval); loop_-&gt;runInLoop( std::bind(&amp;TimerQueue::addTimerInLoop, this, timer)); return &#123;timer, timer-&gt;sequence()&#125;;&#125; 可以看到，在 addTimer 时，本质上也把回调函数加到了 EventLoop 的 runInLoop 中，只不过回调函数是 TimerQueue::addTimerInLoop。 Timer 一旦注册好以后，跟 IO 事件一样，因为 timer 本身也有一个 timerfd，到时以后会唤醒，本质上还是一个 fd 的 IO。 TimerQueue 和 刚才的 wakeupChannel_ 的本质其实是一样的，TimerQueue 内部也有一个 Channel，对应一个 timerfd TimerQueue.h 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152namespace muduo &#123; namespace net &#123; class TimerQueue : noncopyable &#123; private: const int timerfd_; Channel timerfdChannel_; ... &#125;; &#125; // namespace net&#125; // namespace muduonamespace muduo &#123; namespace net &#123; namespace detail &#123; int createTimerfd() &#123; int timerfd = ::timerfd_create(CLOCK_MONOTONIC, TFD_NONBLOCK | TFD_CLOEXEC); if (timerfd &lt; 0) &#123; LOG_SYSFATAL &lt;&lt; \"Failed in timerfd_create\"; &#125; return timerfd; &#125; ... &#125; // namespace detail &#125; // namespace net&#125; // namespace muduo``` `EventLoop.h````cppnamespace muduo &#123; namespace net &#123; class EventLoop : noncopyable &#123; private: int wakeupFd_; std::unique_ptr&lt;Channel&gt; wakeupChannel_; ... &#125;; &#125; // namespace net&#125; // namespace muduonamespace &#123; int createEventfd() &#123; int evtfd = ::eventfd(0, EFD_NONBLOCK | EFD_CLOEXEC); if (evtfd &lt; 0) &#123; LOG_SYSERR &lt;&lt; \"Failed in eventfd\"; abort(); &#125; return evtfd; &#125; 它们的底层都是一个系统调用，各产生一个 fd：timerfd 产生的是定时事件，eventfd 产生的事件，目标是让 poll 快速返回：向 eventfd 写入一个字节，就会产生一个可读事件，从而实现 poll 阻塞方法的快速返回。 无论是 TimerQueue 中的 timerfdChannel_，还是 EventLoop 中的 wakeupChannel_，都需要向 poller 中注册从而监听，这部分代码藏的比较深，在 Channel 的 enableReading 里面： 123456789101112131415void Channel::enableReading() &#123; events_ |= kReadEvent; update();&#125;void Channel::update() &#123; addedToLoop_ = true; loop_-&gt;updateChannel(this);&#125;void EventLoop::updateChannel(Channel *channel) &#123; assert(channel-&gt;ownerLoop() == this); assertInLoopThread(); poller_-&gt;updateChannel(channel);&#125; 套了好几层，最终还是向 poller 注册上了。。。 总之，这个 loop 方法是 Reactor 模式的核心：一个 loop 一个 Thread，先向 poller 注册好要监听的 Channel，Channel 又包括了 3 种类型：要监听的 IO 端口、timerfd 抽象成的时间事件 Channel 和 为了快速推出阻塞而额外留好的 eventfd 抽象成的 Channel。产生事件以后，poller 退出阻塞，调用各个 channel 上注册好的处理回调函数。另外，在 loop 期间其他线程加入的方法，先 pending，然后一并处理一下。 quit 12345678910111213141516171819202122232425namespace muduo &#123; namespace net &#123; class EventLoop : noncopyable &#123; public: void quit() &#123; quit_ = true; if (!isInLoopThread()) &#123; wakeup(); &#125; &#125; void wakeup() &#123; uint64_t one = 1; ssize_t n = sockets::write(wakeupFd_, &amp;one, sizeof one); if (n != sizeof one) &#123; LOG_ERROR &lt;&lt; \"EventLoop::wakeup() writes \" &lt;&lt; n &lt;&lt; \" bytes instead of 8\"; &#125; &#125; ... &#125;; &#125; // namespace net&#125; // namespace muduo quit 函数的目标是结束整个 loop：如果调用者就在当前线程，那么直接把 while 循环开始时的 quit 变量置为 true 即可；反之，为了快速退出 poller 的阻塞，向提前预留好的 wakeupfd 写一个字节，这样 poller 监听的 channel 就产生了可读事件，从而让阻塞退出。 第四层1. EventLoopThread启动一个线程，在其中运行 EventLoop::loop() 构造函数 1234567891011121314151617181920212223242526namespace muduo &#123; namespace net &#123; class EventLoopThread : noncopyable &#123; public: typedef std::function&lt;void(EventLoop *)&gt; ThreadInitCallback; explicit EventLoopThread(const ThreadInitCallback &amp;cb = ThreadInitCallback(), const string &amp;name = string()) : loop_(NULL), exiting_(false), thread_(std::bind(&amp;EventLoopThread::threadFunc, this), name), mutex_(), cond_(mutex_), callback_(cb) &#123; &#125; ... private: EventLoop *loop_ GUARDED_BY(mutex_); bool exiting_; Thread thread_; MutexLock mutex_; Condition cond_ GUARDED_BY(mutex_); ThreadInitCallback callback_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 其中，内部的 thread_ 对象注册的 func 是 123456789101112131415161718void EventLoopThread::threadFunc() &#123; EventLoop loop; if (callback_) &#123; callback_(&amp;loop); &#125; &#123; MutexLockGuard lock(mutex_); loop_ = &amp;loop; cond_.notify(); &#125; loop.loop(); //assert(exiting_); MutexLockGuard lock(mutex_); loop_ = NULL;&#125; 也就是说，在新开的 Thread 中，会先跑 ThreadInitCallBack，然后运行 loop.loop() startLoop 开始一个 loop，注意这里用 mutex 和 cond 保护 loop 的初始化 123456789101112131415EventLoop *EventLoopThread::startLoop() &#123; assert(!thread_.started()); thread_.start(); EventLoop *loop = NULL; &#123; MutexLockGuard lock(mutex_); while (loop_ == NULL) &#123; cond_.wait(); &#125; loop = loop_; &#125; return loop;&#125; 2. TcpConnection整个网络库的核心，封装一次 TCP 连接。每个 TcpConnection 必须归某个 EventLoop 管理，所有的 IO 会转移到这个线程。TcpConnection 既可以用于 Server，也可以用于 Client。 构造函数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455namespace muduo &#123; namespace net &#123; class TcpConnection : noncopyable, public std::enable_shared_from_this&lt;TcpConnection&gt; &#123; public: TcpConnection(EventLoop *loop, const string &amp;name, int sockfd, const InetAddress &amp;localAddr, const InetAddress &amp;peerAddr): loop_(CHECK_NOTNULL(loop)), name_(nameArg), state_(kConnecting), reading_(true), socket_(new Socket(sockfd)), channel_(new Channel(loop, sockfd)), localAddr_(localAddr), peerAddr_(peerAddr), highWaterMark_(64 * 1024 * 1024) &#123; channel_-&gt;setReadCallback(std::bind(&amp;TcpConnection::handleRead, this, _1)); channel_-&gt;setWriteCallback(std::bind(&amp;TcpConnection::handleWrite, this)); channel_-&gt;setCloseCallback(std::bind(&amp;TcpConnection::handleClose, this)); channel_-&gt;setErrorCallback(std::bind(&amp;TcpConnection::handleError, this)); LOG_DEBUG &lt;&lt; \"TcpConnection::ctor[\" &lt;&lt; name_ &lt;&lt; \"] at \" &lt;&lt; this &lt;&lt; \" fd=\" &lt;&lt; sockfd; socket_-&gt;setKeepAlive(true); &#125; ... private: enum StateE &#123; kDisconnected, kConnecting, kConnected, kDisconnecting &#125;; EventLoop *loop_; const string name_; StateE state_; bool reading_; std::unique_ptr&lt;Socket&gt; socket_; std::unique_ptr&lt;Channel&gt; channel_; const InetAddress localAddr_; const InetAddress peerAddr_; size_t highWaterMark_; ... &#125; &#125; // namespace net&#125; // namespace muduo 这个构造函数比较复杂：一个 TcpConnection 和一个 EventLoop 绑定；根据 sockfd 构造一个 Socket 对象；根据 EventLoop 和 sockfd 构造一个 Channel 对象。 localAddr_、peerAddr_ 和 highWaterMark_ 暂时还不知道代表什么； 每个 channel 可以设置 4 种 CallBack，在构造函数中都设置了，接下来依次看一下. 4 种 CallBack ReadCallBack 12345678910111213141516void TcpConnection::handleRead(Timestamp receiveTime) &#123; loop_-&gt;assertInLoopThread(); int savedErrno = 0; ssize_t n = inputBuffer_.readFd(channel_-&gt;fd(), &amp;savedErrno); if (n &gt; 0) &#123; messageCallback_(shared_from_this(), &amp;inputBuffer_, receiveTime); &#125; else if (n == 0) &#123; handleClose(); &#125; else &#123; errno = savedErrno; LOG_SYSERR &lt;&lt; \"TcpConnection::handleRead\"; handleError(); &#125;&#125; ReadCallback 的目标是从内核把数据读到 inputBuffer_ 中，然后执行注册好的 messageCallback_。 WriteCallBack 123456789101112131415161718192021222324void TcpConnection::handleWrite() &#123; loop_-&gt;assertInLoopThread(); if (channel_-&gt;isWriting()) &#123; ssize_t n = sockets::write(channel_-&gt;fd(), outputBuffer_.peek(), outputBuffer_.readableBytes()); if (n &gt; 0) &#123; outputBuffer_.retrieve(n); if (outputBuffer_.readableBytes() == 0) &#123; channel_-&gt;disableWriting(); if (writeCompleteCallback_) &#123; loop_-&gt;queueInLoop(std::bind(writeCompleteCallback_, shared_from_this())); &#125; if (state_ == kDisconnecting) &#123; shutdownInLoop(); &#125; &#125; &#125; else &#123; LOG_SYSERR &lt;&lt; \"TcpConnection::handleWrite\"; &#125; &#125; else &#123; LOG_TRACE &lt;&lt; \"Connection fd = \" &lt;&lt; channel_-&gt;fd() &lt;&lt; \" is down, no more writing\"; &#125;&#125; WriteCallBack 的目标是把 outputBuffer_ 中的数据写到内核缓冲区中。如果把 outputBuffer_ 中的数据都写完了的话，那么就调用注册好的 writeCompleteCallback_ CloseCallBack 12345678910111213141516void TcpConnection::handleClose() &#123; loop_-&gt;assertInLoopThread(); LOG_TRACE &lt;&lt; \"fd = \" &lt;&lt; channel_-&gt;fd() &lt;&lt; \" state = \" &lt;&lt; stateToString(); assert(state_ == kConnected || state_ == kDisconnecting); setState(kDisconnected); channel_-&gt;disableAll(); TcpConnectionPtr guardThis(shared_from_this()); connectionCallback_(guardThis); // must be the last line closeCallback_(guardThis);&#125; 在 CloseCallBack 中不关闭 sockfd，，只是设置状态且关闭 Channel 的读写能力，并调用 connectionCallback_ ErrorCallBack 12345void TcpConnection::handleError() &#123; int err = sockets::getSocketError(channel_-&gt;fd()); LOG_ERROR &lt;&lt; \"TcpConnection::handleError [\" &lt;&lt; name_ &lt;&lt; \"] - SO_ERROR = \" &lt;&lt; err &lt;&lt; \" \" &lt;&lt; strerror_tl(err);&#125; 这个比较简单，获取一下 SocketError 就可以了 send 方法 总共有 3 个 public 方法 TcpConnection.h 123456789101112131415namespace muduo &#123; namespace net &#123; class TcpConnection : noncopyable, public std::enable_shared_from_this&lt;TcpConnection&gt; &#123; public: void send(const void *message, int len); void send(const StringPiece &amp;message); void send(Buffer *message); // this one will swap data ... &#125;; &#125; // namespace net&#125; // namespace muduo 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061void TcpConnection::send(const StringPiece &amp;message) &#123; if (state_ == kConnected) &#123; if (loop_-&gt;isInLoopThread()) &#123; sendInLoop(message); &#125; else &#123; void (TcpConnection::*fp)(const StringPiece &amp;message) = &amp;TcpConnection::sendInLoop; loop_-&gt;runInLoop(std::bind(fp,this, message.as_string())); //std::forward&lt;string&gt;(message))); &#125; &#125;&#125;void TcpConnection::sendInLoop(const StringPiece &amp;message) &#123; sendInLoop(message.data(), message.size());&#125;void TcpConnection::sendInLoop(const void *data, size_t len) &#123; loop_-&gt;assertInLoopThread(); ssize_t nwrote = 0; size_t remaining = len; bool faultError = false; if (state_ == kDisconnected) &#123; LOG_WARN &lt;&lt; \"disconnected, give up writing\"; return; &#125; // if no thing in output queue, try writing directly if (!channel_-&gt;isWriting() &amp;&amp; outputBuffer_.readableBytes() == 0) &#123; nwrote = sockets::write(channel_-&gt;fd(), data, len); if (nwrote &gt;= 0) &#123; remaining = len - nwrote; if (remaining == 0 &amp;&amp; writeCompleteCallback_) &#123; loop_-&gt;queueInLoop(std::bind(writeCompleteCallback_, shared_from_this())); &#125; &#125; else // nwrote &lt; 0 &#123; nwrote = 0; if (errno != EWOULDBLOCK) &#123; LOG_SYSERR &lt;&lt; \"TcpConnection::sendInLoop\"; if (errno == EPIPE || errno == ECONNRESET) // FIXME: any others? &#123; faultError = true; &#125; &#125; &#125; &#125; assert(remaining &lt;= len); if (!faultError &amp;&amp; remaining &gt; 0) &#123; size_t oldLen = outputBuffer_.readableBytes(); if (oldLen + remaining &gt;= highWaterMark_ &amp;&amp; oldLen &lt; highWaterMark_ &amp;&amp; highWaterMarkCallback_) &#123; loop_-&gt;queueInLoop(std::bind(highWaterMarkCallback_, shared_from_this(), oldLen + remaining)); &#125; outputBuffer_.append(static_cast&lt;const char *&gt;(data) + nwrote, remaining); if (!channel_-&gt;isWriting()) &#123; channel_-&gt;enableWriting(); &#125; &#125;&#125; send 方法的主要思想是：如果 outputBuffer_ 中没有数据了，那么尝试直接通过 sockfd 写，不经过 outBuffer_ 这一中间环节了；否则的话，就把数据先写到 outputBuffer_ 中，等待 Channel 有写事件的时候，再通过注册好的 handleWrite 方法中，从 outputBuffer 写到 sockfd 中。 shutdown() 方法 12345678910111213141516void TcpConnection::shutdown() &#123; // FIXME: use compare and swap if (state_ == kConnected) &#123; setState(kDisconnecting); // FIXME: shared_from_this()? loop_-&gt;runInLoop(std::bind(&amp;TcpConnection::shutdownInLoop, this)); &#125;&#125;void TcpConnection::shutdownInLoop() &#123; loop_-&gt;assertInLoopThread(); if (!channel_-&gt;isWriting()) &#123; // we are not writing socket_-&gt;shutdownWrite(); &#125;&#125; 比较简单，就是同步或者异步 shutdown socket forceClose 和 forceCloseWithDelay 123456789101112131415161718192021222324void TcpConnection::forceClose() &#123; // FIXME: use compare and swap if (state_ == kConnected || state_ == kDisconnecting) &#123; setState(kDisconnecting); loop_-&gt;queueInLoop(std::bind(&amp;TcpConnection::forceCloseInLoop, shared_from_this())); &#125;&#125;void TcpConnection::forceCloseWithDelay(double seconds) &#123; if (state_ == kConnected || state_ == kDisconnecting) &#123; setState(kDisconnecting); loop_-&gt;runAfter( seconds, makeWeakCallback(shared_from_this(), &amp;TcpConnection::forceClose)); // not forceCloseInLoop to avoid race condition &#125;&#125;void TcpConnection::forceCloseInLoop() &#123; loop_-&gt;assertInLoopThread(); if (state_ == kConnected || state_ == kDisconnecting) &#123; // as if we received 0 byte in handleRead(); handleClose(); &#125;&#125; 也比较简单，最终是同步或异步调用 CloseCallBack 第五层1. EventLoopThreadPool用于创建 IO 线程池，也就是说把 TcpConnection 分派到一组运行 EventLoop 的线程上。它是 TcpServer 的成员，生命期由后者控制 构造函数 12345678910111213141516171819202122232425namespace muduo &#123; namespace net &#123; class EventLoopThreadPool : noncopyable &#123; public: EventLoopThreadPool(EventLoop *baseLoop, const string &amp;nameArg) : baseLoop_(baseLoop), name_(nameArg), started_(false), numThreads_(0), next_(0) &#123; &#125; ... private: EventLoop *baseLoop_; string name_; bool started_; int numThreads_; int next_; ... &#125;; &#125; // namespace net&#125; // namespace muduo EventLoopThreadPool 有一个基本的 baseLoop_ start 12345678910111213141516171819202122232425262728293031323334namespace muduo &#123; namespace net &#123; class EventLoopThreadPool : noncopyable &#123; public: void start(const ThreadInitCallback &amp;cb = ThreadInitCallback()) &#123; assert(!started_); baseLoop_-&gt;assertInLoopThread(); started_ = true; for (int i = 0; i &lt; numThreads_; ++i) &#123; char buf[name_.size() + 32]; snprintf(buf, sizeof buf, \"%s%d\", name_.c_str(), i); EventLoopThread *t = new EventLoopThread(cb, buf); threads_.push_back(std::unique_ptr&lt;EventLoopThread&gt;(t)); loops_.push_back(t-&gt;startLoop()); &#125; if (numThreads_ == 0 &amp;&amp; cb) &#123; cb(baseLoop_); &#125; &#125; ... private: std::vector&lt;std::unique_ptr&lt;EventLoopThread&gt;&gt; threads_; std::vector&lt;EventLoop *&gt; loops_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 这个方法会开 numThreads_ 个线程，并且在 numThreads_ 线程里面让 EventLoop 开始 loop 起来，在 EventLoopThreadPool 中保存好这个 Thread 和这些 EventLoop getNextLoop 12345678910111213141516171819202122232425262728namespace muduo &#123; namespace net &#123; class EventLoopThreadPool : noncopyable &#123; public: EventLoop *getNextLoop() &#123; baseLoop_-&gt;assertInLoopThread(); assert(started_); EventLoop *loop = baseLoop_; if (!loops_.empty()) &#123; // round-robin loop = loops_[next_]; ++next_; if (implicit_cast&lt;size_t&gt;(next_) &gt;= loops_.size()) &#123; next_ = 0; &#125; &#125; return loop; &#125; ... private: int numThreads_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 这个方法是用来把 TcpConnection 分派到一组运行 EventLoop 的线程上，采取的方式是 round-robin 2. TcpServer用于编写网络服务器，接受客户的连接 构造函数 12345678910111213141516171819202122232425262728293031323334namespace muduo &#123; namespace net &#123; class TcpServer : noncopyable &#123; public: TcpServer(EventLoop *loop, const InetAddress &amp;listenAddr, const string &amp;nameArg, Option option = kNoReusePort) : loop_(CHECK_NOTNULL(loop)), ipPort_(listenAddr.toIpPort()), name_(nameArg), acceptor_(new Acceptor(loop, listenAddr,option == kReusePort)), threadPool_(new EventLoopThreadPool(loop, name_)), connectionCallback_(defaultConnectionCallback), messageCallback_(defaultMessageCallback), nextConnId_(1) &#123; acceptor_-&gt;setNewConnectionCallback(std::bind(&amp;TcpServer::newConnection, this, _1, _2)); &#125; ... private: EventLoop *loop_; // the acceptor loop const string ipPort_; const string name_; std::unique_ptr&lt;Acceptor&gt; acceptor_; // avoid revealing Acceptor std::shared_ptr&lt;EventLoopThreadPool&gt; threadPool_; ConnectionCallback connectionCallback_; MessageCallback messageCallback_; int nextConnId_; ... &#125;; &#125; // namespace net&#125; // namespace muduo 这个构造函数里面也干了很多事情：绑定 EventLoop；构造 Acceptor、构造 EventLoopThreadPool、设置 connectionCallback_ 和 messageCallback_。 其中，acceptor_ 设置了 NewConnectionCallback ： 1234567891011121314151617181920212223242526void TcpServer::newConnection(int sockfd, const InetAddress &amp;peerAddr) &#123; loop_-&gt;assertInLoopThread(); EventLoop *ioLoop = threadPool_-&gt;getNextLoop(); char buf[64]; snprintf(buf, sizeof buf, \"-%s#%d\", ipPort_.c_str(), nextConnId_); ++nextConnId_; string connName = name_ + buf; LOG_INFO &lt;&lt; \"TcpServer::newConnection [\" &lt;&lt; name_ &lt;&lt; \"] - new connection [\" &lt;&lt; connName &lt;&lt; \"] from \" &lt;&lt; peerAddr.toIpPort(); InetAddress localAddr(sockets::getLocalAddr(sockfd)); // FIXME poll with zero timeout to double confirm the new connection // FIXME use make_shared if necessary TcpConnectionPtr conn( new TcpConnection(ioLoop, connName, sockfd, localAddr, peerAddr)); connections_[connName] = conn; conn-&gt;setConnectionCallback(connectionCallback_); conn-&gt;setMessageCallback(messageCallback_); conn-&gt;setWriteCompleteCallback(writeCompleteCallback_); conn-&gt;setCloseCallback(std::bind(&amp;TcpServer::removeConnection, this, _1)); // FIXME: unsafe ioLoop-&gt;runInLoop(std::bind(&amp;TcpConnection::connectEstablished, conn));&#125; 这个方法比较重要。 Acceptor 用于接收 TCP 连接，它监听的是 listenfd；在连接到来后，会将 connfd 传入 newConnectionCallback_ 方法。 在这个 NewConnectionCallback 方法中，首先从 ThreadPool 中 Round-Robin 式的找一个 EventLoop，然后根据这个 connfd 新建一个 TcpConnection 传入这个 EventLoop 中，然后把这个 TcpConnection 的 4 种 CallBack 设置好，最终让这个 TcpConnection 放到 EventLoop 中开始跑。 接下来看看这 4 个 CallBack，这 4 个 CallBack 里面，connectionCallback_、messageCallback_ 和 writeCompleteCallback_ 都可以在 TcpServer 中外部 set，而 CloseCallback 是固定的： TcpConnection 的 4 个 CallBack connectionCallback_ 123456void muduo::net::defaultConnectionCallback(const TcpConnectionPtr &amp;conn) &#123; LOG_TRACE &lt;&lt; conn-&gt;localAddress().toIpPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;peerAddress().toIpPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); // do not call conn-&gt;forceClose(), because some users want to register message callback only.&#125; Default 情况下啥也不干 messageCallback_ 123void muduo::net::defaultMessageCallback(const TcpConnectionPtr &amp;, Buffer *buf, Timestamp) &#123; buf-&gt;retrieveAll();&#125; writeCompleteCallback_ 没有 Default closeCallBack_ 12345678910111213141516171819202122232425262728conn-&gt;setCloseCallback(std::bind(&amp;TcpServer::removeConnection, this, _1)); // FIXME: unsafevoid TcpServer::removeConnection(const TcpConnectionPtr &amp;conn) &#123; // FIXME: unsafe loop_-&gt;runInLoop(std::bind(&amp;TcpServer::removeConnectionInLoop, this, conn));&#125;void TcpServer::removeConnectionInLoop(const TcpConnectionPtr &amp;conn) &#123; loop_-&gt;assertInLoopThread(); LOG_INFO &lt;&lt; \"TcpServer::removeConnectionInLoop [\" &lt;&lt; name_ &lt;&lt; \"] - connection \" &lt;&lt; conn-&gt;name(); size_t n = connections_.erase(conn-&gt;name()); (void) n; assert(n == 1); EventLoop *ioLoop = conn-&gt;getLoop(); ioLoop-&gt;queueInLoop(std::bind(&amp;TcpConnection::connectDestroyed, conn));&#125;void TcpConnection::connectDestroyed() &#123; loop_-&gt;assertInLoopThread(); if (state_ == kConnected) &#123; setState(kDisconnected); channel_-&gt;disableAll(); connectionCallback_(shared_from_this()); &#125; channel_-&gt;remove();&#125; 这个 CloseCallBack 的目标是移除对应 EventLoop 中的 Channel start 方法 123456789void TcpServer::start() &#123; if (started_.getAndSet(1) == 0) &#123; threadPool_-&gt;start(threadInitCallback_); assert(!acceptor_-&gt;listenning()); loop_-&gt;runInLoop( std::bind(&amp;Acceptor::listen, get_pointer(acceptor_))); &#125;&#125; 首先一个 CAS 操作保证一个 TcpServer 只 start 一次，然后让 ThreadPool 开始启动线程，同时让 Acceptor 开始监听端口 析构函数 1234567891011TcpServer::~TcpServer() &#123; loop_-&gt;assertInLoopThread(); LOG_TRACE &lt;&lt; \"TcpServer::~TcpServer [\" &lt;&lt; name_ &lt;&lt; \"] destructing\"; for (auto &amp;item : connections_) &#123; TcpConnectionPtr conn(item.second); item.second.reset(); conn-&gt;getLoop()-&gt;runInLoop( std::bind(&amp;TcpConnection::connectDestroyed, conn)); &#125;&#125; 析构函数中，让所有的 TcpConnection 对象的 Channel 都销毁掉。 3. TcpClient用于编写网络客户端，能发起连接，并且有重试功能。 构造函数 1234567891011121314151617181920212223242526272829303132333435namespace muduo &#123; namespace net &#123; class TcpClient : noncopyable &#123; public: TcpClient(EventLoop *loop, const InetAddress &amp;serverAddr, const string &amp;nameArg): loop_(CHECK_NOTNULL(loop)), connector_(new Connector(loop, serverAddr)), name_(nameArg), connectionCallback_(defaultConnectionCallback), messageCallback_(defaultMessageCallback), retry_(false), connect_(true), nextConnId_(1) &#123; connector_-&gt;setNewConnectionCallback( std::bind(&amp;TcpClient::newConnection, this, _1)); LOG_INFO &lt;&lt; \"TcpClient::TcpClient[\" &lt;&lt; name_ &lt;&lt; \"] - connector \" &lt;&lt; get_pointer(connector_); &#125; ... private: EventLoop *loop_; ConnectorPtr connector_; // avoid revealing Connector const string name_; ConnectionCallback connectionCallback_; MessageCallback messageCallback_; bool retry_; // atomic bool connect_; // atomic // always in loop thread int nextConnId_; ... &#125;; &#125; // namespace net&#125; // namespace muduo TcpClient 绑定一个 EventLoop，内部有一个 connector 用于新建 TCP 连接；connectionCallback_ 和 messageCallback_ 初始都设为 default，default 是啥也不干；Connector 要设置 NewConnection 的 Callback 如下： 1234567891011121314151617181920212223242526void TcpClient::newConnection(int sockfd) &#123; loop_-&gt;assertInLoopThread(); InetAddress peerAddr(sockets::getPeerAddr(sockfd)); char buf[32]; snprintf(buf, sizeof buf, \":%s#%d\", peerAddr.toIpPort().c_str(), nextConnId_); ++nextConnId_; string connName = name_ + buf; InetAddress localAddr(sockets::getLocalAddr(sockfd)); // FIXME poll with zero timeout to double confirm the new connection // FIXME use make_shared if necessary TcpConnectionPtr conn( new TcpConnection(loop_, connName, sockfd, localAddr, peerAddr)); conn-&gt;setConnectionCallback(connectionCallback_); conn-&gt;setMessageCallback(messageCallback_); conn-&gt;setWriteCompleteCallback(writeCompleteCallback_); conn-&gt;setCloseCallback( std::bind(&amp;TcpClient::removeConnection, this, _1)); // FIXME: unsafe &#123; MutexLockGuard lock(mutex_); connection_ = conn; &#125; conn-&gt;connectEstablished();&#125; 这个 Callback 的目标是打开一个 TcpConnection，这个 TcpConnection 注册好各种回调，然后 channel 开始监听。 connect 让内部的 connector 开始 connect 1234567void TcpClient::connect() &#123; // FIXME: check state LOG_INFO &lt;&lt; \"TcpClient::connect[\" &lt;&lt; name_ &lt;&lt; \"] - connecting to \" &lt;&lt; connector_-&gt;serverAddress().toIpPort(); connect_ = true; connector_-&gt;start();&#125; disconnect 断开 TcpConnection，对于 TcpClient，它只有一个 TcpConnection，这点和 TcpServer 明显不同 12345678910void TcpClient::disconnect() &#123; connect_ = false; &#123; MutexLockGuard lock(mutex_); if (connection_) &#123; connection_-&gt;shutdown(); &#125; &#125;&#125; stop 让内部的 connector stop 1234void TcpClient::stop() &#123; connect_ = false; connector_-&gt;stop();&#125; 以上就把 muduo 中的 net 部分基本介绍完了，好累。。","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"Net","slug":"Net","permalink":"http://yoursite.com/tags/Net/"}]},{"title":"muduo 网络编程示例之零：前言","slug":"muduo网络编程示例之零：前言","date":"2020-03-09T02:55:49.386Z","updated":"2020-03-09T02:56:26.000Z","comments":true,"path":"开源组件/muduo/muduo网络编程示例之零：前言/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/muduo%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%A4%BA%E4%BE%8B%E4%B9%8B%E9%9B%B6%EF%BC%9A%E5%89%8D%E8%A8%80/","excerpt":"","text":"原文链接 https://blog.csdn.net/Solstice/article/details/6171831 我将会写一系列文章，介绍用 muduo 网络库完成常见的 TCP 网络编程任务。目前计划如下： UNP 中的简单协议，包括 echo、daytime、time、discard 等。 Boost.Asio 中的示例，包括 timer2~6、chat 等。 Java Netty 中的示例，包括 discard、echo、uptime 等，其中的 discard 和 echo 带流量统计功能。 Python twisted 中的示例，包括 finger01~07 用于测试两台机器的往返延迟的 roundtrip 用于测试两台机器的带宽的 pingpong 云风的串并转换连接服务器 multiplexer，包括单线程和多线程两个版本。 文件传输 一个基于 TCP 的应用层广播 hub socks4a 代理服务器，包括简单的 TCP 中继(relay)。 一个 Sudoku 服务器的演变，从单线程到多线程，从阻塞到 event-based。 一个提供短址服务的 httpd 服务器 其中前面 7 个已经放到了 muduo 代码的 examples 目录中，下载地址是： http://muduo.googlecode.com/files/muduo-0.1.5-alpha.tar.gz 这些例子都比较简单，逻辑不复杂，代码也很短，适合摘取关键部分放到博客上。其中一些有一定的代表性与针对性，比如“如何传输完整的文件”估计是网络编程的初学者经常遇到的问题。请注意，muduo 是设计来开发内网的网络程序，它没有做任何安全方面的加强措施，如果用在公网上可能会受到攻击，在后面的例子中我会谈到这一点。 本系列文章适用于 Linux 2.6.x (x &gt; 28)，主要测试发行版为 Ubuntu 10.04 LTS 和 Debian 6.0 Squeeze，64-bit x86 硬件。 1. TCP 网络编程本质论 2. Muduo 简介 1. TCP 网络编程本质论我认为，TCP 网络编程最本质的是处理三个半事件： 连接的建立，包括服务端接受 (accept) 新连接和客户端成功发起 (connect) 连接。 连接的断开，包括主动断开 (close 或 shutdown) 和被动断开 (read 返回 0)。 消息到达，文件描述符可读。这是最为重要的一个事件，对它的处理方式决定了网络编程的风格（阻塞还是非阻塞，如何处理分包，应用层的缓冲如何设计等等）。 消息发送完毕，这算半个。对于低流量的服务，可以不必关心这个事件；另外，这里“发送完毕”是指将数据写入操作系统的缓冲区，将由 TCP 协议栈负责数据的发送与重传，不代表对方已经收到数据。 这其中有很多难点，也有很多细节需要注意，比方说： 如果要主动关闭连接，如何保证对方已经收到全部数据？如果应用层有缓冲（这在非阻塞网络编程中是必须的，见下文），那么如何保证先发送完缓冲区中的数据，然后再断开连接。直接调用 close(2) 恐怕是不行的。 如果主动发起连接，但是对方主动拒绝，如何定期 (带 back-off) 重试？ 非阻塞网络编程该用边沿触发(edge trigger)还是电平触发(level trigger)？（这两个中文术语有其他译法，我选择了一个电子工程师熟悉的说法。）如果是电平触发，那么什么时候关注 EPOLLOUT 事件？会不会造成 busy-loop？如果是边沿触发，如何防止漏读造成的饥饿？epoll 一定比 poll 快吗？ 在非阻塞网络编程中，为什么要使用应用层缓冲区？假如一次读到的数据不够一个完整的数据包，那么这些已经读到的数据是不是应该先暂存在某个地方，等剩余的数据收到之后再一并处理？见 lighttpd 关于 /r/n/r/n 分包的 bug。假如数据是一个字节一个字节地到达，间隔 10ms，每个字节触发一次文件描述符可读 (readable) 事件，程序是否还能正常工作？lighttpd 在这个问题上出过安全漏洞。 在非阻塞网络编程中，如何设计并使用缓冲区？一方面我们希望减少系统调用，一次读的数据越多越划算，那么似乎应该准备一个大的缓冲区。另一方面，我们系统减少内存占用。如果有 10k 个连接，每个连接一建立就分配 64k 的读缓冲的话，将占用 640M 内存，而大多数时候这些缓冲区的使用率很低。muduo 用 readv 结合栈上空间巧妙地解决了这个问题。 如果使用发送缓冲区，万一接收方处理缓慢，数据会不会一直堆积在发送方，造成内存暴涨？如何做应用层的流量控制？ 如何设计并实现定时器？并使之与网络 IO 共用一个线程，以避免锁。 这些问题在 muduo 的代码中可以找到答案。 2. Muduo 简介我编写 Muduo 网络库的目的之一就是简化日常的 TCP 网络编程，让程序员能把精力集中在业务逻辑的实现上，而不要天天和 Sockets API 较劲。借用 Brooks 的话说，我希望 Muduo 能减少网络编程中的偶发复杂性 (accidental complexity)。 Muduo 只支持 Linux 2.6.x 下的并发非阻塞 TCP 网络编程，它的安装方法见陈硕的 blog 文章。 Muduo 的使用非常简单，不需要从指定的类派生，也不用覆写虚函数，只需要注册几个回调函数去处理前面提到的三个半事件就行了。 以经典的 echo 回显服务为例： 定义 EchoServer class，不需要派生自任何基类： 1234567891011121314151617181920#ifndef MUDUO_EXAMPLES_SIMPLE_ECHO_ECHO_H #define MUDUO_EXAMPLES_SIMPLE_ECHO_ECHO_H// RFC 862 class EchoServer &#123; public: EchoServer(muduo::net::EventLoop* loop, const muduo::net::InetAddress&amp; listenAddr); void start();private: void onConnection(const muduo::net::TcpConnectionPtr&amp; conn); void onMessage(const muduo::net::TcpConnectionPtr&amp; conn, muduo::net::Buffer* buf, muduo::Timestamp time); muduo::net::EventLoop* loop_; muduo::net::TcpServer server_; &#125;;#endif // MUDUO_EXAMPLES_SIMPLE_ECHO_ECHO_H 在构造函数里注册回调函数： 123456789101112131415EchoServer::EchoServer(EventLoop* loop, const InetAddress&amp; listenAddr) : loop_(loop), server_(loop, listenAddr, \"EchoServer\") &#123; server_.setConnectionCallback( boost::bind(&amp;EchoServer::onConnection, this, _1)); server_.setMessageCallback( boost::bind(&amp;EchoServer::onMessage, this, _1, _2, _3)); &#125;void EchoServer::start() &#123; server_.start(); &#125; 实现 EchoServer::onConnection() 和 EchoServer::onMessage()： 12345678910111213141516void EchoServer::onConnection(const TcpConnectionPtr&amp; conn) &#123; LOG_INFO &lt;&lt; \"EchoServer - \" &lt;&lt; conn-&gt;peerAddress().toHostPort() &lt;&lt; \" -&gt; \" &lt;&lt; conn-&gt;localAddress().toHostPort() &lt;&lt; \" is \" &lt;&lt; (conn-&gt;connected() ? \"UP\" : \"DOWN\"); &#125;void EchoServer::onMessage(const TcpConnectionPtr&amp; conn, Buffer* buf, Timestamp time) &#123; string msg(buf-&gt;retrieveAsString()); LOG_INFO &lt;&lt; conn-&gt;name() &lt;&lt; \" echo \" &lt;&lt; msg.size() &lt;&lt; \" bytes at \" &lt;&lt; time.toString(); conn-&gt;send(msg); &#125; 在 main() 里用 EventLoop 让整个程序跑起来： 123456789101112#include \"echo.h\"using namespace muduo; using namespace muduo::net;int main() &#123; LOG_INFO &lt;&lt; \"pid = \" &lt;&lt; getpid(); EventLoop loop; InetAddress listenAddr(2007); EchoServer server(&amp;loop, listenAddr); server.start(); loop.loop(); &#125; 完整的代码见 muduo/examples/simple/echo。 这个几十行的小程序实现了一个并发的 echo 服务程序，可以同时处理多个连接。 对这个程序的详细分析见下一篇博客《Muduo 网络编程示例之一：五个简单 TCP 协议》————————————————版权声明：本文为CSDN博主「陈硕」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/Solstice/article/details/6171831","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"Net","slug":"Net","permalink":"http://yoursite.com/tags/Net/"}]},{"title":"以 boost 中的 function 和 bind 取代虚函数","slug":"以boost中的function和bind取代虚函数","date":"2020-03-09T02:54:14.428Z","updated":"2020-03-09T02:54:59.000Z","comments":true,"path":"开源组件/muduo/以boost中的function和bind取代虚函数/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/%E4%BB%A5boost%E4%B8%AD%E7%9A%84function%E5%92%8Cbind%E5%8F%96%E4%BB%A3%E8%99%9A%E5%87%BD%E6%95%B0/","excerpt":"","text":"原文链接： https://blog.csdn.net/Solstice/article/details/3066268 这是一篇比较情绪化的 blog，中心思想是“继承就像一条贼船，上去就下不来了”，而借助 boost::function和boost::bind，大多数情况下，你都不用上贼船。 boost::function 和 boost::bind 已经纳入了 std::tr1，这或许是 C++0x 最值得期待的功能，它将彻底改变 C++ 库的设计方式，以及应用程序的编写方式。 Scott Meyers 的 Effective C++ 3rd ed.第35条款提到了以 boost::function 和 boost:bind 取代虚函数的做法，这里谈谈我自己使用的感受。 1. 基本用途 2. 对程序库的影响 例1：线程库 常规OO设计： 基于closure的设计： 例2：网络库 3. 对面向对象程序设计的影响 4. 对面向对象设计模式的影响 5. 依赖注入与单元测试 6. 什么时候使用继承？ 7. 基于接口的设计 8. 实现Signal/Slot 1. 基本用途boost::function 就像 C# 里的 delegate，可以指向任何函数，包括成员函数。当用 bind 把某个成员函数绑到某个对象上时，我们得到了一个closure（闭包）。例如： 12345678910111213141516171819202122232425262728293031323334353637383940414243#include &lt;boost/function.hpp&gt;#include &lt;boost/bind.hpp&gt;#include &lt;iostream&gt;class Foo &#123;public: void methodA() &#123; std::cout &lt;&lt; \"Foo::methodA\\n\"; &#125; void methodInt(int a) &#123; std::cout &lt;&lt; \"Foo::methodInt\\t\" &lt;&lt; a &lt;&lt; \"\\n\"; &#125;&#125;;class Bar &#123;public: void methodB() &#123; std::cout &lt;&lt; \"Bar::methodB\\n\"; &#125;&#125;;int main(int argc, char **args) &#123; boost::function&lt;void()&gt; f1; // 无参数，无返回值 Foo foo; f1 = boost::bind(&amp;Foo::methodA, &amp;foo); f1(); // 调用 foo.methodA(); Bar bar; f1 = boost::bind(&amp;Bar::methodB, &amp;bar); f1(); // 调用 bar.methodB(); f1 = boost::bind(&amp;Foo::methodInt, &amp;foo, 42); f1(); // 调用 foo.methodInt(42); boost::function&lt;void(int)&gt; f2; // int 参数，无返回值 f2 = boost::bind(&amp;Foo::methodInt, &amp;foo, _1); f2(53); // 调用 foo.methodInt(53); return 0;&#125; 如果没有 boost::bind，那么 boost::function 就什么都不是，而有了 bind()，“同一个类的不同对象可以 delegate 给不同的实现，从而实现不同的行为”（myan 语），简直就无敌了。 2. 对程序库的影响程序库的设计不应该给使用者带来不必要的限制（耦合），而继承是仅次于最强的一种耦合（最强耦合的是友元）。如果一个程序库限制其使用者必须从某个class派生，那么我觉得这是一个糟糕的设计。不巧的是，目前有些程序库就是这么做的。 例1：线程库常规OO设计：写一个 Thread base class，含有（纯）虚函数 Thread#run()，然后应用程序派生一个继承 class，覆写 run()。程序里的每一种线程对应一个 Thread 的派生类。例如 Java 的 Thread 可以这么用。 缺点：如果一个class的三个method需要在三个不同的线程中执行，就得写helper class(es)并玩一些OO把戏。 基于closure的设计：令 Thread 是一个具体类，其构造函数接受 Callable 对象。应用程序只需提供一个 Callable 对象，创建一份Thread 实体，调用 Thread#start() 即可。Java 的 Thread 也可以这么用，传入一个 Runnable 对象。C# 的 Thread 只支持这一种用法，构造函数的参数是 delegate ThreadStart。boost::thread 也只支持这种用法。 一个基于 closure 的 Thread class 基本结构 12345678910111213141516171819202122232425262728293031323334#include &lt;boost/function.hpp&gt;#include &lt;boost/bind.hpp&gt;class Thread &#123;public: typedef boost::function&lt;void()&gt; ThreadCallback; Thread(ThreadCallback cb) : cb_(cb) &#123;&#125; void start() &#123; /* some magic to call run() in new created thread */ &#125;private: void run() &#123; cb_(); &#125; ThreadCallback cb_; // ...&#125;;class Foo &#123;public: void runInThread();&#125;;// 使用int main() &#123; Foo foo; Thread thread(boost::bind(&amp;Foo::runInThread, &amp;foo)); thread.start();&#125; 例2：网络库以 boost::function 作为桥梁，NetServer class 对其使用者没有任何类型上的限制，只对成员函数的参数和返回类型有限制。使用者 EchoService 也完全不知道 NetServer 的存在，只要在 main() 里把两者装配到一起，程序就跑起来了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// library#include &lt;boost/noncopyable.hpp&gt;#include &lt;boost/function.hpp&gt;#include &lt;boost/bind.hpp&gt;class Connection;class NetServer : boost::noncopyable &#123;public: typedef boost::function&lt;void(Connection *)&gt; ConnectionCallback; typedef boost::function&lt;void(Connection *, const void *, int len)&gt; MessageCallback; NetServer(uint16_t port); ~NetServer(); void run(); void registerConnectionCallback(const ConnectionCallback &amp;); void registerMessageCallback(const MessageCallback &amp;); void sendMessage(Connection *, const void *buf, int len);private: // ...&#125;;// userclass EchoService &#123;public: // 符合NetServer::sendMessage的原型 typedef boost::function&lt;void(Connection *, const void *, int)&gt; SendMessageCallback; EchoService(const SendMessageCallback &amp;sendMsgCb) : sendMessageCb_(sendMsgCb) &#123;&#125; // 符合NetServer::NetServer::MessageCallback的原型 void onMessage(Connection *conn, const void *buf, int size) &#123; printf(\"Received Msg from Connection %d: %.*s/n\", conn-&gt;id(), size, (const char *) buf); sendMessageCb_(conn, buf, size); // echo back &#125; // 符合NetServer::NetServer::ConnectionCallback的原型 void onConnection(Connection *conn) &#123; printf(\"Connection from %s:%d is %s/n\", conn-&gt;ipAddr(), conn-&gt;port(), conn-&gt;connected() ? \"UP\" : \"DOWN\"); &#125;private: SendMessageCallback sendMessageCb_;&#125;;// 扮演上帝的角色，把各部件拼起来int main() &#123; NetServer server(7); EchoService echo(bind(&amp;NetServer::sendMessage, &amp;server, _1, _2, _3)); server.registerMessageCallback( bind(&amp;EchoService::onMessage, &amp;echo, _1, _2, _3)); server.registerConnectionCallback( bind(&amp;EchoService::onConnection, &amp;echo, _1)); server.run();&#125; 3. 对面向对象程序设计的影响一直以来，我对面向对象有一种厌恶感，叠床架屋，绕来绕去的，一拳拳打在棉花上，不解决实际问题。面向对象三要素是封装、继承和多态。我认为封装是根本的，继承和多态则是可有可无。用 class 来表示 concept，这是根本的；至于继承和多态，其耦合性太强，往往不划算。 继承和多态不仅规定了函数的名称、参数、返回类型，还规定了类的继承关系。在现代的 OO 编程语言里，借助反射和 attribute/annotation，已经大大放宽了限制。举例来说，JUnit 3.x 是用反射，找出派生类里的名字符合 void test*() 的函数来执行，这里就没继承什么事，只是对函数的名称有部分限制（继承是全面限制，一字不差）。至于 JUnit 4.x 和 NUnit 2.x 则更进一步，以 annoatation/attribute 来标明 test case，更没继承什么事了。 我的猜测是，当初提出面向对象的时候，closure 还没有一个通用的实现，所以它没能算作基本的抽象工具之一。现在既然 closure 已经这么方便了，或许我们应该重新审视面向对象设计，至少不要那么滥用继承。 自从找到了 boost::function+boost::bind 这对神兵利器，不用再考虑类直接的继承关系，只需要基于对象的设计(object-based)，拳拳到肉，程序写起来顿时顺手了很多。 4. 对面向对象设计模式的影响既然虚函数能用 closure 代替，那么很多 OO 设计模式，尤其是行为模式，失去了存在的必要。另外，既然没有继承体系，那么创建型模式似乎也没啥用了。 最明显的是 Strategy，不用累赘的 Strategy 基类和ConcreteStrategyA、ConcreteStrategyB 等派生类，一个 boost::function&lt;&gt; 成员就解决问题。在《设计模式》这本书提到了23个模式，我认为 iterator 有用（或许再加个 State），其他都在摆谱，拉虚架子，没啥用。或许它们解决了面向对象中的常见问题，不过要是我的程序里连面向对象（指继承和多态）都不用，那似乎也不用叨扰面向对象设计模式了。 或许 closure-based programming 将作为一种新的 programming paradiam 而流行起来。 5. 依赖注入与单元测试前面的 EchoService 可算是依赖注入的例子，EchoService 需要一个什么东西来发送消息，它对这个“东西”的要求只是函数原型满足 SendMessageCallback，而并不关系数据到底发到网络上还是发到控制台。在正常使用的时候，数据应该发给网络，而在做单元测试的时候，数据应该发给某个 DataSink。 安照面向对象的思路，先写一个 AbstractDataSink interface，包含 sendMessage() 这个虚函数，然后派生出两个 classes：NetDataSink 和 MockDataSink，前面那个干活用，后面那个单元测试用。EchoService 的构造函数应该以 AbstractDataSink* 为参数，这样就实现了所谓的接口与实现分离。 我认为这么做纯粹是脱了裤子放屁，直接传入一个 SendMessageCallback 对象就能解决问题。在单元测试的时候，可以 boost::bind() 到 MockServer 上，或某个全局函数上，完全不用继承和虚函数，也不会影响现有的设计。 6. 什么时候使用继承？如果是指 OO 中的 public 继承，即为了接口与实现分离，那么我只会在派生类的数目和功能完全确定的情况下使用。换句话说，不为将来的扩展考虑，这时候面向对象或许是一种不错的描述方法。一旦要考虑扩展，什么办法都没用，还不如把程序写简单点，将来好大改或重写。 如果是功能继承，那么我会考虑继承 boost::noncopyable 或 boost::enable_shared_from_this，下一篇 blog 会讲到 enable_shared_from_this 在实现多线程安全的 Signal/Slot 时的妙用。 例如，IO-Multiplex 在不同的操作系统下有不同的推荐实现，最通用的 select()，POSIX 的 poll()，Linux 的 epoll()，FreeBSD 的 kqueue 等等，数目固定，功能也完全确定，不用考虑扩展。那么设计一个 NetLoop base class 加若干具体 classes 就是不错的解决办法。 7. 基于接口的设计这个问题来自那个经典的讨论：不会飞的企鹅（Penguin）究竟应不应该继承自鸟（Bird），如果 Bird 定义了 virtual function fly() 的话。讨论的结果是，把具体的行为提出来，作为 interface，比如 Flyable （能飞的），Runnable（能跑的），然后让企鹅实现 Runnable，麻雀实现 Flyable 和 Runnable。（其实麻雀只能双脚跳，不能跑，这里不作深究。） 进一步的讨论表明，interface 的粒度应足够小，或许包含一个 method 就够了，那么 interface 实际上退化成了给类型打的标签(tag)。在这种情况下，完全可以使用boost::function来代替，比如： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#include &lt;boost/function.hpp&gt;#include &lt;boost/bind.hpp&gt;// 企鹅能游泳，也能跑class Penguin &#123;public: void run(); void swim();&#125;;// 麻雀能飞，也能跑class Sparrow &#123;public: void fly(); void run();&#125;;// 以 closure 作为接口typedef boost::function&lt;void()&gt; FlyCallback;typedef boost::function&lt;void()&gt; RunCallback;typedef boost::function&lt;void()&gt; SwimCallback;// 一个既用到run，也用到fly的客户classclass Foo &#123;public: Foo(FlyCallback flyCb, RunCallback runCb) : flyCb_(flyCb), runCb_(runCb) &#123;&#125;private: FlyCallback flyCb_; RunCallback runCb_;&#125;;// 一个既用到run，也用到swim的客户classclass Bar &#123;public: Bar(SwimCallback swimCb, RunCallback runCb) : swimCb_(swimCb), runCb_(runCb) &#123;&#125;private: SwimCallback swimCb_; RunCallback runCb_;&#125;;int main() &#123; Sparrow s; Penguin p; // 装配起来，Foo要麻雀，Bar要企鹅。 Foo foo(boost::bind(&amp;Sparrow::fly, &amp;s), boost::bind(&amp;Sparrow::run, &amp;s)); Bar bar(boost::bind(&amp;Penguin::swim, &amp;p), boost::bind(&amp;Penguin::run, &amp;p)); return 0;&#125; 8. 实现Signal/Slotboost::function + boost::bind 描述了一对一的回调，在项目中，我们借助 boost::shared_ptr + boost::weak_ptr 简洁地实现了多播(multi-cast)，即一对多的回调，并且考虑了对象的生命期管理与多线程安全；并且，自然地，对使用者的类型不作任何限制，篇幅略长，留作下一篇blog吧。（boost::signals 也实现了 Signal/Slot，但可惜不是线程安全的。） 最后，向伟大的C语言致敬！","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"Boost","slug":"Boost","permalink":"http://yoursite.com/tags/Boost/"}]},{"title":"muduo 中的网络库","slug":"muduo中的网络库","date":"2020-03-09T02:52:15.035Z","updated":"2020-03-09T02:52:57.000Z","comments":true,"path":"开源组件/muduo/muduo中的网络库/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/muduo%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E5%BA%93/","excerpt":"","text":"1. Buffer 1. Buffer 部分源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113namespace muduo &#123; namespace net &#123; class Buffer : public muduo::copyable &#123; public: static const size_t kCheapPrepend = 8; static const size_t kInitialSize = 1024; explicit Buffer(size_t initialSize = kInitialSize) : buffer_(kCheapPrepend + initialSize), readerIndex_(kCheapPrepend), writerIndex_(kCheapPrepend) &#123; assert(readableBytes() == 0); assert(writableBytes() == initialSize); assert(prependableBytes() == kCheapPrepend); &#125; // implicit copy-ctor, move-ctor, dtor and assignment are fine // NOTE: implicit move-ctor is added in g++ 4.6 void swap(Buffer &amp;rhs) &#123; buffer_.swap(rhs.buffer_); std::swap(readerIndex_, rhs.readerIndex_); std::swap(writerIndex_, rhs.writerIndex_); &#125; ... // retrieve returns void, to prevent // string str(retrieve(readableBytes()), readableBytes()); // the evaluation of two functions are unspecified void retrieve(size_t len) &#123; assert(len &lt;= readableBytes()); if (len &lt; readableBytes()) &#123; readerIndex_ += len; &#125; else &#123; retrieveAll(); &#125; &#125; ... void append(const char * /*restrict*/ data, size_t len) &#123; ensureWritableBytes(len); std::copy(data, data + len, beginWrite()); hasWritten(len); &#125; /// /// Peek int64_t from network endian /// /// Require: buf-&gt;readableBytes() &gt;= sizeof(int64_t) int64_t peekInt64() const &#123; assert(readableBytes() &gt;= sizeof(int64_t)); int64_t be64 = 0; ::memcpy(&amp;be64, peek(), sizeof be64); return sockets::networkToHost64(be64); &#125; ... void prepend(const void * /*restrict*/ data, size_t len) &#123; assert(len &lt;= prependableBytes()); readerIndex_ -= len; const char *d = static_cast&lt;const char *&gt;(data); std::copy(d, d + len, begin() + readerIndex_); &#125; ... void shrink(size_t reserve) &#123; // FIXME: use vector::shrink_to_fit() in C++ 11 if possible. Buffer other; other.ensureWritableBytes(readableBytes() + reserve); other.append(toStringPiece()); swap(other); &#125; /// Read data directly into buffer. /// /// It may implement with readv(2) /// @return result of read(2), @c errno is saved ssize_t readFd(int fd, int *savedErrno); private: ... void makeSpace(size_t len) &#123; if (writableBytes() + prependableBytes() &lt; len + kCheapPrepend) &#123; // FIXME: move readable data buffer_.resize(writerIndex_ + len); &#125; else &#123; // move readable data to the front, make space inside buffer assert(kCheapPrepend &lt; readerIndex_); size_t readable = readableBytes(); std::copy(begin() + readerIndex_, begin() + writerIndex_, begin() + kCheapPrepend); readerIndex_ = kCheapPrepend; writerIndex_ = readerIndex_ + readable; assert(readable == readableBytes()); &#125; &#125; private: std::vector&lt;char&gt; buffer_; size_t readerIndex_; size_t writerIndex_; static const char kCRLF[]; &#125;; &#125; // namespace net&#125; // namespace muduo 提供的 public 方法 1234void swap(Buffer &amp;rhs);void retrieve(size_t len);void append(const char * /*restrict*/ data, size_t len);... 底层是一个 vector&lt;char&gt; 类型的 buffer_； 具体原理见 “./5_Buffer类的设计.md” ssize_t readFd(int fd, int *savedErrno) 方法 123456789101112131415161718192021222324252627ssize_t Buffer::readFd(int fd, int *savedErrno) &#123; // saved an ioctl()/FIONREAD call to tell how much to read char extrabuf[65536]; struct iovec vec[2]; const size_t writable = writableBytes(); vec[0].iov_base = begin() + writerIndex_; vec[0].iov_len = writable; vec[1].iov_base = extrabuf; vec[1].iov_len = sizeof extrabuf; // when there is enough space in this buffer, don't read into extrabuf. // when extrabuf is used, we read 128k-1 bytes at most. const int iovcnt = (writable &lt; sizeof extrabuf) ? 2 : 1; const ssize_t n = sockets::readv(fd, vec, iovcnt); if (n &lt; 0) &#123; *savedErrno = errno; &#125; else if (implicit_cast&lt;size_t&gt;(n) &lt;= writable) &#123; writerIndex_ += n; &#125; else &#123; writerIndex_ = buffer_.size(); append(extrabuf, n - writable); &#125; return n;&#125; 这个方法的原理是： 走内核的系统调用较为耗时，为了提高效率，最好一次读完 最终希望达到的目标是，数据从内核缓冲区全部进入 buffer 中的缓冲区(底层是一个 vector) 第一点和第二点有所矛盾，因此这个方法在栈上先分配一个 char extrabuf[65536];：如果 buffer 中的空间大小足够，就直接从内核拷贝到 buffer 中；如果不够，那就先读到栈上的 65536 字节的缓冲区，然后再 append 到 buffer 中，而 append 的过程是 buffer 类本身要考虑的事情","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"IO","slug":"IO","permalink":"http://yoursite.com/tags/IO/"}]},{"title":"Buffer 类的设计","slug":"Buffer类的设计","date":"2020-03-09T02:49:12.510Z","updated":"2020-03-09T02:49:47.000Z","comments":true,"path":"开源组件/muduo/Buffer类的设计/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/Buffer%E7%B1%BB%E7%9A%84%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"原文链接：https://blog.csdn.net/Solstice/article/details/6329080 1.Muduo 的 IO 模型 2.为什么 non-blocking 网络编程中应用层 buffer 是必须的？ TcpConnection 必须要有 output buffer TcpConnection 必须要有 input buffer 3. Buffer 的要求 Buffer::readFd() 线程安全？ 4. Muduo Buffer 的数据结构 5. Muduo Buffer 的操作 基本的 read-write cycle 自动增长 size() 与 capacity() 内部腾挪 prepend 6. 其他设计方案 不用 vector？ Zero copy ? 7. 性能是不是问题？看跟谁比 1.Muduo 的 IO 模型UNPv1 第 6.2 节总结了 Unix/Linux 上的五种 IO 模型：阻塞(blocking)、非阻塞(non-blocking)、IO 复用(IO multiplexing)、信号驱动(signal-driven)、异步(asynchronous)。这些都是单线程下的 IO 模型。 C10k 问题的页面介绍了五种 IO 策略，把线程也纳入考量。（现在 C10k 已经不是什么问题，C100k 也不是大问题，C1000k 才算得上挑战）。 在这个多核时代，线程是不可避免的。那么服务端网络编程该如何选择线程模型呢？我赞同 libev 作者的观点：one loop per thread is usually a good model。之前我也不止一次表述过这个观点，见《多线程服务器的常用编程模型》《多线程服务器的适用场合》。 如果采用 one loop per thread 的模型，多线程服务端编程的问题就简化为如何设计一个高效且易于使用的 event loop，然后每个线程 run 一个 event loop 就行了（当然、同步和互斥是不可或缺的）。在“高效”这方面已经有了很多成熟的范例（libev、libevent、memcached、varnish、lighttpd、nginx），在“易于使用”方面我希望 muduo 能有所作为。（muduo 可算是用现代 C++ 实现了 Reactor 模式，比起原始的 Reactor 来说要好用得多。） event loop 是 non-blocking 网络编程的核心，在现实生活中，non-blocking 几乎总是和 IO-multiplexing 一起使用，原因有两点： 没有人真的会用轮询 (busy-pooling) 来检查某个 non-blocking IO 操作是否完成，这样太浪费 CPU cycles。 IO-multiplex 一般不能和 blocking IO 用在一起，因为 blocking IO 中 read()/write()/accept()/connect() 都有可能阻塞当前线程，这样线程就没办法处理其他 socket 上的 IO 事件了。见 UNPv1 第 16.6 节“nonblocking accept”的例子。 所以，当我提到 non-blocking 的时候，实际上指的是 non-blocking + IO-muleiplexing，单用其中任何一个是不现实的。另外，本文所有的“连接”均指 TCP 连接，socket 和 connection 在文中可互换使用。 当然，non-blocking 编程比 blocking 难得多，见陈硕在《Muduo 网络编程示例之零：前言》中“TCP 网络编程本质论”一节列举的难点。基于 event loop 的网络编程跟直接用 C/C++ 编写单线程 Windows 程序颇为相像： 程序不能阻塞，否则窗口就失去响应了； 在 event handler 中，程序要尽快交出控制权，返回窗口的事件循环。 2.为什么 non-blocking 网络编程中应用层 buffer 是必须的？Non-blocking IO 的核心思想是避免阻塞在 read() 或 write() 或其他 IO 系统调用上，这样可以最大限度地复用 thread-of-control，让一个线程能服务于多个 socket 连接。IO 线程只能阻塞在 IO-multiplexing 函数上，如 select()/poll()/epoll_wait()。这样一来，应用层的缓冲是必须的，每个 TCP socket 都要有 stateful 的 input buffer 和 output buffer。 TcpConnection 必须要有 output buffer考虑一个常见场景：程序想通过 TCP 连接发送 100k 字节的数据，但是在 write() 调用中，操作系统只接受了 80k 字节（受 TCP advertised window 的控制，细节见 TCPv1），你肯定不想在原地等待，因为不知道会等多久（取决于对方什么时候接受数据，然后滑动 TCP 窗口）。程序应该尽快交出控制权，返回 event loop。在这种情况下，剩余的 20k 字节数据怎么办？ 对于应用程序而言，它只管生成数据，它不应该关心到底数据是一次性发送还是分成几次发送，这些应该由网络库来操心，程序只要调用 TcpConnection::send() 就行了，网络库会负责到底。网络库应该接管这剩余的 20k 字节数据，把它保存在该 TCP connection 的 output buffer 里，然后注册 POLLOUT 事件，一旦 socket 变得可写就立刻发送数据。当然，这第二次 write() 也不一定能完全写入 20k 字节，如果还有剩余，网络库应该继续关注 POLLOUT 事件；如果写完了 20k 字节，网络库应该停止关注 POLLOUT，以免造成 busy loop。（Muduo EventLoop 采用的是 epoll level trigger，这么做的具体原因我以后再说。） 如果程序又写入了 50k 字节，而这时候 output buffer 里还有待发送的 20k 数据，那么网络库不应该直接调用 write()，而应该把这 50k 数据 append 在那 20k 数据之后，等 socket 变得可写的时候再一并写入。 如果 output buffer 里还有待发送的数据，而程序又想关闭连接（对程序而言，调用 TcpConnection::send() 之后他就认为数据迟早会发出去），那么这时候网络库不能立刻关闭连接，而要等数据发送完毕，见我在《为什么 muduo 的 shutdown() 没有直接关闭 TCP 连接？》一文中的讲解。 综上，要让程序在 write 操作上不阻塞，网络库必须要给每个 tcp connection 配置 output buffer。 TcpConnection 必须要有 input bufferTCP 是一个无边界的字节流协议，接收方必须要处理“收到的数据尚不构成一条完整的消息”和“一次收到两条消息的数据”等等情况。一个常见的场景是，发送方 send 了两条 10k 字节的消息（共 20k），接收方收到数据的情况可能是： 一次性收到 20k 数据 分两次收到，第一次 5k，第二次 15k 分两次收到，第一次 15k，第二次 5k 分两次收到，第一次 10k，第二次 10k 分三次收到，第一次 6k，第二次 8k，第三次 6k 其他任何可能 网络库在处理“socket 可读”事件的时候，必须一次性把 socket 里的数据读完（从操作系统 buffer 搬到应用层 buffer），否则会反复触发 POLLIN 事件，造成 busy-loop。（Again, Muduo EventLoop 采用的是 epoll level trigger，这么做的具体原因我以后再说。） 那么网络库必然要应对“数据不完整”的情况，收到的数据先放到 input buffer 里，等构成一条完整的消息再通知程序的业务逻辑。这通常是 codec 的职责，见陈硕《Muduo 网络编程示例之二：Boost.Asio 的聊天服务器》一文中的“TCP 分包”的论述与代码。 所以，在 tcp 网络编程中，网络库必须要给每个 tcp connection 配置 input buffer。 所有 muduo 中的 IO 都是带缓冲的 IO (buffered IO)，你不会自己去 read() 或 write() 某个 socket，只会操作 TcpConnection 的 input buffer 和 output buffer。更确切的说，是在 onMessage() 回调里读取 input buffer；调用 TcpConnection::send() 来间接操作 output buffer，一般不会直接操作 output buffer。 btw, muduo 的 onMessage() 的原型如下，它既可以是 free function，也可以是 member function，反正 muduo TcpConnection 只认 boost::function&lt;&gt;。 1void onMessage(const TcpConnectionPtr&amp; conn, Buffer* buf, Timestamp receiveTime); 对于网络程序来说，一个简单的验收测试是：输入数据每次收到一个字节（200 字节的输入数据会分 200 次收到，每次间隔 10 ms），程序的功能不受影响。对于 Muduo 程序，通常可以用 codec 来分离“消息接收”与“消息处理”，见陈硕《在 muduo 中实现 protobuf 编解码器与消息分发器》一文中对“编解码器 codec”的介绍。 如果某个网络库只提供相当于 char buf[8192] 的缓冲，或者根本不提供缓冲区，而仅仅通知程序“某 socket 可读/某 socket 可写”，要程序自己操心 IO buffering，这样的网络库用起来就很不方便了。（我有所指，你懂得。） 3. Buffer 的要求Muduo Buffer 的设计考虑了常见的网络编程需求，我试图在易用性和性能之间找一个平衡点，目前这个平衡点更偏向于易用性。 Muduo Buffer 的设计要点： 对外表现为一块连续的内存(char*, len)，以方便客户代码的编写。 其 size() 可以自动增长，以适应不同大小的消息。它不是一个 fixed size array (即 char buf[8192])。 内部以 vector of char 来保存数据，并提供相应的访问函数。 Buffer 其实像是一个 queue，从末尾写入数据，从头部读出数据。 谁会用 Buffer？谁写谁读？根据前文分析，TcpConnection 会有两个 Buffer 成员，input buffer 与 output buffer。 input buffer，TcpConnection 会从 socket 读取数据，然后写入 input buffer（其实这一步是用 Buffer::readFd() 完成的）；客户代码从 input buffer 读取数据。 output buffer，客户代码会把数据写入 output buffer（其实这一步是用 TcpConnection::send() 完成的；TcpConnection 从 output buffer 读取数据并写入 socket。 其实，input 和 output 是针对客户代码而言，客户代码从 input 读，往 output 写。TcpConnection 的读写正好相反。 这里不介绍每个成员函数的作用，留给《Muduo 网络编程示例》系列。下文会仔细介绍 readIndex 和 writeIndex 的作用。 Buffer::readFd()我在《Muduo 网络编程示例之零：前言》中写道 在非阻塞网络编程中，如何设计并使用缓冲区？ 一方面我们希望减少系统调用，一次读的数据越多越划算，那么似乎应该准备一个大的缓冲区。 另一方面，我们系统减少内存占用。如果有 10k 个连接，每个连接一建立就分配 64k 的读缓冲的话，将占用 640M 内存，而大多数时候这些缓冲区的使用率很低。 muduo 用 readv 结合栈上空间巧妙地解决了这个问题。具体做法是，在栈上准备一个 65536 字节的 stackbuf，然后利用 readv() 来读取数据，iovec 有两块，第一块指向 muduo Buffer 中的 writable 字节，另一块指向栈上的 stackbuf。这样如果读入的数据不多，那么全部都读到 Buffer 中去了；如果长度超过 Buffer 的 writable 字节数，就会读到栈上的 stackbuf 里，然后程序再把 stackbuf 里的数据 append 到 Buffer 中。 这么做利用了临时栈上空间，避免开巨大 Buffer 造成的内存浪费，也避免反复调用 read() 的系统开销（通常一次 readv() 系统调用就能读完全部数据）。 这算是一个小小的创新吧。 线程安全？muduo::net::Buffer 不是线程安全的，这么做是有意的，原因如下： 对于 input buffer，onMessage() 回调始终发生在该 TcpConnection 所属的那个 IO 线程，应用程序应该在 onMessage() 完成对 input buffer 的操作，并且不要把 input buffer 暴露给其他线程。这样所有对 input buffer 的操作都在同一个线程，Buffer class 不必是线程安全的。对于 output buffer，应用程序不会直接操作它，而是调用 TcpConnection::send() 来发送数据，后者是线程安全的。如果 TcpConnection::send() 调用发生在该 TcpConnection 所属的那个 IO 线程，那么它会转而调用 TcpConnection::sendInLoop()，sendInLoop() 会在当前线程（也就是 IO 线程）操作 output buffer；如果 TcpConnection::send() 调用发生在别的线程，它不会在当前线程调用 sendInLoop() ，而是通过 EventLoop::runInLoop() 把 sendInLoop() 函数调用转移到 IO 线程（听上去颇为神奇？），这样 sendInLoop() 还是会在 IO 线程操作 output buffer，不会有线程安全问题。当然，跨线程的函数转移调用涉及函数参数的跨线程传递，一种简单的做法是把数据拷一份，绝对安全（不明白的同学请阅读代码）。 另一种更为高效做法是用 swap()。这就是为什么 TcpConnection::send() 的某个重载以 Buffer* 为参数，而不是 const Buffer&amp;，这样可以避免拷贝，而用 Buffer::swap() 实现高效的线程间数据转移。（最后这点，仅为设想，暂未实现。目前仍然以数据拷贝方式在线程间传递，略微有些性能损失。） 4. Muduo Buffer 的数据结构Buffer 的内部是一个 vector of char，它是一块连续的内存。此外，Buffer 有两个 data members (readerIndex_ 和 writerIndex_)，指向该 vector 中的元素。这两个 indices 的类型是 int，不是 char，目的是应对迭代器失效。muduo Buffer 的设计参考了 Netty 的 ChannelBuffer 和 libevent 1.4.x 的 evbuffer。*不过，其 prependable 可算是一点“微创新”**。 Muduo Buffer 的数据结构如下： 图 1 两个 indices 把 vector 的内容分为三块：prependable、readable、writable，各块的大小是（公式一）： prependable = readIndex readable = writeIndex - readIndex writable = size() - writeIndex 注：如前文所述，对于客户代码和 TcpConnection 来说，无论这个 buffer 用于 input 还是 output，客户代码和 TcpConnection 的方向永远是相反的： input buffer output buffer 客户代码 read write TcpConnection write read （prependable 的作用留到后面讨论。） readIndex 和 writeIndex 满足以下不变式(invariant): 0 ≤ readIndex ≤ writeIndex ≤ data.size() Muduo Buffer 里有两个常数 kCheapPrepend 和 kInitialSize，定义了 prependable 的初始大小和 writable 的初始大小。（readable 的初始大小为 0。）在初始化之后，Buffer 的数据结构如下：括号里的数字是该变量或常量的值。 图 2 根据以上（公式一）可算出各块的大小，刚刚初始化的 Buffer 里没有 payload 数据，所以 readable == 0。 5. Muduo Buffer 的操作基本的 read-write cycleBuffer 初始化后的情况见图 1，如果有人向 Buffer 写入了 200 字节，那么其布局是： 图 3 图 3 中 writeIndex 向后移动了 200 字节，readIndex 保持不变，readable 和 writable 的值也有变化。 如果有人从 Buffer read() &amp; retrieve() （下称“读入”）了 50 字节，结果见图 4。与上图相比，readIndex 向后移动 50 字节，writeIndex 保持不变，readable 和 writable 的值也有变化（这句话往后从略）。 图 4 然后又写入了 200 字节，writeIndex 向后移动了 200 字节，readIndex 保持不变，见图 5。 图 5 接下来，一次性读入 350 字节，请注意，由于全部数据读完了，readIndex 和 writeIndex 返回原位以备新一轮使用，见图 6，这和图 2 是一样的。 图 6 以上过程可以看作是发送方发送了两条消息，长度分别为 50 字节和 350 字节，接收方分两次收到数据，每次 200 字节，然后进行分包，再分两次回调客户代码。 自动增长Muduo Buffer 不是固定长度的，它可以自动增长，这是使用 vector 的直接好处。 假设当前的状态如图 7 所示。（这和前面图 5 是一样的。） 图 7 客户代码一次性写入 1000 字节，而当前可写的字节数只有 624，那么 buffer 会自动增长以容纳全部数据，得到的结果是图 8。注意 readIndex 返回到了前面，以保持 prependable 等于 kCheapPrependable。由于 vector 重新分配了内存，原来指向它元素的指针会失效，这就是为什么 readIndex 和 writeIndex 是整数下标而不是指针。 代码如下： 123456789101112131415161718192021...void makeSpace(size_t len) &#123; if (writableBytes() + prependableBytes() &lt; len + kCheapPrepend) &#123; // FIXME: move readable data buffer_.resize(writerIndex_ + len); &#125; else &#123; // move readable data to the front, make space inside buffer assert(kCheapPrepend &lt; readerIndex_); size_t readable = readableBytes(); std::copy(begin() + readerIndex_, begin() + writerIndex_, begin() + kCheapPrepend); readerIndex_ = kCheapPrepend; writerIndex_ = readerIndex_ + readable; assert(readable == readableBytes()); &#125;&#125;... 图 8 然后读入 350 字节，readIndex 前移，见图 9。 图 9 最后，读完剩下的 1000 字节，readIndex 和 writeIndex 返回 kCheapPrependable，见图 10。 图 10 注意 buffer 并没有缩小大小，下次写入 1350 字节就不会重新分配内存了。换句话说，Muduo Buffer 的 size() 是自适应的，它一开始的初始值是 1k，如果程序里边经常收发 10k 的数据，那么用几次之后它的 size() 会自动增长到 10k，然后就保持不变。这样一方面避免浪费内存（有的程序可能只需要 4k 的缓冲），另一方面避免反复分配内存。当然，客户代码可以手动 shrink() buffer size()。 size() 与 capacity()使用 vector 的另一个好处是它的 capcity() 机制减少了内存分配的次数。比方说程序反复写入 1 字节，muduo Buffer 不会每次都分配内存，vector 的 capacity() 以指数方式增长，让 push_back() 的平均复杂度是常数。比方说经过第一次增长，size() 刚好满足写入的需求，如图 11。但这个时候 vector 的 capacity() 已经大于 size()，在接下来写入 capacity()-size() 字节的数据时，都不会重新分配内存，见图 12。 图 11 图 12 细心的读者可能会发现用 capacity() 也不是完美的，它有优化的余地。具体来说，vector::resize() 会初始化(memset/bzero)内存，而我们不需要它初始化，因为反正立刻就要填入数据。比如，在图 12 的基础上写入 200 字节，由于 capacity() 足够大，不会重新分配内存，这是好事；但是 vector::resize() 会先把那 200 字节设为 0 （图 13），然后 muduo buffer 再填入数据（图 14）。这么做稍微有点浪费，不过我不打算优化它，除非它确实造成了性能瓶颈。（精通 STL 的读者可能会说用 vector::append() 以避免浪费，但是 writeIndex 和 size() 不一定是对齐的，会有别的麻烦。） 图 13 图 14 google protobuf 中有一个 STLStringResizeUninitialized 函数，干的就是这个事情。 内部腾挪有时候，经过若干次读写，readIndex 移到了比较靠后的位置，留下了巨大的 prependable 空间，见图 14。 图 14 这时候，如果我们想写入 300 字节，而 writable 只有 200 字节，怎么办？muduo Buffer 在这种情况下不会重新分配内存，而是先把已有的数据移到前面去，腾出 writable 空间，见图 15。 图 15 然后，就可以写入 300 字节了，见图 16。 图 16 这么做的原因是，如果重新分配内存，反正也是要把数据拷到新分配的内存区域，代价只会更大。 prepend前面说 muduo Buffer 有个小小的创新（或许不是创新，我记得在哪儿看到过类似的做法，忘了出处），即提供 prependable 空间，让程序能以很低的代价在数据前面添加几个字节。 比方说，程序以固定的4个字节表示消息的长度（即《Muduo 网络编程示例之二：Boost.Asio 的聊天服务器》中的 LengthHeaderCodec），我要序列化一个消息，但是不知道它有多长，那么我可以一直 append() 直到序列化完成（图 17，写入了 200 字节），然后再在序列化数据的前面添加消息的长度（图 18，把 200 这个数 prepend 到首部）。 图 17 图 18 通过预留 kCheapPrependable 空间，可以简化客户代码，一个简单的空间换时间思路。 6. 其他设计方案这里简单谈谈其他可能的应用层 buffer 设计方案。 不用 vector？如果有 STL 洁癖，那么可以自己管理内存，以 4 个指针为 buffer 的成员，数据结构见图 19。 图 19 说实话我不觉得这种方案比 vector 好。代码变复杂，性能也未见得有 noticeable 的改观。 如果放弃“连续性”要求，可以用 circular buffer，这样可以减少一点内存拷贝（没有“内部腾挪”）。 Zero copy ?如果对性能有极高的要求，受不了 copy() 与 resize()，那么可以考虑实现分段连续的 zero copy buffer 再配合 gather scatter IO，数据结构如图 20，这是 libevent 2.0.x 的设计方案。TCPv2介绍的 BSD TCP/IP 实现中的 mbuf 也是类似的方案，Linux 的 sk_buff 估计也差不多。细节有出入，但基本思路都是不要求数据在内存中连续，而是用链表把数据块链接到一起。 图 20 当然，高性能的代价是代码变得晦涩难读，buffer 不再是连续的，parse 消息会稍微麻烦。如果你的程序只处理 protobuf Message，这不是问题，因为 protobuf 有 ZeroCopyInputStream 接口，只要实现这个接口，parsing 的事情就交给 protobuf Message 去操心了。 7. 性能是不是问题？看跟谁比看到这里，有的读者可能会嘀咕，muduo Buffer 有那么多可以优化的地方，其性能会不会太低？对此，我的回应是“可以优化，不一定值得优化。” Muduo 的设计目标是用于开发公司内部的分布式程序。换句话说，它是用来写专用的 Sudoku server 或者游戏服务器，不是用来写通用的 httpd 或 ftpd 或 www proxy。前者通常有业务逻辑，后者更强调高并发与高吞吐。 以 Sudoku 为例，假设求解一个 Sudoku 问题需要 0.2ms，服务器有 8 个核，那么理想情况下每秒最多能求解 40,000 个问题。每次 Sudoku 请求的数据大小低于 100 字节（一个 9x9 的数独只要 81 字节，加上 header 也可以控制在 100 bytes 以下），就是说 100 x 40000 = 4 MB per second 的吞吐量就足以让服务器的 CPU 饱和。在这种情况下，去优化 Buffer 的内存拷贝次数似乎没有意义。 再举一个例子，目前最常用的千兆以太网的裸吞吐量是 125MB/s，扣除以太网 header、IP header、TCP header之后，应用层的吞吐率大约在 115 MB/s 上下。而现在服务器上最常用的 DDR2/DDR3 内存的带宽至少是 4GB/s，比千兆以太网高 40 倍以上。就是说，对于几 k 或几十 k 大小的数据，在内存里边拷几次根本不是问题，因为受以太网延迟和带宽的限制，跟这个程序通信的其他机器上的程序不会觉察到性能差异。 最后举一个例子，如果你实现的服务程序要跟数据库打交道，那么瓶颈常常在 DB 上，优化服务程序本身不见得能提高性能（从 DB 读一次数据往往就抵消了你做的全部 low-level 优化），这时不如把精力投入在 DB 调优上。 专用服务程序与通用服务程序的另外一点区别是 benchmark 的对象不同。如果你打算写一个 httpd，自然有人会拿来和目前最好的 nginx 对比，立马就能比出性能高低。然而，如果你写一个实现公司内部业务的服务程序（比如分布式存储或者搜索或者微博或者短网址），由于市面上没有同等功能的开源实现，你不需要在优化上投入全部精力，只要一版做得比一版好就行。先正确实现所需的功能，投入生产应用，然后再根据真实的负载情况来做优化，这恐怕比在编码阶段就盲目调优要更 effective 一些。 Muduo 的设计目标之一是吞吐量能让千兆以太网饱和，也就是每秒收发 120 兆字节的数据。这个很容易就达到，不用任何特别的努力。 如果确实在内存带宽方面遇到问题，说明你做的应用实在太 critical，或许应该考虑放到 Linux kernel 里边去，而不是在用户态尝试各种优化。毕竟只有把程序做到 kernel 里才能真正实现 zero copy，否则，核心态和用户态之间始终是有一次内存拷贝的。如果放到 kernel 里还不能满足需求，那么要么自己写新的 kernel，或者直接用 FPGA 或 ASIC 操作 network adapter 来实现你的高性能服务器。","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"}]},{"title":"muduo 中的 base 组件","slug":"muduo中的base组件","date":"2020-03-09T02:48:00.867Z","updated":"2020-03-09T02:48:37.000Z","comments":true,"path":"开源组件/muduo/muduo中的base组件/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/muduo%E4%B8%AD%E7%9A%84base%E7%BB%84%E4%BB%B6/","excerpt":"","text":"在 muduo/muduo/base 中，主要包括： 123456789101. Atomic2. BlockingQueue3. BoundedBlockingQueue4. Condition5. CountDownLatch6. CurrentThread7. Mutex8. Thread9. ThreadLocal10. Singleton 等。 还有几个工具类，包括： 123456781. Date2. Exception3. FileUtil4. Logging5. AsyncLogging6. GzipFile7. TimeZone8. TimeStamp 接下来依次看一下。 1.Atomic 2.Mutex 3.Singleton 4.Condition 5.CountDownLatch 6.Thread 7.ThreadLocal 8.ThreadPool 9.BlockingQueue 1.Atomic 部分源码 123456789101112131415161718192021222324252627282930313233343536namespace muduo &#123; namespace detail &#123; template&lt;typename T&gt; class AtomicIntegerT : noncopyable &#123; public: AtomicIntegerT() : value_(0) &#123; &#125; T get() &#123; // in gcc &gt;= 4.7: __atomic_load_n(&amp;value_, __ATOMIC_SEQ_CST) return __sync_val_compare_and_swap(&amp;value_, 0, 0); &#125; T getAndAdd(T x) &#123; // in gcc &gt;= 4.7: __atomic_fetch_add(&amp;value_, x, __ATOMIC_SEQ_CST) return __sync_fetch_and_add(&amp;value_, x); &#125; T getAndSet(T newValue) &#123; // in gcc &gt;= 4.7: __atomic_exchange_n(&amp;value, newValue, __ATOMIC_SEQ_CST) return __sync_lock_test_and_set(&amp;value_, newValue); &#125; ... private: volatile T value_; &#125; &#125; // namespace detail typedef detail::AtomicIntegerT&lt;int32_t&gt; AtomicInt32; typedef detail::AtomicIntegerT&lt;int64_t&gt; AtomicInt64;&#125; // namespace muduo 底层用一个 volatile T value_ 变量，C++ 中的 volatile 语义是确保读取和写入这个被修饰的变量时，都强制在内存中进行，防止出现编译器优化或从寄存器中取值，保证多线程的可见性。 Atomic 类的底层函数是 1231. T get()2. T getAndAdd(T x)3. T getAndSet(T newValue) 它们用到的是 gcc 提供的 CAS 操作系列函数 Atomic 类提供的全部方法： 123456789101. T get()2. T getAndAdd(T x)3. T getAndSet(T newValue)4. T addAndGet(T x)5. T incrementAndGet()6. T decrementAndGet()7. void add(T x)8. void increment()9. void decrement() &#123; 其中 4-9 使用了 1-3 作为底层方法 一个不错的习惯是提供了常见模板类的具体实例 AtomicInt32 和 AtomicInt64 2.Mutex 部分源码 123456789101112131415161718192021222324252627282930313233343536373839namespace muduo &#123; class CAPABILITY(\"mutex\") MutexLock : noncopyable &#123; public: MutexLock(): holder_(0) &#123; pthread_mutex_init(&amp;mutex_, NULL); &#125; ~MutexLock() &#123; assert(holder_ == 0); pthread_mutex_destroy(&amp;mutex_); &#125; // must be called when locked, i.e. for assertion bool isLockedByThisThread() const &#123; return holder_ == CurrentThread::tid(); &#125; void lock() ACQUIRE() &#123; pthread_mutex_lock(&amp;mutex_); assignHolder(); &#125; void unlock() RELEASE() &#123; unassignHolder(); pthread_mutex_unlock(&amp;mutex_); &#125; private: void unassignHolder() &#123; holder_ = 0; &#125; void assignHolder() &#123; holder_ = CurrentThread::tid(); &#125; pthread_mutex_t mutex_; pid_t holder_; &#125;; 底层是一个 pthread_mutex_t 的互斥锁，和一个 pid_t 类型的变量，用于标识持有锁的线程 底层用到的方法是四个 12341. pthread_mutex_init(&amp;mutex_, NULL);2. pthread_mutex_lock(&amp;mutex_);3. pthread_mutex_unlock(&amp;mutex_);4. pthread_mutex_destroy(&amp;mutex_); 要注意的是 lock 和 unlock 中的顺序 真正使用时，为了防止忘记 unlock 的情况发生，使用一个包装类 MutexLockGuard 进行控制：这个类的在构造函数中加锁，在析构函数中解锁。 12345678910111213class MutexLockGuard : noncopyable &#123;public: explicit MutexLockGuard(MutexLock &amp;mutex) ACQUIRE(mutex) : mutex_(mutex) &#123; mutex_.lock(); &#125; ~MutexLockGuard() RELEASE() &#123; mutex_.unlock(); &#125;private: MutexLock &amp;mutex_;&#125;; 3.Singleton 部分源码 12345678910111213141516171819202122232425262728293031323334353637383940414243namespace muduo &#123; template&lt;typename T&gt; class Singleton : noncopyable &#123; public: Singleton() = delete; ~Singleton() = delete; static T &amp;instance() &#123; pthread_once(&amp;ponce_, &amp;Singleton::init); assert(value_ != NULL); return *value_; &#125; private: static void init() &#123; value_ = new T(); if (!detail::has_no_destroy&lt;T&gt;::value) &#123; ::atexit(destroy); &#125; &#125; static void destroy() &#123; typedef char T_must_be_complete_type[sizeof(T) == 0 ? -1 : 1]; T_must_be_complete_type dummy; (void) dummy; delete value_; value_ = NULL; &#125; private: static pthread_once_t ponce_; static T *value_; &#125;; template&lt;typename T&gt; pthread_once_t Singleton&lt;T&gt;::ponce_ = PTHREAD_ONCE_INIT; template&lt;typename T&gt; T *Singleton&lt;T&gt;::value_ = NULL;&#125; 底层使用了 pthread_once_t 作为控制多线程下单例类安全的措施 将单例类的构造函数和析构函数全部禁用掉 123public: Singleton() = delete; ~Singleton() = delete; 4.Condition 部分源码 123456789101112131415161718192021222324252627282930313233namespace muduo &#123; class Condition : noncopyable &#123; public: explicit Condition(MutexLock &amp;mutex): mutex_(mutex) &#123; pthread_cond_init(&amp;pcond_, NULL); &#125; ~Condition() &#123; pthread_cond_destroy(&amp;pcond_); &#125; void wait() &#123; MutexLock::UnassignGuard ug(mutex_); pthread_cond_wait(&amp;pcond_, mutex_.getPthreadMutex()); &#125; // returns true if time out, false otherwise. bool waitForSeconds(double seconds); void notify() &#123; pthread_cond_signal(&amp;pcond_); &#125; void notifyAll() &#123; pthread_cond_broadcast(&amp;pcond_); &#125; private: MutexLock &amp;mutex_; pthread_cond_t pcond_; &#125;;&#125; // namespace muduo 这个类的底层对 pthread_cond_t 对象进行了封装，底层方法是 123451. pthread_cond_init(&amp;pcond_, NULL);2. pthread_cond_wait(&amp;pcond_, mutex_.getPthreadMutex());3. pthread_cond_signal(&amp;pcond_);4. pthread_cond_broadcast(&amp;pcond_);5. pthread_cond_destroy(&amp;pcond_); 条件变量要和互斥锁关联，所以在构造函数中强制绑定一个 Mutex；在 wait 方法前要先持有锁，结束后要释放锁，因此使用 Mutex 的包装类 MutexLockGuard 自动控制锁的持有和释放 5.CountDownLatch 部分源码 CountDownLatch.h 1234567891011121314151617181920namespace muduo &#123; class CountDownLatch : noncopyable &#123; public: explicit CountDownLatch(int count); void wait(); void countDown(); int getCount() const; private: mutable MutexLock mutex_; Condition condition_; int count_; &#125;;&#125; // namespace muduo CountDownLatch.cc 12345678910111213141516171819202122232425CountDownLatch::CountDownLatch(int count) : mutex_(), condition_(mutex_), count_(count) &#123;&#125;void CountDownLatch::wait() &#123; MutexLockGuard lock(mutex_); while (count_ &gt; 0) &#123; condition_.wait(); &#125;&#125;void CountDownLatch::countDown() &#123; MutexLockGuard lock(mutex_); --count_; if (count_ == 0) &#123; condition_.notifyAll(); &#125;&#125;int CountDownLatch::getCount() const &#123; MutexLockGuard lock(mutex_); return count_;&#125; 原理挺简单的，使用 mutex 和 condition 配合实现逻辑 6.Thread 部分源码 Thread.h 1234567891011121314151617181920212223242526272829303132333435363738394041namespace muduo &#123; class Thread : noncopyable &#123; public: typedef std::function&lt;void()&gt; ThreadFunc; explicit Thread(ThreadFunc, const string &amp;name = string()); // FIXME: make it movable in C++11 ~Thread(); void start(); int join(); // return pthread_join() bool started() const &#123; return started_; &#125; pthread_t pthreadId() const &#123; return pthreadId_; &#125; pid_t tid() const &#123; return tid_; &#125; const string&amp; name() const &#123; return name_; &#125; static int numCreated() &#123; return numCreated_.get(); &#125; private: void setDefaultName(); bool started_; bool joined_; pthread_t pthreadId_; pid_t tid_; ThreadFunc func_; string name_; CountDownLatch latch_; static AtomicInt32 numCreated_; &#125;;&#125; // namespace muduo Thread.cc 12345678910111213141516171819202122232425262728293031323334353637383940414243444546namespace muduo &#123; Thread::Thread(ThreadFunc func, const string &amp;n) : started_(false), joined_(false), pthreadId_(0), tid_(0), func_(std::move(func)), name_(n), latch_(1) &#123; setDefaultName(); &#125; Thread::~Thread() &#123; if (started_ &amp;&amp; !joined_) &#123; pthread_detach(pthreadId_); &#125; &#125; void Thread::start() &#123; assert(!started_); started_ = true; detail::ThreadData* data = new detail::ThreadData(func_, name_, &amp;tid_, &amp;latch_); if (pthread_create(&amp;pthreadId_, NULL, &amp;detail::startThread, data)) &#123; started_ = false; delete data; // or no delete? LOG_SYSFATAL &lt;&lt; \"Failed in pthread_create\"; &#125; else &#123; latch_.wait(); assert(tid_ &gt; 0); &#125; &#125; int Thread::join() &#123; assert(started_); assert(!joined_); joined_ = true; return pthread_join(pthreadId_, NULL); &#125; ...&#125; 析构函数12345Thread::~Thread() &#123; if (started_ &amp;&amp; !joined_) &#123; pthread_detach(pthreadId_); &#125;&#125; 如果当前这个线程 start 但是未 join，那么就 pthread_detach，线程结束后自动释放存储器资源（例如栈） start 函数 在 muduo::detail 命名空间中定义了 ThreadData 结构体 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849namespace muduo &#123; namespace detail &#123; struct ThreadData &#123; typedef muduo::Thread::ThreadFunc ThreadFunc; ThreadFunc func_; string name_; pid_t* tid_; CountDownLatch *latch_; ThreadData(ThreadFunc func, const string &amp;name, pid_t *tid, CountDownLatch *latch) : func_(std::move(func)), name_(name), tid_(tid), latch_(latch) &#123;&#125; void runInThread() &#123; *tid_ = muduo::CurrentThread::tid(); tid_ = NULL; latch_-&gt;countDown(); latch_ = NULL; muduo::CurrentThread::t_threadName = name_.empty() ? \"muduoThread\" : name_.c_str(); ::prctl(PR_SET_NAME, muduo::CurrentThread::t_threadName); try &#123; func_(); muduo::CurrentThread::t_threadName = \"finished\"; &#125; catch (const Exception &amp;ex) &#123; muduo::CurrentThread::t_threadName = \"crashed\"; fprintf(stderr, \"exception caught in Thread %s\\n\", name_.c_str()); fprintf(stderr, \"reason: %s\\n\", ex.what()); fprintf(stderr, \"stack trace: %s\\n\", ex.stackTrace()); abort(); &#125; catch (const std::exception &amp;ex) &#123; muduo::CurrentThread::t_threadName = \"crashed\"; fprintf(stderr, \"exception caught in Thread %s\\n\", name_.c_str()); fprintf(stderr, \"reason: %s\\n\", ex.what()); abort(); &#125; catch (...) &#123; muduo::CurrentThread::t_threadName = \"crashed\"; fprintf(stderr, \"unknown exception caught in Thread %s\\n\", name_.c_str()); throw; // rethrow &#125; &#125; &#125;; &#125;&#125; 整体思路是将参数先传进 ThreadData 结构体，然后用 pthread_create 开新线程，新线程的入口函数是 detail::startThread： 12345678910namespace muduo &#123; namespace detail &#123; void *startThread(void *obj) &#123; ThreadData *data = static_cast&lt;ThreadData *&gt;(obj); data-&gt;runInThread(); delete data; return NULL; &#125; &#125;&#125; 在这个入口函数中，会执行 data-&gt;runInThread(); 在 runInThread() 方法中，会将 countDownLatch count down，从而结束 start Thread 的线程。 join 函数 对 pthread_join 进行封装 使用示例 Thread_test.cc 123456789int main() &#123; printf(\"pid=%d, tid=%d\\n\", ::getpid(), muduo::CurrentThread::tid()); muduo::Thread t1(threadFunc); t1.start(); printf(\"t1.tid=%d\\n\", t1.tid()); t1.join(); } 12345678910111213141516171819202122232425262728293031323334353637383940414243### 7.ThreadLocal0. 部分源码&#96;&#96;&#96;cppnamespace muduo &#123; template&lt;typename T&gt; class ThreadLocal : noncopyable &#123; public: ThreadLocal() &#123; pthread_key_create(&amp;pkey_, &amp;ThreadLocal::destructor); &#125; ~ThreadLocal() &#123; pthread_key_delete(pkey_); &#125; T &amp;value() &#123; T *perThreadValue &#x3D; static_cast&lt;T *&gt;(pthread_getspecific(pkey_)); if (!perThreadValue) &#123; T *newObj &#x3D; new T(); pthread_setspecific(pkey_, newObj); perThreadValue &#x3D; newObj; &#125; return *perThreadValue; &#125; private: static void destructor(void *x) &#123; T *obj &#x3D; static_cast&lt;T *&gt;(x); typedef char T_must_be_complete_type[sizeof(T) &#x3D;&#x3D; 0 ? -1 : 1]; T_must_be_complete_type dummy; (void) dummy; delete obj; &#125; private: pthread_key_t pkey_; &#125;;&#125; &#x2F;&#x2F; namespace muduo 利用 pthread_key_create 创建线程私有数据，在析构时调用 pthread_key_delete 销毁；使用 pthread_getspecific 获取线程私有数据 8.ThreadPool 部分源码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950namespace muduo &#123; class ThreadPool : noncopyable &#123; public: typedef std::function&lt;void()&gt; Task; explicit ThreadPool(const string &amp;nameArg = string(\"ThreadPool\")); ~ThreadPool(); // Must be called before start(). void setMaxQueueSize(int maxSize) &#123; maxQueueSize_ = maxSize; &#125; void setThreadInitCallback(const Task &amp;cb) &#123; threadInitCallback_ = cb; &#125; void start(int numThreads); void stop(); const string &amp;name() const &#123; return name_; &#125; size_t queueSize() const; // Could block if maxQueueSize &gt; 0 // There is no move-only version of std::function in C++ as of C++14. // So we don't need to overload a const&amp; and an &amp;&amp; versions // as we do in (Bounded)BlockingQueue. // https://stackoverflow.com/a/25408989 void run(Task f); private: bool isFull() const; void runInThread(); Task take(); mutable MutexLock mutex_; Condition notEmpty_; Condition notFull_; string name_; Task threadInitCallback_; std::vector&lt;std::unique_ptr&lt;muduo::Thread&gt;&gt; threads_; std::deque &lt;Task&gt; queue_; size_t maxQueueSize_; bool running_; &#125;;&#125; // namespace muduo 构造函数 12345678ThreadPool::ThreadPool(const string &amp;nameArg) : mutex_(), notEmpty_(mutex_), notFull_(mutex_), name_(nameArg), maxQueueSize_(0), running_(false) &#123;&#125; 析构函数 1234567891011121314151617ThreadPool::~ThreadPool() &#123; if (running_) &#123; stop(); &#125;&#125;void ThreadPool::stop() &#123; &#123; MutexLockGuard lock(mutex_); running_ = false; notEmpty_.notifyAll(); &#125; for (auto &amp;thr : threads_) &#123; thr-&gt;join(); &#125;&#125; 思路是在析构 ThreadPool 的时候，将每个未执行完的线程 join 一并执行掉，直到全部结束后再退出析构函数 start 方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152void ThreadPool::start(int numThreads) &#123; assert(threads_.empty()); running_ = true; threads_.reserve(numThreads); for (int i = 0; i &lt; numThreads; ++i) &#123; char id[32]; snprintf(id, sizeof id, \"%d\", i + 1); threads_.emplace_back(new muduo::Thread( std::bind(&amp;ThreadPool::runInThread, this), name_ + id)); threads_[i]-&gt;start(); &#125; if (numThreads == 0 &amp;&amp; threadInitCallback_) &#123; threadInitCallback_(); &#125;&#125;void ThreadPool::runInThread() &#123; try &#123; if (threadInitCallback_) &#123; threadInitCallback_(); &#125; while (running_) &#123; Task task(take()); if (task) &#123; task(); &#125; &#125; &#125; catch (const Exception &amp;ex) &#123; ... &#125; ...&#125;ThreadPool::Task ThreadPool::take() &#123; MutexLockGuard lock(mutex_); // always use a while-loop, due to spurious wakeup while (queue_.empty() &amp;&amp; running_) &#123; notEmpty_.wait(); &#125; Task task; if (!queue_.empty()) &#123; task = queue_.front(); queue_.pop_front(); if (maxQueueSize_ &gt; 0) &#123; notFull_.notify(); &#125; &#125; return task;&#125; start 以后，会开 numThreads 个线程开始跑，每个线程都会从队列中取任务 take，然后执行之。 run 方法 1234567891011121314void ThreadPool::run(Task task) &#123; if (threads_.empty()) &#123; task(); &#125; else &#123; MutexLockGuard lock(mutex_); while (isFull()) &#123; notFull_.wait(); &#125; assert(!isFull()); queue_.push_back(std::move(task)); notEmpty_.notify(); &#125;&#125; 这个方法的目标是向线程池中添加任务，本质上是向队列 queue_ 中添加任务 9.BlockingQueue 部分源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748namespace muduo &#123; template&lt;typename T&gt; class BlockingQueue : noncopyable &#123; public: BlockingQueue() : mutex_(), notEmpty_(mutex_), queue_() &#123; &#125; void put(const T &amp;x) &#123; MutexLockGuard lock(mutex_); queue_.push_back(x); notEmpty_.notify(); // wait morphing saves us // http://www.domaigne.com/blog/computing/condvars-signal-with-mutex-locked-or-not/ &#125; void put(T &amp;&amp;x) &#123; MutexLockGuard lock(mutex_); queue_.push_back(std::move(x)); notEmpty_.notify(); &#125; T take() &#123; MutexLockGuard lock(mutex_); // always use a while-loop, due to spurious wakeup while (queue_.empty()) &#123; notEmpty_.wait(); &#125; assert(!queue_.empty()); T front(std::move(queue_.front())); queue_.pop_front(); return front; &#125; size_t size() const &#123; MutexLockGuard lock(mutex_); return queue_.size(); &#125; private: mutable MutexLock mutex_; Condition notEmpty_; std::deque &lt;T&gt; queue_; &#125;;&#125; // namespace muduo 底层用的是 deque，原理挺简单的","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"}]},{"title":"muduo 项目中的 CMakeLists","slug":"muduo项目中的CMakeLists","date":"2020-03-08T08:30:22.299Z","updated":"2020-03-08T08:30:23.000Z","comments":true,"path":"开源组件/muduo/muduo项目中的CMakeLists/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/muduo%E9%A1%B9%E7%9B%AE%E4%B8%AD%E7%9A%84CMakeLists/","excerpt":"","text":"目录 base net test1 muduo base位置：muduo/muduo/base/CMakeLists.txt 这个目录下有一堆.cc文件和.h文件 1234567AsyncLogging.hAsyncLogging.ccAtomic.hBlockingQueue.hCountDownLatch.hCountDownLatch.cc... CMakeLists.txt 1234567891011121314151617181920212223242526272829303132333435# 所有的源文件 set 进 base_SRCS 这个变量set(base_SRCS AsyncLogging.cc Condition.cc CountDownLatch.cc CurrentThread.cc Date.cc Exception.cc FileUtil.cc LogFile.cc Logging.cc LogStream.cc ProcessInfo.cc Timestamp.cc Thread.cc ThreadPool.cc TimeZone.cc )# 将指定的源文件生成链接文件add_library(muduo_base $&#123;base_SRCS&#125;)# 将目标文件与库文件进行链接target_link_libraries(muduo_base pthread rt)COMPILE_FLAGS &quot;-std&#x3D;c++0x&quot;)install(TARGETS muduo_base DESTINATION lib)file(GLOB HEADERS &quot;*.h&quot;)install(FILES $&#123;HEADERS&#125; DESTINATION include&#x2F;muduo&#x2F;base)if(MUDUO_BUILD_EXAMPLES) add_subdirectory(tests)endif() net位置：muduo/muduo/net/CMakeLists.txt 当前目录下是一堆头文件和源文件 12345Acceptor.hAcceptor.ccBuffer.hBuffer.cc... CMakeLists.txt 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465include(CheckFunctionExists)check_function_exists(accept4 HAVE_ACCEPT4)if(NOT HAVE_ACCEPT4) set_source_files_properties(SocketsOps.cc PROPERTIES COMPILE_FLAGS &quot;-DNO_ACCEPT4&quot;)endif()set(net_SRCS Acceptor.cc Buffer.cc Channel.cc Connector.cc EventLoop.cc EventLoopThread.cc EventLoopThreadPool.cc InetAddress.cc Poller.cc poller&#x2F;DefaultPoller.cc poller&#x2F;EPollPoller.cc poller&#x2F;PollPoller.cc Socket.cc SocketsOps.cc TcpClient.cc TcpConnection.cc TcpServer.cc Timer.cc TimerQueue.cc )add_library(muduo_net $&#123;net_SRCS&#125;)target_link_libraries(muduo_net muduo_base)install(TARGETS muduo_net DESTINATION lib)set(HEADERS Buffer.h Callbacks.h Channel.h Endian.h EventLoop.h EventLoopThread.h EventLoopThreadPool.h InetAddress.h TcpClient.h TcpConnection.h TcpServer.h TimerId.h )install(FILES $&#123;HEADERS&#125; DESTINATION include&#x2F;muduo&#x2F;net)# 添加子目录 http 到 build 中. http 子目录中同样有头文件、源文件和 CMakeLists.txtadd_subdirectory(http)add_subdirectory(inspect)if(MUDUO_BUILD_EXAMPLES) add_subdirectory(tests)endif()if(PROTOBUF_FOUND) add_subdirectory(protobuf) add_subdirectory(protorpc)else() add_subdirectory(protobuf EXCLUDE_FROM_ALL) add_subdirectory(protorpc EXCLUDE_FROM_ALL)endif() test1位置：muduo/muduo/base/tests/CMakeLists.txt 这个目录下是各种 test 文件，包含了 123AsyncLogging_test.ccAtomic_unittest.cc... 这些 test 文件中包含了 main 方法，用多个 add_executable 编译成多个可执行文件 CMakeLists.txt 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586add_executable(asynclogging_test AsyncLogging_test.cc)target_link_libraries(asynclogging_test muduo_base)add_executable(atomic_unittest Atomic_unittest.cc)add_test(NAME atomic_unittest COMMAND atomic_unittest)# Add a test to the project to be run by __ctest__ commandadd_executable(blockingqueue_test BlockingQueue_test.cc)target_link_libraries(blockingqueue_test muduo_base)add_executable(blockingqueue_bench BlockingQueue_bench.cc)target_link_libraries(blockingqueue_bench muduo_base)add_executable(boundedblockingqueue_test BoundedBlockingQueue_test.cc)target_link_libraries(boundedblockingqueue_test muduo_base)add_executable(date_unittest Date_unittest.cc)target_link_libraries(date_unittest muduo_base)add_test(NAME date_unittest COMMAND date_unittest)add_executable(exception_test Exception_test.cc)target_link_libraries(exception_test muduo_base)add_test(NAME exception_test COMMAND exception_test)add_executable(fileutil_test FileUtil_test.cc)target_link_libraries(fileutil_test muduo_base)add_test(NAME fileutil_test COMMAND fileutil_test)add_executable(fork_test Fork_test.cc)target_link_libraries(fork_test muduo_base)if(ZLIB_FOUND) add_executable(gzipfile_test GzipFile_test.cc) target_link_libraries(gzipfile_test muduo_base z) add_test(NAME gzipfile_test COMMAND gzipfile_test)endif()add_executable(logfile_test LogFile_test.cc)target_link_libraries(logfile_test muduo_base)add_executable(logging_test Logging_test.cc)target_link_libraries(logging_test muduo_base)add_executable(logstream_bench LogStream_bench.cc)target_link_libraries(logstream_bench muduo_base)if(BOOSTTEST_LIBRARY)add_executable(logstream_test LogStream_test.cc)target_link_libraries(logstream_test muduo_base boost_unit_test_framework)add_test(NAME logstream_test COMMAND logstream_test)endif()add_executable(mutex_test Mutex_test.cc)target_link_libraries(mutex_test muduo_base)add_executable(processinfo_test ProcessInfo_test.cc)target_link_libraries(processinfo_test muduo_base)add_executable(singleton_test Singleton_test.cc)target_link_libraries(singleton_test muduo_base)add_executable(singleton_threadlocal_test SingletonThreadLocal_test.cc)target_link_libraries(singleton_threadlocal_test muduo_base)add_executable(thread_bench Thread_bench.cc)target_link_libraries(thread_bench muduo_base)add_executable(thread_test Thread_test.cc)target_link_libraries(thread_test muduo_base)add_executable(threadlocal_test ThreadLocal_test.cc)target_link_libraries(threadlocal_test muduo_base)add_executable(threadlocalsingleton_test ThreadLocalSingleton_test.cc)target_link_libraries(threadlocalsingleton_test muduo_base)add_executable(threadpool_test ThreadPool_test.cc)target_link_libraries(threadpool_test muduo_base)add_executable(timestamp_unittest Timestamp_unittest.cc)target_link_libraries(timestamp_unittest muduo_base)add_test(NAME timestamp_unittest COMMAND timestamp_unittest)add_executable(timezone_unittest TimeZone_unittest.cc)target_link_libraries(timezone_unittest muduo_base)add_test(NAME timezone_unittest COMMAND timezone_unittest) muduo位置：muduo/CMakeLists.txt 这个目录下有： 1234add_subdirectory(muduo&#x2F;base)add_subdirectory(muduo&#x2F;net)add_subdirectory(contrib)add_subdirectory(examples) CMakeLists.txt 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116cmake_minimum_required(VERSION 2.6)project(muduo C CXX)enable_testing()if(NOT CMAKE_BUILD_TYPE) set(CMAKE_BUILD_TYPE &quot;Release&quot;)endif()# only build examples if this is the main projectif(CMAKE_PROJECT_NAME STREQUAL &quot;muduo&quot;) option(MUDUO_BUILD_EXAMPLES &quot;Build Muduo examples&quot; ON)endif()set(CXX_FLAGS -g # -DVALGRIND -DCHECK_PTHREAD_RETURN_VALUE -D_FILE_OFFSET_BITS&#x3D;64 -Wall -Wextra -Werror -Wconversion -Wno-unused-parameter -Wold-style-cast -Woverloaded-virtual -Wpointer-arith -Wshadow -Wwrite-strings -march&#x3D;native # -MMD -std&#x3D;c++11 -rdynamic )if(CMAKE_BUILD_BITS EQUAL 32) list(APPEND CXX_FLAGS &quot;-m32&quot;)endif()if(CMAKE_CXX_COMPILER_ID STREQUAL &quot;Clang&quot;) list(APPEND CXX_FLAGS &quot;-Wno-null-dereference&quot;) list(APPEND CXX_FLAGS &quot;-Wno-sign-conversion&quot;) list(APPEND CXX_FLAGS &quot;-Wno-unused-local-typedef&quot;) list(APPEND CXX_FLAGS &quot;-Wthread-safety&quot;) list(REMOVE_ITEM CXX_FLAGS &quot;-rdynamic&quot;)endif()string(REPLACE &quot;;&quot; &quot; &quot; CMAKE_CXX_FLAGS &quot;$&#123;CXX_FLAGS&#125;&quot;)set(CMAKE_CXX_FLAGS_DEBUG &quot;-O0&quot;)set(CMAKE_CXX_FLAGS_RELEASE &quot;-O2 -DNDEBUG&quot;)set(EXECUTABLE_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;&#x2F;bin)set(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;&#x2F;lib)find_package(Boost REQUIRED)find_package(Protobuf)find_package(CURL)find_package(ZLIB)find_path(CARES_INCLUDE_DIR ares.h)find_library(CARES_LIBRARY NAMES cares)find_path(MHD_INCLUDE_DIR microhttpd.h)find_library(MHD_LIBRARY NAMES microhttpd)find_library(BOOSTTEST_LIBRARY NAMES boost_unit_test_framework)find_library(BOOSTPO_LIBRARY NAMES boost_program_options)find_library(BOOSTSYSTEM_LIBRARY NAMES boost_system)find_path(TCMALLOC_INCLUDE_DIR gperftools&#x2F;heap-profiler.h)find_library(TCMALLOC_LIBRARY NAMES tcmalloc_and_profiler)find_path(HIREDIS_INCLUDE_DIR hiredis&#x2F;hiredis.h)find_library(HIREDIS_LIBRARY NAMES hiredis)find_path(GD_INCLUDE_DIR gd.h)find_library(GD_LIBRARY NAMES gd)find_program(THRIFT_COMPILER thrift)find_path(THRIFT_INCLUDE_DIR thrift)find_library(THRIFT_LIBRARY NAMES thrift)if(CARES_INCLUDE_DIR AND CARES_LIBRARY) message(STATUS &quot;found cares&quot;)endif()if(CURL_FOUND) message(STATUS &quot;found curl&quot;)endif()if(PROTOBUF_FOUND) message(STATUS &quot;found protobuf&quot;)endif()if(TCMALLOC_INCLUDE_DIR AND TCMALLOC_LIBRARY) message(STATUS &quot;found tcmalloc&quot;)endif()if(ZLIB_FOUND) message(STATUS &quot;found zlib&quot;)endif()if(HIREDIS_INCLUDE_DIR AND HIREDIS_LIBRARY) message(STATUS &quot;found hiredis&quot;)endif()if(GD_INCLUDE_DIR AND GD_LIBRARY) message(STATUS &quot;found gd&quot;)endif()if(THRIFT_COMPILER AND THRIFT_INCLUDE_DIR AND THRIFT_LIBRARY) message(STATUS &quot;found thrift&quot;)endif()include_directories($&#123;Boost_INCLUDE_DIRS&#125;)include_directories($&#123;PROJECT_SOURCE_DIR&#125;)string(TOUPPER $&#123;CMAKE_BUILD_TYPE&#125; BUILD_TYPE)message(STATUS &quot;CXX_FLAGS &#x3D; &quot; $&#123;CMAKE_CXX_FLAGS&#125; &quot; &quot; $&#123;CMAKE_CXX_FLAGS_$&#123;BUILD_TYPE&#125;&#125;)add_subdirectory(muduo&#x2F;base)add_subdirectory(muduo&#x2F;net)if(MUDUO_BUILD_EXAMPLES) add_subdirectory(contrib) add_subdirectory(examples)else() if(CARES_INCLUDE_DIR AND CARES_LIBRARY) add_subdirectory(examples&#x2F;cdns) endif()endif()","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"CMake","slug":"CMake","permalink":"http://yoursite.com/tags/CMake/"}]},{"title":"发布一个基于 Reactor 模式的 C++ 网络库","slug":"发布一个基于 Reactor 模式的 C++ 网络库","date":"2020-03-08T08:21:03.674Z","updated":"2020-03-08T08:22:59.000Z","comments":true,"path":"开源组件/muduo/发布一个基于 Reactor 模式的 C++ 网络库/","link":"","permalink":"http://yoursite.com/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/%E5%8F%91%E5%B8%83%E4%B8%80%E4%B8%AA%E5%9F%BA%E4%BA%8E%20Reactor%20%E6%A8%A1%E5%BC%8F%E7%9A%84%20C++%20%E7%BD%91%E7%BB%9C%E5%BA%93/","excerpt":"","text":"1来源 https:&#x2F;&#x2F;blog.csdn.net&#x2F;Solstice&#x2F;article&#x2F;details&#x2F;5848547 123陈硕 (giantchen_AT_gmail)Blog.csdn.net&#x2F;Solstice2010 Aug 30 本文主要介绍 muduo 网络库的使用。其设计与实现将有另文讲解。 目录 由来 下载与编译 例子 基本结构 公开接口 内部实现 线程模型 结语 由来半年前我写了一篇《学之者生，用之者死——ACE历史与简评》，其中提到“我心目中理想的网络库”的样子： 线程安全，支持多核多线程 不考虑可移植性，不跨平台，只支持 Linux，不支持 Windows。 在不增加复杂度的前提下可以支持 FreeBSD/Darwin，方便将来用 Mac 作为开发用机，但不为它做性能优化。也就是说 IO multiplexing 使用 poll 和 epoll。 主要支持 x86-64，兼顾 IA32 不支持 UDP，只支持 TCP 不支持 IPv6，只支持 IPv4 不考虑广域网应用，只考虑局域网 只支持一种使用模式：non-blocking IO + one event loop per thread，不考虑阻塞 IO API 简单易用，只暴露具体类和标准库里的类，不使用 - non-trivial templates，也不使用虚函数 只满足常用需求的 90%，不面面俱到，必要的时候以 app 来适应 lib 只做 library，不做成 framework 争取全部代码在 5000 行以内（不含测试） 以上条件都满足时，可以考虑搭配 Google Protocol Buffers RPC 在想清楚这些目标之后，我开始第三次尝试编写自己的 C++ 网络库。与前两次不同，这次我一开始就想好了库的名字，叫 muduo （木铎），并在 Google code 上创建了项目： http://code.google.com/p/muduo/ 。muduo 的主体内容在 5 月底已经基本完成，现在我把它开源。 本文主要介绍 muduo 网络库的使用，其设计与实现将有另文讲解。 下载与编译下载地址： http://muduo.googlecode.com/files/muduo-0.1.0-alpha.tar.gz SHA1 Checksum: 5d3642e311177ded89ed0d15c10921738f8c984c Muduo 使用了 Linux 较新的系统调用，要求 Linux 的内核版本大于 2.6.28 （我自己用的是 2.6.32 ）。在 Debian Squeeze / Ubuntu 10.04 LTS 上编译测试通过，32 位和 64 位系统都能使用。 Muduo 采用 CMake 为 build system，安装方法： $ sudo apt-get install cmake Muduo 依赖 Boost，很容易安装： $ sudo apt-get install libboost1.40-dev # 或 libboost1.42-dev 编译方法很简单： $ tar zxf muduo-0.1.0-alpha.tar.gz $ cd muduo/ $ ./build.sh 编译生成的可执行文件和静态库文件分别位于 ../build/debug/{bin,lib} 如果要编译 release 版，可执行 $ BUILD_TYPE=release ./build.sh 编译生成的可执行文件和静态库文件分别位于 ../build/release/{bin,lib} 编译完成之后请试运行其中的例子。比如 bin/inspector_test ，然后通过浏览器访问 http://10.0.0.10:12345/ 或 http://10.0.0.10:12345/proc/status，其中 10.0.0.10 替换为你的 Linux box 的 IP。 例子Muduo 附带了几十个小例子，位于 examples 目录。其中包括从 Boost.Asio、JBoss Netty、Python Twisted 等处移植过来的例子。 examples 12345678910111213141516171819202122232425262728293031323334353637|-- simple # 简单网络协议的实现| |-- allinone # 在一个程序里同时实现下面 5 个协议| |-- chargen # RFC 864，可测试带宽| |-- daytime # RFC 867| |-- discard # RFC 863| |-- echo # RFC 862| |-- time # RFC 868| &#96;-- timeclient # time 协议的客户端|-- hub # 一个简单的 pub&#x2F;sub&#x2F;hub 服务，演示应用级的广播|-- roundtrip # 测试两台机器的网络延时与时间差|-- asio # 从 Boost.Asio 移植的例子| |-- chat # 聊天服务| &#96;-- tutorial # 一系列 timers|-- netty # 从 JBoss Netty 移植的例子| |-- discard # 可用于测试带宽，服务器可多线程运行| |-- echo # 可用于测试带宽，服务器可多线程运行| &#96;-- uptime # TCP 长连接&#96;-- twisted # 从 Python Twisted 移植的例子 &#96;-- finger # finger01 ~ 07 基本结构Muduo 的目录结构如下。 1234567891011muduo|-- base # 与网络无关的基础代码，已提前发布&#96;-- net # 网络库 |-- http # 一个简单的可嵌入的 web 服务器 |-- inspect # 基于以上 web 服务器的“窥探器”，用于报告进程的状态 &#96;-- poller # poll(2) 和 epoll(4) 两种 IO multiplexing 后端 Muduo 是基于 Reactor 模式的网络库，其核心是个事件循环 EventLoop，用于响应计时器和 IO 事件。Muduo 采用基于对象（object based）而非面向对象（object oriented）的设计风格，其接口多以 boost::function + boost::bind 表达。 Muduo 的头文件明确分为客户可见和客户不可见两类。客户可见的为白底，客户不可见的为灰底。 这里简单介绍各个头文件及 class 的作用，详细的介绍留给以后的博客。 公开接口 Buffer 仿 Netty ChannelBuffer 的 buffer class，数据的读写通过 buffer 进行 InetAddress 封装 IPv4 地址 (end point)，注意，muduo 目前不能解析域名，只认 IP EventLoop 反应器 Reactor，用户可以注册计时器回调 EventLoopThread 启动一个线程，在其中运行 EventLoop::loop() TcpConnection 整个网络库的核心，封装一次 TCP 连接 TcpClient 用于编写网络客户端，能发起连接，并且有重试功能 TcpServer 用于编写网络服务器，接受客户的连接 在这些类中，TcpConnection 的生命期依靠 shared_ptr 控制（即用户和库共同控制）。Buffer 的生命期由 TcpConnection 控制。其余类的生命期由用户控制。 HttpServer 和 Inspector，暴露出一个 http 界面，用于监控进程的状态，类似于 Java JMX。这么做的原因是，《程序员修炼之道》第 6 章第 34 条提到“对于更大、更复杂的服务器代码，提供其操作的内部试图的一种漂亮技术是使用内建的 Web 服务器”，Jeff Dean 也说“（每个 Google 的服务器进程）Export HTML-based status pages for easy diagnosis”。 内部实现 Channel 是 selectable IO channel，负责注册与响应 IO 事件，它不拥有 file descriptor。它是 Acceptor、Connector、EventLoop、TimerQueue、TcpConnection 的成员，生命期由后者控制。 Socket 封装一个 file descriptor，并在析构时关闭 fd。它是 Acceptor、TcpConnection 的成员，生命期由后者控制。EventLoop、TimerQueue 也拥有 fd，但是不封装为 Socket。SocketsOps 封装各种 sockets 系统调用。 EventLoop 封装事件循环，也是事件分派的中心。它用 eventfd(2) 来异步唤醒，这有别于传统的用一对 pipe(2) 的办法。它用 TimerQueue 作为计时器管理，用 Poller 作为 IO Multiplexing。 Poller 是 PollPoller 和 EPollPoller 的基类，采用“电平触发”的语意。它是 EventLoop 的成员，生命期由后者控制。 PollPoller 和 EPollPoller 封装 poll(2) 和 epoll(4) 两种 IO Multiplexing 后端。Poll 的存在价值是便于调试，因为 poll(2) 调用是上下文无关的，用 strace 很容易知道库的行为是否正确。 Connector 用于发起 TCP 连接，它是 TcpClient 的成员，生命期由后者控制。 Acceptor 用于接受 TCP 连接，它是 TcpServer 的成员，生命期由后者控制。 TimerQueue 用 timerfd 实现定时，这有别于传统的设置 poll/epoll_wait 的等待时长的办法。为了简单起见，目前用链表来管理 Timer，如果有必要可改为优先队列，这样复杂度可从 O(n) 降为O(ln n) （某些操作甚至是 O(1)）。它是 EventLoop 的成员，生命期由后者控制。 EventLoopThreadPool 用于创建 IO 线程池，也就是说把 TcpConnection 分派到一组运行 EventLoop 的线程上。它是 TcpServer 的成员，生命期由后者控制。 类图 线程模型Muduo 的线程模型符合我主张的 one loop per thread + thread pool 模型。每个线程最多有一个 EventLoop。每个 TcpConnection 必须归某个 EventLoop 管理，所有的 IO 会转移到这个线程，换句话说一个 file descriptor 只能由一个线程读写。TcpConnection 所在的线程由其所属的 EventLoop 决定，这样我们可以很方便地把不同的 TCP 连接放到不同的线程去，也可以把一些 TCP 连接放到一个线程里。TcpConnection 和 EventLoop 是线程安全的，可以跨线程调用。TcpServer 直接支持多线程，它有两种模式： 单线程，accept 与 TcpConnection 用同一个线程做 IO。 多线程，accept 与 EventLoop 在同一个线程，另外创建一个 EventLoopThreadPool，新到的连接会按 round-robin 方式分配到线程池中。 结语Muduo 是我对常见网络编程任务的总结，用它我能很容易地编写多线程的 TCP 服务器和客户端。Muduo 是我业余时间的作品，代码估计还有很多 bug，功能也不完善（例如不支持 signal 处理），待日后慢慢改进吧。","categories":[{"name":"开源组件","slug":"开源组件","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"muduo","slug":"开源组件/muduo","permalink":"http://yoursite.com/categories/%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/muduo/"}],"tags":[{"name":"Cpp","slug":"Cpp","permalink":"http://yoursite.com/tags/Cpp/"},{"name":"IO","slug":"IO","permalink":"http://yoursite.com/tags/IO/"},{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"},{"name":"Reactor","slug":"Reactor","permalink":"http://yoursite.com/tags/Reactor/"}]}]}